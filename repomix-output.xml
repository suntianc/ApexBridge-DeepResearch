This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
app/
  api/
    __init__.py
    history.py
    research.py
  core/
    __init__.py
    config.py
    llm.py
  modules/
    debate/
      __init__.py
      mad_framework.py
    insight/
      __init__.py
      prompts.py
    knowledge/
      __init__.py
      rdb.py
      vector.py
    orchestrator/
      __init__.py
      dag.py
      graph.py
      state.py
    perception/
      __init__.py
      crawler.py
      search.py
    verification/
      __init__.py
      verification_agent.py
    __init__.py
  __init__.py
  worker.py
.gitignore
.repomixignore
LICENSE
main.py
repomix.config.json
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(mkdir:*)",
      "Bash(python:*)",
      "Bash(tree:*)",
      "Bash(git add:*)",
      "Bash(git commit:*)",
      "Bash(find:*)",
      "Bash(python3 -m py_compile:*)",
      "Bash(python3 -c:*)",
      "Bash(pip3 install -r requirements.txt 2>&1:*)",
      "Bash(cat:*)"
    ]
  }
}
</file>

<file path="app/modules/debate/mad_framework.py">
from typing import Dict, Literal
import json
import asyncio

from app.core.config import settings
from app.core.llm import simple_llm_call
from app.modules.insight.prompts import ResearchPrompts

class DebateResult(Dict):
    winner: Literal["Affirmative", "Negative", "Uncertain"]
    conclusion: str
    reasoning: str

class MADFramework:
    """
    Multi-Agent Debate (MAD) æ¡†æ¶
    ç”¨äºè§£å†³äº‹å®å†²çªæˆ–é«˜æ­§ä¹‰é—®é¢˜
    """
    
    @staticmethod
    async def conduct_debate(topic: str, context: str) -> DebateResult:
        """
        æ‰§è¡Œä¸€è½®æ ‡å‡†çš„è¾©è®ºï¼šæ­£æ–¹ vs åæ–¹ -> æ³•å®˜è£å†³
        """
        print(f"âš–ï¸ [MAD] Starting debate on: {topic}")
        
        # 1. å¹¶è¡Œç”ŸæˆåŒæ–¹è¾©è¯ (Parallel Generation)
        # ä½¿ç”¨ Smart æ¨¡å‹ä»¥ä¿è¯é€»è¾‘æ€§
        task_affirmative = simple_llm_call(
            ResearchPrompts.debate_argument(topic, "æ­£æ–¹ (æ”¯æŒ/è‚¯å®š)", context),
            model=settings.MODEL_SMART
        )
        
        task_negative = simple_llm_call(
            ResearchPrompts.debate_argument(topic, "åæ–¹ (åå¯¹/æ€€ç–‘)", context),
            model=settings.MODEL_SMART
        )
        
        # å¹¶å‘æ‰§è¡Œ
        arg_aff, arg_neg = await asyncio.gather(task_affirmative, task_negative)
        
        print(f"ğŸ—£ï¸ [MAD] Affirmative: {arg_aff[:50]}...")
        print(f"ğŸ—£ï¸ [MAD] Negative: {arg_neg[:50]}...")
        
        # 2. æ³•å®˜è£å†³ (Judge)
        judge_prompt = ResearchPrompts.debate_judgment(topic, arg_aff, arg_neg)
        judge_response = await simple_llm_call(judge_prompt, model=settings.MODEL_SMART)
        
        try:
            clean_json = judge_response.replace("```json", "").replace("```", "").strip()
            result = json.loads(clean_json)
            print(f"âš–ï¸ [MAD] Judgment: {result.get('winner')} - {result.get('conclusion')[:50]}...")
            return result
        except Exception as e:
            print(f"âš ï¸ MAD Judgment parsing failed: {e}")
            return {
                "winner": "Uncertain",
                "conclusion": "Debate failed to reach consensus.",
                "reasoning": str(e)
            }
</file>

<file path="app/modules/orchestrator/dag.py">
# app/modules/orchestrator/dag.py
from typing import List, Dict, Optional
from enum import Enum
from pydantic import BaseModel, Field
from datetime import datetime

class TaskStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped" # ğŸŸ¢ æ–°å¢ï¼šè·³è¿‡çŠ¶æ€

class ResearchTask(BaseModel):
    id: str
    description: str
    dependencies: List[str] = Field(default_factory=list)
    status: TaskStatus = TaskStatus.PENDING
    result: Optional[str] = None
    error: Optional[str] = None # ğŸŸ¢ æ–°å¢ï¼šé”™è¯¯ä¿¡æ¯
    created_at: datetime = Field(default_factory=datetime.now)
    completed_at: Optional[datetime] = None
    retry_count: int = 0 # ğŸŸ¢ æ–°å¢ï¼šé‡è¯•è®¡æ•°ï¼ˆé¢„ç•™ç»™ Graph ç”¨ï¼‰

class DAGManager:
    def __init__(self, tasks: List[Dict] = None):
        self.tasks: Dict[str, ResearchTask] = {}
        if tasks:
            self.load_from_state(tasks)

    def load_from_state(self, task_list: List[Dict]):
        for t_data in task_list:
            task = ResearchTask(**t_data)
            self.tasks[task.id] = task

    def to_state(self) -> List[Dict]:
        return [task.model_dump(mode='json') for task in self.tasks.values()]

    def add_task(self, id: str, description: str, dependencies: List[str] = None):
        if id in self.tasks:
            # å…è®¸æ›´æ–°å·²å­˜åœ¨çš„ä»»åŠ¡ï¼ˆåªè¦ä¸æ˜¯è¿è¡Œä¸­ï¼‰
            if self.tasks[id].status == TaskStatus.PENDING:
                 self.tasks[id].description = description
                 self.tasks[id].dependencies = dependencies or []
            return
        
        deps = dependencies or []
        self.tasks[id] = ResearchTask(id=id, description=description, dependencies=deps)

    def get_ready_tasks(self) -> List[ResearchTask]:
        """è·å–å¯æ‰§è¡Œä»»åŠ¡"""
        ready_tasks = []
        for task in self.tasks.values():
            if task.status != TaskStatus.PENDING:
                continue
            
            dependencies_met = True
            for dep_id in task.dependencies:
                dep_task = self.tasks.get(dep_id)
                # ğŸŸ¢ é€»è¾‘å¢å¼ºï¼šå¦‚æœä¾èµ–çš„ä»»åŠ¡å¤±è´¥æˆ–è·³è¿‡ï¼Œå½“å‰ä»»åŠ¡ä¹Ÿä¸èƒ½æ‰§è¡Œ
                if not dep_task or dep_task.status not in [TaskStatus.COMPLETED]:
                    dependencies_met = False
                    
                    # ğŸ›¡ï¸ ç†”æ–­ä¼ å¯¼ï¼šå¦‚æœä¾èµ–æŒ‚äº†ï¼Œè‡ªå·±ä¹Ÿè®¾ä¸ºè·³è¿‡
                    if dep_task and dep_task.status in [TaskStatus.FAILED, TaskStatus.SKIPPED]:
                        self.skip_task(task.id, reason=f"Dependency {dep_id} failed/skipped")
                    
                    break
            
            if dependencies_met:
                ready_tasks.append(task)
        
        return ready_tasks

    def set_task_running(self, task_id: str):
        if task_id in self.tasks:
            self.tasks[task_id].status = TaskStatus.RUNNING

    def complete_task(self, task_id: str, result: str):
        if task_id in self.tasks:
            t = self.tasks[task_id]
            t.status = TaskStatus.COMPLETED
            t.result = result
            t.completed_at = datetime.now()

    def fail_task(self, task_id: str, error: str):
        """ğŸŸ¢ æ ‡è®°ä»»åŠ¡å¤±è´¥"""
        if task_id in self.tasks:
            t = self.tasks[task_id]
            t.status = TaskStatus.FAILED
            t.error = error
            t.completed_at = datetime.now()
            print(f"âŒ [DAG] Task {task_id} FAILED: {error}")

    def skip_task(self, task_id: str, reason: str):
        """ğŸŸ¢ æ ‡è®°ä»»åŠ¡è·³è¿‡"""
        if task_id in self.tasks:
            t = self.tasks[task_id]
            t.status = TaskStatus.SKIPPED
            t.result = f"SKIPPED: {reason}"
            t.completed_at = datetime.now()
            print(f"â­ï¸ [DAG] Task {task_id} SKIPPED: {reason}")

    def is_all_completed(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä»»åŠ¡éƒ½å·²å¤„ç†ï¼ˆå®Œæˆã€å¤±è´¥æˆ–è·³è¿‡éƒ½ç®—ç»“æŸï¼‰"""
        return all(t.status in [TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.SKIPPED] 
                   for t in self.tasks.values())
</file>

<file path="app/modules/verification/verification_agent.py">
# app/modules/verification/verification_agent.py

from typing import List, Dict, Literal
from pydantic import BaseModel
import json
import asyncio

from langchain_text_splitters import RecursiveCharacterTextSplitter

from app.core.config import settings
from app.core.llm import simple_llm_call
from app.modules.perception.search import search_generic as search_tool
from app.modules.insight.prompts import ResearchPrompts
# ğŸŸ¢ å¼•å…¥è¾©è®ºæ¡†æ¶
from app.modules.debate.mad_framework import MADFramework

class FactClaim(BaseModel):
    original_text: str
    claim: str
    verification_status: Literal["Verified", "Disputed", "Unconfirmed"] = "Unconfirmed"
    explanation: str = ""
    source_url: str = ""

class VerificationAgent:
    """
    [éªŒè¯æ™ºèƒ½ä½“ V3]
    é›†æˆ MAD (å¤šæ™ºèƒ½ä½“è¾©è®º) çš„ç»ˆæéªŒè¯ç³»ç»Ÿ
    èƒ½åŠ›ï¼šåˆ†å—æå– -> ç‹¬ç«‹éªŒè¯ -> äº‰è®®è‡ªåŠ¨è¾©è®º
    """
    
    @staticmethod
    async def extract_claims(text: str) -> List[FactClaim]:
        """ç¬¬ä¸€æ­¥ï¼šæå–å…³é”®äº‹å®æ–­è¨€ (Map-Reduce æ¨¡å¼)"""
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=4000,
            chunk_overlap=500,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", " ", ""]
        )
        chunks = splitter.split_text(text)
        
        print(f"ğŸ›¡ï¸ [Verification] Split text into {len(chunks)} chunks for extraction.")
        
        async def process_chunk(chunk_text: str) -> List[dict]:
            prompt = ResearchPrompts.verification_claims_extraction(chunk_text)
            response = await simple_llm_call(prompt, model=settings.MODEL_FAST)
            try:
                clean_json = response.replace("```json", "").replace("```", "").strip()
                return json.loads(clean_json)
            except Exception:
                return []

        results_list = await asyncio.gather(*[process_chunk(chunk) for chunk in chunks])
        
        unique_claims = {}
        for batch in results_list:
            if not isinstance(batch, list): continue
            for item in batch:
                claim_text = item.get("claim", "").strip()
                if not claim_text: continue
                if claim_text not in unique_claims:
                    try:
                        unique_claims[claim_text] = FactClaim(**item)
                    except: pass
        
        final_claims = list(unique_claims.values())
        print(f"ğŸ›¡ï¸ [Verification] Extracted {len(final_claims)} unique claims.")
        return final_claims

    @staticmethod
    async def verify_claim(claim: FactClaim) -> FactClaim:
        """
        ç¬¬äºŒæ­¥ï¼šç‹¬ç«‹æœç´¢éªŒè¯ + ğŸŸ¢ è‡ªåŠ¨è¾©è®ºå‡çº§
        """
        print(f"ğŸ” [Verification] Checking: {claim.claim}")
        
        # 1. è·å–ä¸Šä¸‹æ–‡
        try:
            results = await search_tool(f"verify {claim.claim}", num_results=3)
            context = "\n".join([r["snippet"] for r in results]) if results else "No search results found."
        except Exception as e:
            print(f"âš ï¸ Search failed: {e}")
            context = "Search failed."
        
        # 2. åˆå§‹ LLM åˆ¤å®š
        prompt = ResearchPrompts.verification_claim_check(claim.claim, context)
        response = await simple_llm_call(prompt, model=settings.MODEL_SMART)
        
        try:
            clean_json = response.replace("```json", "").replace("```", "").strip()
            data = json.loads(clean_json)
            
            initial_status = data.get("status", "Unconfirmed")
            claim.explanation = data.get("explanation", "No explanation.")
            if results:
                claim.source_url = results[0]["url"]

            # ğŸŸ¢ 3. MAD è‡ªåŠ¨å‡çº§æœºåˆ¶ (Auto-Escalation)
            # å¦‚æœåˆå§‹åˆ¤å®šæœ‰äº‰è®®ï¼Œå¯åŠ¨è¾©è®ºæ¡†æ¶è¿›è¡Œæ·±ç©¶
            if initial_status == "Disputed":
                print(f"ğŸš¨ [Verification] Dispute detected! Escalating to MAD protocol for: {claim.claim}")
                
                # å¯åŠ¨è¾©è®º
                debate_result = await MADFramework.conduct_debate(claim.claim, context)
                
                # æ ¹æ®è¾©è®ºç»“æœæ›´æ–°çŠ¶æ€
                # å¦‚æœæ­£æ–¹(Affirmative)èµ¢äº†ï¼Œè¯´æ˜åŸæ–­è¨€å…¶å®æ˜¯æˆç«‹çš„ï¼Œä¹‹å‰å¯èƒ½è¯¯åˆ¤
                if debate_result["winner"] == "Affirmative":
                    claim.verification_status = "Verified"
                    claim.explanation = f"[MAD Overrule] {debate_result['conclusion']}"
                    print(f"âœ… [MAD] Overruled dispute -> Verified")
                    
                # å¦‚æœåæ–¹(Negative)èµ¢äº†ï¼Œç¡®è®¤æ˜¯é”™è¯¯çš„
                elif debate_result["winner"] == "Negative":
                    claim.verification_status = "Disputed"
                    claim.explanation = f"[MAD Confirmed Dispute] {debate_result['conclusion']}"
                    print(f"âŒ [MAD] Confirmed dispute.")
                    
                else:
                    claim.verification_status = "Unconfirmed"
                    claim.explanation = f"[MAD Uncertain] {debate_result['conclusion']}"

            else:
                # æ²¡æœ‰äº‰è®®ï¼Œç›´æ¥é‡‡çº³åˆå§‹ç»“æœ
                claim.verification_status = initial_status

        except Exception as e:
            print(f"âš ï¸ Verification logic failed: {e}")
            
        return claim

    @classmethod
    async def verify_report(cls, draft: str) -> str:
        """ä¸»å…¥å£"""
        # 1. æå–
        claims = await cls.extract_claims(draft)
        if not claims:
            return draft
            
        # 2. éªŒè¯ (å¹¶å‘æ§åˆ¶)
        sem = asyncio.Semaphore(5)
        async def sem_task(c):
            async with sem:
                return await cls.verify_claim(c)

        verified_claims = await asyncio.gather(*[sem_task(c) for c in claims])
        
        # 3. æŠ¥å‘Šç”Ÿæˆ
        report_suffix = "\n\n---\n### ğŸ›¡ï¸ äº‹å®æ ¸æŸ¥æŠ¥å‘Š (Automated Verification)\n"
        has_dispute = False
        
        for c in verified_claims:
            icon = "âœ…"
            if c.verification_status == "Disputed":
                icon = "âŒ"
                has_dispute = True
            elif c.verification_status == "Unconfirmed":
                icon = "âš ï¸"
            
            # å¦‚æœç»è¿‡äº† MADï¼ŒåŠ ä¸Šæ ‡è®°
            mad_tag = "âš–ï¸" if "[MAD" in c.explanation else ""
            source_link = f"([Source]({c.source_url}))" if c.source_url else ""
            
            report_suffix += f"- {icon} {mad_tag} **{c.verification_status}**: {c.claim}\n  *è¯´æ˜: {c.explanation}* {source_link}\n"
            
        final_draft = draft
        if has_dispute:
            warning = "> âš ï¸ **è­¦å‘Šï¼šæœ¬æŠ¥å‘ŠåŒ…å«éƒ¨åˆ†å­˜åœ¨äº‰è®®çš„äº‹å®ï¼Œç³»ç»Ÿå·²ä»‹å…¥å¤šæ™ºèƒ½ä½“è¾©è®º(MAD)è¿›è¡Œè£å†³ï¼Œè¯¦æƒ…è§æ–‡æœ«ã€‚**\n\n"
            final_draft = warning + final_draft + report_suffix
        else:
            final_draft = final_draft + report_suffix
            
        return final_draft
</file>

<file path=".repomixignore">
# Add patterns to ignore here, one per line
# Example:
# *.log
# tmp/
**/*.md
research/
</file>

<file path="repomix.config.json">
{
  "$schema": "https://repomix.com/schemas/latest/schema.json",
  "input": {
    "maxFileSize": 52428800
  },
  "output": {
    "filePath": "repomix-output.xml",
    "style": "xml",
    "parsableStyle": false,
    "fileSummary": true,
    "directoryStructure": true,
    "files": true,
    "removeComments": false,
    "removeEmptyLines": false,
    "compress": false,
    "topFilesLength": 5,
    "showLineNumbers": false,
    "truncateBase64": false,
    "copyToClipboard": false,
    "includeFullDirectoryStructure": false,
    "tokenCountTree": false,
    "git": {
      "sortByChanges": true,
      "sortByChangesMaxCommits": 100,
      "includeDiffs": false,
      "includeLogs": false,
      "includeLogsCount": 50
    }
  },
  "include": [],
  "ignore": {
    "useGitignore": true,
    "useDotIgnore": true,
    "useDefaultPatterns": true,
    "customPatterns": []
  },
  "security": {
    "enableSecurityCheck": true
  },
  "tokenCount": {
    "encoding": "o200k_base"
  }
}
</file>

<file path="app/api/__init__.py">
"""
API è·¯ç”±æ¨¡å—

æä¾›RESTful APIæ¥å£ï¼ŒåŒ…æ‹¬ï¼š
    - research: ç ”ç©¶ç›¸å…³APIæ¥å£
    - history: å†å²è®°å½•APIæ¥å£

æœ¬æ¨¡å—å°è£…äº†æ‰€æœ‰å¯¹å¤–æš´éœ²çš„APIè·¯ç”±ï¼Œ
é€šè¿‡FastAPIæ¡†æ¶å®ç°é«˜æ•ˆçš„ç½‘ç»œè¯·æ±‚å¤„ç†ã€‚

ä½¿ç”¨ç¤ºä¾‹:
    from app.api import research, history

ä½œè€…: ApexBridge Team
ç‰ˆæœ¬: 1.0.0
"""

from . import research, history

# æ˜ç¡®åˆ—å‡ºæ‰€æœ‰å…¬å¼€çš„å­æ¨¡å—
__all__ = [
    "research",
    "history",
]
</file>

<file path="app/api/research.py">
# app/api/research.py
from fastapi import APIRouter
from sse_starlette.sse import EventSourceResponse
from app.modules.orchestrator.graph import build_graph
from app.core.config import settings
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver
import aiosqlite
import asyncio
import json
import uuid
import traceback
# ğŸŸ¢ å¼•å…¥è¶…æ—¶æ§åˆ¶åº“ (è¯·ç¡®ä¿ pip install async_timeout)
from async_timeout import timeout 

router = APIRouter()

# è·å–æœªç¼–è¯‘çš„å›¾è°±æ„å»ºå™¨
workflow_builder = build_graph()

@router.get("/stream")
async def stream_research(topic: str):
    # ç”Ÿæˆå”¯ä¸€ä¼šè¯ ID
    thread_id = str(uuid.uuid4())
    task_id = thread_id
    config = {
        "configurable": {"thread_id": thread_id},
        "recursion_limit": settings.MAX_RECURSION_LIMIT
    }

    async def event_generator():
        # åˆå§‹åŒ–çŠ¶æ€
        inputs = {
            "task_id": task_id,
            "task": topic,
            "clarified_intent": topic, # åˆå§‹æ—¶æ„å›¾ç­‰äºé¢˜ç›®
            "plan": [],
            "knowledge_graph": [],
            "reflection_logs": [],
            "iteration_count": 0,
            "max_iterations": 3,
            
            # å…¼å®¹å­—æ®µ
            "topic": topic,          # æš‚æ—¶ä¿ç•™ï¼Œgraph.py è¿˜åœ¨ç”¨
            "draft_report": "",      # æš‚æ—¶ä¿ç•™ï¼ŒAnalyst/Critic äº¤äº’ç”¨
            "final_report": "",      # æš‚æ—¶ä¿ç•™ï¼ŒPublisher ç”¨
        }
        
        try:
            # ğŸŸ¢ ä½¿ç”¨é…ç½®åŒ–çš„è¶…æ—¶æ—¶é—´
            # async_timeout ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¼šåœ¨è¶…æ—¶åæŠ›å‡º asyncio.TimeoutError
            async with timeout(settings.GLOBAL_TIMEOUT_SEC):
                
                # 1. æ˜¾å¼åˆ›å»ºè¿æ¥
                async with aiosqlite.connect(settings.CHECKPOINT_DB_PATH) as conn:
                    
                    # ğŸ©¹ã€ç³»ç»Ÿæ€§ä¿®å¤ / Monkey Patchã€‘
                    # ä¿®å¤ langgraph åœ¨ aiosqlite ä¸Šè°ƒç”¨ is_alive çš„å…¼å®¹æ€§é—®é¢˜
                    setattr(conn, "is_alive", lambda: True)
                    
                    # 2. å°†ä¿®å¤åçš„è¿æ¥ä¼ ç»™ Checkpointer
                    checkpointer = AsyncSqliteSaver(conn)
                    
                    # 3. ç¼–è¯‘å›¾è°±
                    graph = workflow_builder.compile(checkpointer=checkpointer)
                    
                    # 4. è¿è¡Œå›¾è°± (æµå¼)
                    async for event in graph.astream(inputs, config=config):
                        for node_name, state_update in event.items():
                            payload = {
                                "step": node_name,
                                "data": state_update
                            }
                            
                            json_str = json.dumps(
                                payload, 
                                default=str, 
                                ensure_ascii=False
                            )
                            
                            yield {
                                "event": "update",
                                "data": json_str
                            }
                            # ç¼“å†²ä¸€ä¸‹ï¼Œé¿å…å‰ç«¯æ¸²æŸ“è¿‡å¿«å¡é¡¿
                            await asyncio.sleep(0.1)

                    yield {"event": "finish", "data": "DONE"}

        except asyncio.TimeoutError:
            print(f"â° Task timed out after {settings.GLOBAL_TIMEOUT_SEC}s")
            error_payload = json.dumps(
                {"error": f"Global Timeout: Research stopped after {settings.GLOBAL_TIMEOUT_SEC} seconds."}, 
                ensure_ascii=False
            )
            yield {"event": "error", "data": error_payload}
                
        except Exception as e:
            print(f"âŒ Error in stream: {e}")
            traceback.print_exc()
            
            error_payload = json.dumps(
                {"error": str(e)}, 
                ensure_ascii=False
            )
            yield {"event": "error", "data": error_payload}

    return EventSourceResponse(event_generator())
</file>

<file path="app/core/__init__.py">
"""
æ ¸å¿ƒé…ç½®æ¨¡å—

æä¾›ç³»ç»Ÿæ ¸å¿ƒé…ç½®å’ŒLLMè°ƒç”¨æ¥å£ã€‚

ä¸»è¦ç»„ä»¶:
    - simple_llm_call: é€šç”¨LLMè°ƒç”¨æ¥å£
        æ”¯æŒDeepSeekã€OpenAIã€Claudeã€Ollamaç­‰å¤šç§æ¨¡å‹
        é€šè¿‡LiteLLMæä¾›ç»Ÿä¸€çš„è°ƒç”¨æ–¹å¼

ä½¿ç”¨ç¤ºä¾‹:
    from app.core import simple_llm_call

    response = await simple_llm_call("Hello, world!")

ä½œè€…: ApexBridge Team
ç‰ˆæœ¬: 1.0.0
"""

from .llm import simple_llm_call

__all__ = ["simple_llm_call"]
</file>

<file path="app/core/config.py">
# app/core/config.py
from pydantic_settings import BaseSettings
from pydantic import field_validator
from typing import List, Literal
import os

class Settings(BaseSettings):
    # --- åŸºç¡€é…ç½® ---
    API_HOST: str = "0.0.0.0"
    API_PORT: int = 23800
    LANCEDB_PATH: str = "./data/lancedb"
    CHECKPOINT_DB_PATH: str = "./data/checkpoints.db"
    
    # --- æœç´¢é…ç½® (Search Configuration) ---
    # æœç´¢æœåŠ¡æä¾›å•†: "tavily" æˆ– "searxng"
    SEARCH_PROVIDER: Literal["tavily", "searxng"] = "tavily"
    
    # SearXNG é…ç½®
    SEARXNG_BASE_URL: str = "http://localhost:8888/search"
    
    # Tavily é…ç½® (æ”¯æŒå¤šKeyè½®è¯¢)
    # åœ¨ .env ä¸­é…ç½®: TAVILY_API_KEYS="tvly-xxx,tvly-yyy,tvly-zzz"
    TAVILY_API_KEYS: List[str] = []

    @field_validator("TAVILY_API_KEYS", mode="before")
    def parse_api_keys(cls, v):
        if isinstance(v, str):
            # å°†é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²è½¬ä¸ºåˆ—è¡¨ï¼Œå¹¶å»é™¤ç©ºæ ¼
            return [key.strip() for key in v.split(",") if key.strip()]
        return v or []

    # --- æ¨¡å‹é…ç½® (ä¿æŒä¸å˜) ---
    DEEPSEEK_API_KEY: str | None = None
    OPENAI_API_KEY: str | None = None
    MAX_RECURSION_LIMIT: int = 100
    GLOBAL_TIMEOUT_SEC: int = 600

    # Phase 4: åˆ†å±‚æ¨¡å‹ç­–ç•¥
    MODEL_PLANNER: str = "deepseek/deepseek-reasoner"
    MODEL_WRITER: str = "deepseek/deepseek-chat"
    MODEL_CRITIC: str = "deepseek/deepseek-reasoner"
    MODEL_FAST: str = "deepseek/deepseek-chat"
    MODEL_LONG: str = "deepseek/deepseek-chat"
    MODEL_SMART: str = "deepseek/deepseek-reasoner"

    # Embedding é…ç½®
    EMBEDDING_MODEL: str = "ollama/nomic-embed-text"
    EMBEDDING_DIMENSION: int = 768

    class Config:
        env_file = ".env"
        extra = "ignore"

settings = Settings()

os.makedirs(os.path.dirname(settings.CHECKPOINT_DB_PATH), exist_ok=True)
os.makedirs(settings.LANCEDB_PATH, exist_ok=True)
</file>

<file path="app/core/llm.py">
from litellm import completion
import os
from dotenv import load_dotenv

# åŠ è½½ .env ç¯å¢ƒå˜é‡
load_dotenv()

async def simple_llm_call(
    prompt: str, 
    model: str = "deepseek/deepseek-chat", # é»˜è®¤æ”¹ä¸º DeepSeek V3
    temperature: float = 0.7
) -> str:
    """
    é€šç”¨ LLM è°ƒç”¨æ¥å£ï¼Œæ”¯æŒ DeepSeek, OpenAI, Claude, Ollama ç­‰
    """
    
    # æ‰“å°å½“å‰ä½¿ç”¨çš„æ¨¡å‹ï¼Œæ–¹ä¾¿è°ƒè¯•
    print(f"ğŸ¤– [LLM Call] Model: {model}")

    try:
        # LiteLLM ä¼šè‡ªåŠ¨æ ¹æ® model å‰ç¼€è¯†åˆ«ä¾›åº”å•†
        # deepseek/deepseek-chat -> è‡ªåŠ¨æ˜ å°„åˆ° DeepSeek API
        # ollama/deepseek-r1 -> è‡ªåŠ¨æ˜ å°„åˆ°æœ¬åœ° Ollama
        
        response = completion(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            # å¦‚æœæ˜¯ DeepSeek APIï¼Œä¸éœ€è¦æ‰‹åŠ¨è®¾ base_urlï¼ŒLiteLLM å†…ç½®äº†æ”¯æŒ
            # å¦‚æœæ˜¯ Ollamaï¼ŒLiteLLM é»˜è®¤è¿æ¥ http://localhost:11434
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        print(f"âŒ [LLM Error] {model} failed: {str(e)}")
        return f"Error generation response with {model}. Details: {str(e)}"

# --- ä½¿ç”¨è¯´æ˜ ---
# 1. DeepSeek API: 
#    model="deepseek/deepseek-chat" (V3)
#    model="deepseek/deepseek-reasoner" (R1)
#
# 2. æœ¬åœ° DeepSeek (é€šè¿‡ Ollama):
#    model="ollama/deepseek-r1"
#
# 3. OpenAI:
#    model="gpt-4o"
</file>

<file path="app/modules/insight/__init__.py">
"""
æ´å¯Ÿæ¨¡å— - æ´å¯Ÿç”Ÿæˆå’ŒPromptç®¡ç†

è´Ÿè´£ç”Ÿæˆå’Œç®¡ç†æ™ºèƒ½æ´å¯Ÿï¼Œä»¥åŠä¸ºLLMæä¾›ç»“æ„åŒ–çš„æç¤ºè¯æ¨¡æ¿ã€‚

æœ¬æ¨¡å—çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
    - è®¾è®¡å’Œç®¡ç†Promptæ¨¡æ¿
    - æ ¹æ®ä¸åŒåœºæ™¯ç”Ÿæˆå®šåˆ¶åŒ–æç¤ºè¯
    - ä¼˜åŒ–LLMè¾“å…¥ä»¥è·å¾—æ›´å¥½çš„è¾“å‡ºè´¨é‡
    - ç”Ÿæˆæ·±åº¦åˆ†æå’Œæ´å¯ŸæŠ¥å‘Š

å­æ¨¡å—:
    - prompts: Promptæ¨¡æ¿ç®¡ç†
        æä¾›å„ç§åœºæ™¯ä¸‹çš„æç¤ºè¯æ¨¡æ¿
        åŒ…æ‹¬ç ”ç©¶ã€åˆ†æã€æ€»ç»“ã€åˆ›ä½œç­‰å¤šç§ç±»å‹
        æ”¯æŒåŠ¨æ€æ¨¡æ¿å‚æ•°åŒ–å’Œæ¨¡æ¿é“¾å¼ç»„åˆ

ä¸»è¦åŠŸèƒ½:
    1. æ™ºèƒ½Promptæ¨¡æ¿åº“
        - ç ”ç©¶ç±»æ¨¡æ¿ï¼šæ·±åº¦ç ”ç©¶ã€å¯¹æ¯”åˆ†æ
        - æ€»ç»“ç±»æ¨¡æ¿ï¼šæ‘˜è¦ç”Ÿæˆã€è¦ç‚¹æå–
        - åˆ›ä½œç±»æ¨¡æ¿ï¼šå†…å®¹åˆ›ä½œã€åˆ›æ„ç”Ÿæˆ
        - åˆ†æç±»æ¨¡æ¿ï¼šæ•°æ®åˆ†æã€è¶‹åŠ¿æ´å¯Ÿ

    2. åŠ¨æ€æ¨¡æ¿ç”Ÿæˆ
        - æ ¹æ®è¾“å…¥å‚æ•°ç”Ÿæˆå®šåˆ¶åŒ–æç¤ºè¯
        - æ”¯æŒæ¨¡æ¿åµŒå¥—å’Œç»„åˆ
        - æ™ºèƒ½å‚æ•°å¡«å……å’ŒéªŒè¯

    3. æç¤ºè¯ä¼˜åŒ–
        - è‡ªåŠ¨ä¼˜åŒ–æç¤ºè¯ç»“æ„
        - æä¾›æç¤ºè¯è´¨é‡è¯„ä¼°
        - æ”¯æŒA/Bæµ‹è¯•å’Œè¿­ä»£æ”¹è¿›

    4. æ´å¯Ÿç”Ÿæˆ
        - åŸºäºæ”¶é›†çš„æ•°æ®ç”Ÿæˆæ·±åº¦æ´å¯Ÿ
        - æä¾›å¤šç»´åº¦åˆ†æç»“æœ
        - ç”Ÿæˆå¯è§£é‡Šçš„åˆ†ææŠ¥å‘Š

ä½¿ç”¨ç¤ºä¾‹:
    from app.modules.insight import prompts

ä½œè€…: ApexBridge Team
ç‰ˆæœ¬: 1.0.0
"""

from . import prompts

# æ˜ç¡®åˆ—å‡ºæ‰€æœ‰å…¬å¼€çš„å­æ¨¡å—
__all__ = [
    "prompts",
]
</file>

<file path="app/modules/insight/prompts.py">
# app/modules/insight/prompts.py
from textwrap import dedent

class ResearchPrompts:
    """
    ç»Ÿä¸€ç®¡ç†ç³»ç»Ÿä¸­çš„æ‰€æœ‰ Prompt æ¨¡æ¿ã€‚
    ä½¿ç”¨ dedent å»é™¤ç¼©è¿›ï¼Œä¿æŒ Prompt å¹²å‡€ã€‚
    """

    @staticmethod
    def planner_initial(topic: str) -> str:
        """[è§„åˆ’è€…] ç¬¬ä¸€è½®ï¼šå¹¿åº¦ä¼˜å…ˆè§„åˆ’"""
        return dedent(f"""
            ç”¨æˆ·æƒ³ç ”ç©¶ '{topic}'ã€‚
            è¯·ç”Ÿæˆ 1 ä¸ªæœ€é€‚åˆå…¥é—¨æˆ–è·å–å®è§‚æ•°æ®çš„æœç´¢å¼•æ“å…³é”®è¯ã€‚
            
            è¦æ±‚ï¼š
            1. å…³é”®è¯è¦ç²¾å‡†ï¼Œåˆ©äºæœç´¢å¼•æ“ç†è§£ã€‚
            2. åªè¿”å›å…³é”®è¯æœ¬èº«ï¼Œä¸è¦åŒ…å«è§£é‡Šæˆ–å¼•å·ã€‚
        """).strip()

    @staticmethod
    def planner_gap_driven(topic: str, gap: str) -> str:
        """[è§„åˆ’è€…] åç»­è½®æ¬¡ï¼šåŸºäºç¼ºå£çš„æ·±åº¦è§„åˆ’"""
        return dedent(f"""
            ç ”ç©¶ä¸»é¢˜: {topic}
            ç›®å‰å·²æœ‰çš„åˆ†ææŒ‡å‡ºç¼ºå¤±ä¿¡æ¯(Gap): "{gap}"
            
            ä»»åŠ¡ï¼š
            è¯·ç”Ÿæˆ 1 ä¸ª**éå¸¸å…·ä½“**çš„æœç´¢å…³é”®è¯ï¼Œä¸“é—¨ç”¨æ¥æŒ–æ˜ä¸Šè¿°ç¼ºå¤±çš„ä¿¡æ¯ã€‚
            
            ç¤ºä¾‹ç­–ç•¥ï¼š
            - å¦‚æœç¼ºæ•°æ®ï¼Œæœç´¢è¯åº”åŒ…å« "statistics", "data", "report", "2024" ç­‰ã€‚
            - å¦‚æœç¼ºæŠ€æœ¯ç»†èŠ‚ï¼Œæœç´¢è¯åº”åŒ…å« "architecture", "specs", "whitepaper" ç­‰ã€‚
            
            è¦æ±‚ï¼š
            åªè¿”å›å…³é”®è¯æœ¬èº«ï¼Œä¸è¦åŒ…å«å¼•å·ã€‚
        """).strip()

    @staticmethod
    def analyst_reasoning(topic: str, context: str) -> str:
        """[åˆ†æå¸ˆ] RAG åˆ†æä¸ Gap è¯†åˆ« (é€‚ç”¨äº DeepSeek R1)"""
        return dedent(f"""
            ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ·±åº¦ç ”ç©¶å‘˜ã€‚åŸºäºä»¥ä¸‹ã€å·²æ ¸å®çš„çŸ¥è¯†åº“ç‰‡æ®µã€‘è¿›è¡Œåˆ†æã€‚
            
            ã€å·²æ ¸å®ä¿¡æ¯ã€‘ï¼š
            {context}
            
            ã€ä»»åŠ¡ã€‘ï¼š
            1. ç»¼åˆç›®å‰å…³äº '{topic}' çš„æ‰€æœ‰ä¿¡æ¯ï¼Œå†™ä¸€æ®µæ·±åº¦åˆ†æï¼ˆDraftï¼‰ã€‚
            2. æ‰¹åˆ¤æ€§åœ°æŒ‡å‡ºï¼šæˆ‘ä»¬è¿˜**ç¼ºå°‘**ä»€ä¹ˆå…³é”®æ•°æ®æˆ–è§†è§’ï¼Ÿ(Gap Analysis)ã€‚
            
            ã€è¾“å‡ºè¦æ±‚ã€‘ï¼š
            - å¦‚æœä¿¡æ¯å·²ç»éå¸¸å……åˆ†ï¼Œèƒ½å¤Ÿå›ç­” '{topic}' çš„æ ¸å¿ƒé—®é¢˜ï¼ŒGap è¯·å›å¤ "æ— "ã€‚
            - å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·åœ¨ Gap éƒ¨åˆ†æ˜ç¡®æŒ‡å‡ºä¸‹ä¸€æ­¥éœ€è¦æœä»€ä¹ˆï¼ˆä¾‹å¦‚ï¼šâ€œç¼ºä¹å…·ä½“çš„2024å¹´Q4æˆæœ¬æ•°æ®â€ï¼‰ã€‚
            - ä¿æŒå®¢è§‚ã€ä¸­ç«‹ï¼Œç”¨æ•°æ®è¯´è¯ã€‚
        """).strip()

    @staticmethod
    def publisher_final_report(topic: str, context: str) -> str:
        """[å‡ºç‰ˆè€…] æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆ"""
        return dedent(f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡Œä¸šåˆ†æå¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹ã€å·²æ ¸å®çš„ä¿¡æ¯ç‰‡æ®µã€‘ï¼Œæ’°å†™ä¸€ä»½å…³äº '{topic}' çš„æ·±åº¦ç ”ç©¶æŠ¥å‘Šã€‚
            
            ã€ä¿¡æ¯ç‰‡æ®µã€‘ï¼š
            {context}
            
            ã€æŠ¥å‘Šè¦æ±‚ã€‘ï¼š
            1. æ ¼å¼ï¼šMarkdownã€‚
            2. ç»“æ„ï¼š
               - ğŸ“‘ **æ‘˜è¦** (Executive Summary)
               - ğŸ§­ **ç›®å½•**
               - ğŸ“– **æ­£æ–‡** (åˆ†ç« èŠ‚ï¼Œé€»è¾‘æ¸…æ™°)
               - ğŸ“Š **ç»“è®ºä¸å±•æœ›**
            3. **å¼•ç”¨æ ‡æ³¨** (å…³é”®)ï¼šåœ¨æ­£æ–‡ä¸­å¼•ç”¨å…·ä½“æ•°æ®æˆ–è§‚ç‚¹æ—¶ï¼Œå¿…é¡»åœ¨å¥æœ«æ ‡æ³¨æ¥æºï¼Œæ ¼å¼ä¸º [Source: URL]ã€‚
            4. è¯­æ°”ï¼šå®¢è§‚ã€ä¸“ä¸šã€è¯¦å®ã€‚
            5. å†…å®¹ï¼šé‡ç‚¹å›ç­”ç”¨æˆ·æœ€åˆçš„é—®é¢˜ï¼Œå¹¶æ•´åˆä¹‹å‰åˆ†æä¸­å‘ç°çš„è¡ç”Ÿè§‚ç‚¹ã€‚
        """).strip()
    
    @staticmethod
    def critic_evaluation(topic: str, draft: str) -> str:
        """[æ‰¹è¯„å®¶] è¯„ä¼°è‰ç¨¿è´¨é‡ä¸å¹»è§‰"""
        return dedent(f"""
            ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„å­¦æœ¯å®¡ç¨¿äººã€‚è¯·è¯„ä¼°å…³äº '{topic}' çš„ç ”ç©¶è‰ç¨¿ã€‚
            
            ã€å¾…è¯„è‰ç¨¿ã€‘ï¼š
            {draft}
            
            ã€è¯„ä¼°æ ‡å‡†ã€‘ï¼š
            1. **æ•°æ®æ”¯æ’‘ (30åˆ†)**: æ˜¯å¦æœ‰å…·ä½“çš„æ•°å­—ã€æ—¥æœŸã€å®ä½“ï¼Ÿ(ç©ºæ³›çš„æè¿°æ‰£åˆ†)
            2. **é€»è¾‘é—­ç¯ (30åˆ†)**: ç»“è®ºæ˜¯å¦ç”±è®ºæ®æ¨å¯¼å¾—å‡ºï¼Ÿ
            3. **å›ç­”åˆ‡é¢˜ (20åˆ†)**: æ˜¯å¦è§£å†³äº†ç”¨æˆ·æœ€åˆçš„ç ”ç©¶æ„å›¾ï¼Ÿ
            4. **ä¿¡æ¯æ·±åº¦ (20åˆ†)**: æ˜¯å¦æä¾›äº†è¶…è¶Šå¸¸è¯†çš„æ´å¯Ÿï¼Ÿ
            
            ã€è¾“å‡ºè¦æ±‚ã€‘ï¼š
            è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼ˆä¸è¦è¾“å‡º Markdown ä»£ç å—ï¼Œç›´æ¥è¾“å‡º JSONï¼‰ï¼š
            {{
                "score": <0-10åˆ†ï¼Œæ€»åˆ†/10>,
                "critique": "<å…·ä½“çš„æ‰¹è¯„æ„è§ï¼ŒæŒ‡å‡ºå“ªé‡Œç¼ºæ•°æ®ï¼Œå“ªé‡Œé€»è¾‘ä¸é€š>",
                "adjustment": "<ç»™è§„åˆ’è€…çš„å…·ä½“å»ºè®®ï¼Œä¾‹å¦‚ï¼š'æœç´¢Xå…¬å¸çš„è´¢æŠ¥'ï¼Œ'æŸ¥æ‰¾YæŠ€æœ¯çš„åŸç†å›¾'>"
            }}
        """).strip()
    @staticmethod
    def planner_dag_generation(topic: str, existing_plan_json: str = "[]") -> str:
        """[è§„åˆ’è€…] ç”Ÿæˆæˆ–æ›´æ–° DAG ä»»åŠ¡å›¾"""
        return dedent(f"""
            ä½ æ˜¯ä¸€ä¸ªé«˜çº§é¡¹ç›®ç»ç†ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç ”ç©¶ä¸»é¢˜ '{topic}' æ‹†è§£ä¸ºä¸€ä»½è¯¦ç»†çš„æ‰§è¡Œè®¡åˆ’ã€‚
            
            ã€å½“å‰è®¡åˆ’çŠ¶æ€ã€‘ï¼š
            {existing_plan_json}
            
            ã€ä»»åŠ¡è¦æ±‚ã€‘ï¼š
            1. å¦‚æœè®¡åˆ’ä¸ºç©ºï¼Œè¯·ä»å¤´ç”Ÿæˆä¸€ä»½å®Œæ•´çš„ DAGï¼ˆæœ‰å‘æ— ç¯å›¾ï¼‰è®¡åˆ’ã€‚
            2. ä»»åŠ¡å¿…é¡»å…·æœ‰ä¾èµ–å…³ç³»ã€‚ä¾‹å¦‚ï¼šåˆ†æç«å“ï¼ˆTask Bï¼‰é€šå¸¸ä¾èµ–äºæ”¶é›†æ•°æ®ï¼ˆTask Aï¼‰ã€‚
            3. ä»»åŠ¡æè¿°å¿…é¡»å…·ä½“ï¼ŒåŒ…å«å…·ä½“çš„æœç´¢æ„å›¾ã€‚
            
            ã€è¾“å‡ºæ ¼å¼ã€‘ï¼š
            è¯·ä¸¥æ ¼åªè¿”å›ä¸€ä¸ª JSON åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
            [
                {{
                    "id": "task_1",
                    "description": "æœç´¢ 2024 å¹´å…¨çƒ GPU å¸‚åœºè§„æ¨¡åŠä¸»è¦ç©å®¶",
                    "dependencies": [] 
                }},
                {{
                    "id": "task_2",
                    "description": "æŸ¥æ‰¾ NVIDIA å’Œ AMD æœ€æ–°çš„è´¢æŠ¥æ•°æ®",
                    "dependencies": ["task_1"]
                }}
            ]
            
            æ³¨æ„ï¼š
            - id å¿…é¡»å”¯ä¸€ã€‚
            - dependencies åªèƒ½åŒ…å«å·²å®šä¹‰çš„ idã€‚
            - ä¸è¦ä½¿ç”¨ Markdown ä»£ç å—ï¼Œç›´æ¥è¿”å› JSON å­—ç¬¦ä¸²ã€‚
        """).strip()
    # äº‹å®æå– Prompt
    @staticmethod
    def planner_dag_replanning(topic: str, existing_plan_json: str, feedback: str) -> str:
        """[è§„åˆ’è€…] æ ¹æ® Critic åé¦ˆè¿½åŠ æ–°ä»»åŠ¡"""
        return dedent(f"""
            ä½ æ˜¯ä¸€ä¸ªæ•æ·çš„é¡¹ç›®ç»ç†ã€‚æˆ‘ä»¬å¯¹ '{topic}' çš„åˆæ­¥ç ”ç©¶æ”¶åˆ°äº†æ‰¹è¯„åé¦ˆï¼Œéœ€è¦è¡¥å……è°ƒæŸ¥ã€‚
            
            ã€å½“å‰å·²å®Œæˆçš„è®¡åˆ’ã€‘ï¼š
            {existing_plan_json}
            
            ã€æ‰¹è¯„ä¸å»ºè®®ã€‘ï¼š
            {feedback}
            
            ã€ä»»åŠ¡ã€‘ï¼š
            è¯·æ ¹æ®å»ºè®®ï¼Œç”Ÿæˆ **1-3 ä¸ªæ–°çš„è¡¥æ•‘ä»»åŠ¡**ï¼Œæ·»åŠ åˆ°ç°æœ‰çš„ DAG ä¸­ã€‚
            
            ã€è¦æ±‚ã€‘ï¼š
            1. æ–°ä»»åŠ¡çš„ ID å¿…é¡»ä¸ä¸ç°æœ‰ ID å†²çªï¼ˆå»ºè®®ä½¿ç”¨ "fix_task_1", "fix_task_2" ç­‰ï¼‰ã€‚
            2. æ–°ä»»åŠ¡å¯ä»¥ä¾èµ–ç°æœ‰çš„å·²å®Œæˆä»»åŠ¡ã€‚
            3. åªè¿”å›æ–°å¢ä»»åŠ¡çš„ JSON åˆ—è¡¨ã€‚
            
            ã€è¾“å‡ºæ ¼å¼ç¤ºä¾‹ã€‘ï¼š
            [
                {{
                    "id": "fix_data_2024",
                    "description": "æœç´¢æœ€æ–°çš„2024å¹´Q4è¥æ”¶æ•°æ®ä»¥ä¿®æ­£è¿‡æ—¶ä¿¡æ¯",
                    "dependencies": ["task_1"]
                }}
            ]
        """).strip()
    @staticmethod
    def verification_claims_extraction(text: str) -> str:
        """[éªŒè¯è€…] æå–äº‹å®æ–­è¨€"""
        return dedent(f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº‹å®æ ¸æŸ¥å‘˜ã€‚è¯·ä»ä¸‹é¢çš„æ–‡æœ¬ä¸­æå– 3-5 ä¸ªæœ€å…³é”®çš„ã€åŒ…å«å…·ä½“æ•°æ®æˆ–ç¡®å®šæ€§é™ˆè¿°çš„äº‹å®æ–­è¨€ã€‚
            
            ã€å¾…æå–æ–‡æœ¬ã€‘ï¼š
            {text}
            
            ã€æå–è¦æ±‚ã€‘ï¼š
            1. **åªæå–å®¢è§‚äº‹å®**ï¼šå¦‚é‡‘é¢ã€å¢é•¿ç‡ã€å…·ä½“æ—¥æœŸã€å®ä½“å…³ç³»ã€å¹¶è´­äº‹ä»¶ã€‚
            2. **å®ç¼ºæ¯‹æ»¥**ï¼šåªæå–åŒ…å«**å…·ä½“æ•°å­—ã€æ—¥æœŸã€ç‰¹å®šå®ä½“è¡Œä¸ºã€æ˜ç¡®å› æœå…³ç³»**çš„å¥å­ã€‚
            3. **å¿½ç•¥ç©ºæ³›å†…å®¹**ï¼šå¦‚æœè¯¥ç‰‡æ®µåªæ˜¯ä»‹ç»èƒŒæ™¯ã€è¿‡æ¸¡è¯­å¥æˆ–ä¸»è§‚è¯„è®ºï¼Œä¸åŒ…å«ç¡¬æ ¸äº‹å®ï¼Œè¯·è¿”å›ç©ºåˆ—è¡¨ã€‚
            4. **å¿½ç•¥ä¸»è§‚è§‚ç‚¹**ï¼šå¿½ç•¥â€œæˆ‘è®¤ä¸ºâ€ã€â€œå¯èƒ½â€ã€â€œé¢„è®¡â€ç­‰æ¨¡ç³Šæè¿°ã€‚
            5. **åŸå­åŒ–**ï¼šæ¯ä¸ªæ–­è¨€åº”è¯¥æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å¥å­ã€‚
            
            ã€è¾“å‡ºæ ¼å¼ã€‘ï¼š
            è¯·ä¸¥æ ¼ä»…è¿”å› JSON åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
            [
                {{
                    "original_text": "åŸæ–‡ç‰‡æ®µ...", 
                    "claim": "é‡å†™åçš„ç‹¬ç«‹äº‹å®é™ˆè¿°"
                }}
            ]
        """).strip()

    # äº‹å®æ ¸æŸ¥ Prompt
    @staticmethod
    def verification_claim_check(claim: str, context: str) -> str:
        """[éªŒè¯è€…] åŸºäºæœç´¢ç»“æœæ ¸æŸ¥æ–­è¨€"""
        return dedent(f"""
            è¯·æ ¹æ®æä¾›çš„æœç´¢è¯æ®ï¼ŒéªŒè¯ä»¥ä¸‹æ–­è¨€çš„çœŸå®æ€§ã€‚
            
            ã€å¾…éªŒè¯æ–­è¨€ã€‘ï¼š
            {claim}
            
            ã€æœç´¢åˆ°çš„è¯æ®ã€‘ï¼š
            {context}
            
            ã€åˆ¤å®šé€»è¾‘ã€‘ï¼š
            - **Verified (å·²éªŒè¯)**: è¯æ®ä¸­æœ‰æ˜ç¡®æ•°æ®æ”¯æŒè¯¥æ–­è¨€ã€‚
            - **Disputed (æœ‰äº‰è®®)**: è¯æ®ä¸­çš„æ•°æ®/äº‹å®ä¸æ–­è¨€ç›´æ¥å†²çªï¼ˆä¾‹å¦‚ï¼šæ–­è¨€è¯´å¢é•¿50%ï¼Œè¯æ®è¯´ä¸‹é™10%ï¼‰ã€‚
            - **Unconfirmed (æœªç¡®è®¤)**: è¯æ®ä¸è¶³ï¼Œæˆ–è¯æ®ä¸æ–­è¨€æ— å…³ã€‚
            
            ã€è¾“å‡ºæ ¼å¼ã€‘ï¼š
            è¯·ä¸¥æ ¼ä»…è¿”å› JSON å¯¹è±¡ï¼š
            {{
                "status": "Verified" | "Disputed" | "Unconfirmed",
                "explanation": "ä¸€å¥è¯è§£é‡Šç†ç”±ï¼Œå¼•ç”¨è¯æ®ä¸­çš„æ¥æºï¼ˆå¦‚æœæœ‰ï¼‰"
            }}
        """).strip()
    # ğŸŸ¢ æ–°å¢ï¼šMAD è¾©è®ºç›¸å…³æç¤ºè¯
    @staticmethod
    def debate_argument(topic: str, stance: str, context: str) -> str:
        """[è¾©æ‰‹] ç”Ÿæˆè¾©è®ºé™ˆè¿°"""
        return dedent(f"""
            ä½ æ­£åœ¨å‚ä¸ä¸€åœºå…³äº '{topic}' çš„ä¸¥è‚ƒè¾©è®ºã€‚
            
            ã€ä½ çš„ç«‹åœºã€‘ï¼š{stance} (è¯·æåŠ›å¯»æ‰¾è¯æ®æ”¯æŒæ­¤ç«‹åœºï¼Œæˆ–åé©³å¯¹æ–¹)
            
            ã€å¯ç”¨è¯æ®ã€‘ï¼š
            {context}
            
            ã€è¦æ±‚ã€‘ï¼š
            1. é€»è¾‘ä¸¥å¯†ï¼Œé’ˆé”‹ç›¸å¯¹ã€‚
            2. å¿…é¡»æ˜¾å¼å¼•ç”¨è¯æ®ï¼ˆå¦‚ "æ ¹æ®[Source 1]..."ï¼‰ã€‚
            3. ä¿æŒç®€çŸ­æœ‰åŠ›ï¼ˆ200å­—ä»¥å†…ï¼‰ã€‚
            
            è¯·è¾“å‡ºä½ çš„è¾©è®ºé™ˆè¿°ï¼š
        """).strip()

    @staticmethod
    def debate_judgment(topic: str, affirmative_arg: str, negative_arg: str) -> str:
        """[æ³•å®˜] æœ€ç»ˆè£å†³"""
        return dedent(f"""
            ä½œä¸ºä¸­ç«‹çš„å¤§æ³•å®˜ï¼Œè¯·æ ¹æ®åŒæ–¹è¾©è¯å¯¹ '{topic}' åšå‡ºçœŸç†è£å†³ã€‚
            
            ã€æ­£æ–¹è§‚ç‚¹ã€‘ï¼š
            {affirmative_arg}
            
            ã€åæ–¹è§‚ç‚¹ã€‘ï¼š
            {negative_arg}
            
            ã€åˆ¤å†³æ ‡å‡†ã€‘ï¼š
            1. è°çš„è¯æ®æ›´æƒå¨ã€æ›´æ–°ï¼Ÿ
            2. è°çš„é€»è¾‘é“¾æ¡æ›´å®Œæ•´ï¼Ÿ
            3. å¿½ç•¥ä¿®è¾æŠ€å·§ï¼Œåªçœ‹äº‹å®å¯†åº¦ã€‚
            
            ã€è¾“å‡ºæ ¼å¼ã€‘ï¼š
            è¯·è¿”å› JSONï¼š
            {{
                "winner": "Affirmative" | "Negative" | "Uncertain",
                "conclusion": "æœ€ç»ˆè®¤å®šçš„äº‹å®ç»“è®º",
                "reasoning": "åˆ¤å†³ç†ç”±"
            }}
        """).strip()
# å®ä¾‹åŒ–ï¼ˆå¦‚æœéœ€è¦å•ä¾‹ï¼Œæˆ–è€…ç›´æ¥ç”¨é™æ€æ–¹æ³•ï¼‰
prompts = ResearchPrompts()
</file>

<file path="app/modules/knowledge/__init__.py">
"""
çŸ¥è¯†å¼•æ“æ¨¡å— - æ•°æ®å­˜å‚¨ã€ç´¢å¼•å’Œæ£€ç´¢

è´Ÿè´£ç³»ç»Ÿä¸­æ‰€æœ‰æ•°æ®çš„æŒä¹…åŒ–ã€ç´¢å¼•å’Œç®¡ç†ã€‚

æœ¬æ¨¡å—æä¾›å¤šå±‚æ¬¡çš„æ•°æ®å­˜å‚¨è§£å†³æ–¹æ¡ˆï¼š
    - å‘é‡æ•°æ®åº“å­˜å‚¨ï¼šç”¨äºè¯­ä¹‰æœç´¢å’Œç›¸ä¼¼åº¦åŒ¹é…
    - å…³ç³»å‹æ•°æ®åº“å­˜å‚¨ï¼šç”¨äºç»“æ„åŒ–æ•°æ®å’Œäº‹åŠ¡ç®¡ç†
    - æ··åˆå­˜å‚¨æ¨¡å¼ï¼šç»“åˆä¸¤ç§æ•°æ®åº“çš„ä¼˜åŠ¿

å­æ¨¡å—:
    - vector: å‘é‡æ•°æ®åº“ (LanceDB)
        æä¾›é«˜æ•ˆå‘é‡å­˜å‚¨å’Œç›¸ä¼¼åº¦æ£€ç´¢
        æ”¯æŒæ–‡æœ¬åµŒå…¥ã€å›¾åƒå‘é‡ç­‰å¤šæ¨¡æ€æ•°æ®
        å®ç°è¯­ä¹‰æœç´¢å’Œæ™ºèƒ½æ¨èåŠŸèƒ½

    - rdb: å…³ç³»å‹æ•°æ®åº“ (SQLModel)
        æä¾›ç»“æ„åŒ–æ•°æ®å­˜å‚¨å’Œç®¡ç†
        æ”¯æŒå¤æ‚æŸ¥è¯¢ã€äº‹åŠ¡å¤„ç†å’Œæ•°æ®å®Œæ•´æ€§
        ç®¡ç†ç”¨æˆ·æ•°æ®ã€ç³»ç»Ÿé…ç½®ç­‰ç»“æ„åŒ–ä¿¡æ¯

ä¸»è¦åŠŸèƒ½:
    1. å¤šæ¨¡æ€æ•°æ®å­˜å‚¨ (æ–‡æœ¬ã€å›¾åƒã€å‘é‡)
    2. é«˜æ•ˆç›¸ä¼¼åº¦æœç´¢å’Œæ£€ç´¢
    3. ç»“æ„åŒ–æ•°æ®ç®¡ç†å’ŒæŸ¥è¯¢
    4. æ•°æ®ç´¢å¼•ä¼˜åŒ–
    5. æ•°æ®å¤‡ä»½å’Œæ¢å¤
    6. æ”¯æŒæ°´å¹³æ‰©å±•

æ•°æ®æµ:
    æ„ŸçŸ¥æ¨¡å—æ”¶é›†æ•°æ® â†’ çŸ¥è¯†æ¨¡å—å­˜å‚¨å’Œç´¢å¼• â†’ å…¶ä»–æ¨¡å—æ£€ç´¢å’Œä½¿ç”¨

ä½¿ç”¨ç¤ºä¾‹:
    from app.modules.knowledge import vector, rdb

ä½œè€…: ApexBridge Team
ç‰ˆæœ¬: 1.0.0
"""

from . import vector, rdb

# æ˜ç¡®åˆ—å‡ºæ‰€æœ‰å…¬å¼€çš„å­æ¨¡å—
__all__ = [
    "vector",
    "rdb",
]
</file>

<file path="app/modules/knowledge/vector.py">
# app/modules/knowledge/vector.py
import lancedb
import os
import time
import pyarrow as pa
from typing import List, Dict
from litellm import embedding
from langchain_text_splitters import RecursiveCharacterTextSplitter
from app.core.config import settings

# 1. åˆå§‹åŒ– LanceDB (æœ¬åœ°æ–‡ä»¶æ¨¡å¼)
DB_PATH = settings.LANCEDB_PATH
os.makedirs(DB_PATH, exist_ok=True)
db = lancedb.connect(DB_PATH)

# å®šä¹‰è¡¨ç»“æ„ (Schema) - åŠ¨æ€å‘é‡ç»´åº¦
# æ ¹æ® EMBEDDING_MODEL å’Œ EMBEDDING_DIMENSION é…ç½®
schema = pa.schema([
    pa.field("vector", pa.list_(pa.float32(), settings.EMBEDDING_DIMENSION)),
    pa.field("text", pa.string()),
    pa.field("source", pa.string()),
    pa.field("chunk_id", pa.string()),
    pa.field("model", pa.string()),
    pa.field("task_id", pa.string())
])

def get_embedding(text: str) -> List[float]:
    """
    ä½¿ç”¨é…ç½®çš„åµŒå…¥æ¨¡å‹è·å–å‘é‡
    æ”¯æŒå¤šç§æ¨¡å‹ï¼šOllamaæœ¬åœ°æ¨¡å‹ã€äº‘ç«¯APIç­‰
    """
    try:
        response = embedding(
            model=settings.EMBEDDING_MODEL,
            input=[text]
        )
        return response.data[0]['embedding']

    except Exception as e:
        print(f"âŒ Embedding Error ({settings.EMBEDDING_MODEL}): {e}")
        return [0.0] * settings.EMBEDDING_DIMENSION

class KnowledgeBase:
    def __init__(self, table_name: str = "research_context"):
        self.table_name = table_name
        
        # ğŸŸ¢ é²æ£’æ€§å¢å¼ºï¼šç»´åº¦å…¼å®¹æ€§æ£€æŸ¥ä¸è‡ªåŠ¨è¿ç§»
        try:
            # å°è¯•æ‰“å¼€ç°æœ‰è¡¨
            self.table = db.open_table(table_name)
            
            # æ£€æŸ¥ schema ä¸­çš„å‘é‡ç»´åº¦
            # PyArrow çš„ FixedSizeListType å…·æœ‰ list_size å±æ€§
            vec_field = self.table.schema.field("vector")
            existing_dim = vec_field.type.list_size
            
            if existing_dim != settings.EMBEDDING_DIMENSION:
                print(f"âš ï¸ [Knowledge] Dimension mismatch detected! Table: {existing_dim}, Config: {settings.EMBEDDING_DIMENSION}")
                
                # å¤‡ä»½æ—§è¡¨ (é‡å‘½å)
                backup_name = f"{table_name}_backup_{int(time.time())}"
                try:
                    db.rename_table(table_name, backup_name)
                    print(f"ğŸ“¦ Archived old table to '{backup_name}'.")
                except Exception as rename_err:
                    print(f"âš ï¸ Failed to rename table: {rename_err}")
                
                # åˆ›å»ºæ–°è¡¨
                print("ğŸ†• Creating new table with correct dimension...")
                self.table = db.create_table(table_name, schema=schema)
                
        except Exception:
            # å¦‚æœè¡¨ä¸å­˜åœ¨ï¼Œç›´æ¥åˆ›å»º
            # print(f"â„¹ï¸ Table '{table_name}' not found, creating new one.")
            self.table = db.create_table(table_name, schema=schema)

    def add_documents(self, documents: List[Dict], task_id: str):
        """
        æ¥æ”¶çˆ¬å–ç»“æœ -> åˆ‡ç‰‡ -> å‘é‡åŒ– -> å­˜å…¥
        """
        # æ ¹æ®åµŒå…¥æ¨¡å‹è°ƒæ•´ chunk_size
        if "openai" in settings.EMBEDDING_MODEL:
            chunk_size = 800
        else:
            chunk_size = 1200

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=200
        )

        data_to_insert = []

        print(f"ğŸ’¾ [Knowledge] Processing {len(documents)} docs with {settings.EMBEDDING_MODEL} (Dim: {settings.EMBEDDING_DIMENSION})...")

        for doc in documents:
            clean_content = doc["content"].replace("\n\n\n", "\n")
            chunks = splitter.split_text(clean_content)

            for idx, chunk in enumerate(chunks):
                vec = get_embedding(chunk)
                if len(vec) == settings.EMBEDDING_DIMENSION:
                    data_to_insert.append({
                        "vector": vec,
                        "text": chunk,
                        "source": doc.get("source", "unknown"),
                        "chunk_id": f"{doc.get('url', 'unknown')}_{idx}",
                        "model": settings.EMBEDDING_MODEL,
                        "task_id": task_id
                    })

        if data_to_insert:
            self.table.add(data_to_insert)
            print(f"âœ… [Knowledge] Inserted {len(data_to_insert)} chunks (Dim: {settings.EMBEDDING_DIMENSION})")

    def search(self, query: str, task_id: str, limit: int = 5) -> str:
        """
        è¯­ä¹‰æ£€ç´¢
        """
        print(f"ğŸ” [Retrieval] Searching for: {query[:30]}...")
        try:
            query_vec = get_embedding(query)
            results = self.table.search(query_vec).where(f"task_id = '{task_id}'").limit(limit).to_list()
            
            context = ""
            for item in results:
                context += f"--- Source: {item['source']} ---\n{item['text']}\n\n"
                
            print(f"âœ… [Retrieval] Found {len(results)} relevant chunks")
            return context
        except Exception as e:
            print(f"âš ï¸ Retrieval failed: {e}")
            return ""
        
    def clear_task_data(self, task_id: str):
        try:
            self.table.delete(f"task_id = '{task_id}'")
            print(f"ğŸ§¹ [Knowledge] Cleared vectors for task: {task_id}")
        except Exception as e:
            print(f"âš ï¸ Failed to clear task data: {e}")
</file>

<file path="app/modules/orchestrator/__init__.py">
"""
ç¼–æ’å™¨æ¨¡å— - å·¥ä½œæµç¼–æ’å™¨

ä½¿ç”¨LangGraphå®ç°çŠ¶æ€æœºå’Œå·¥ä½œæµç¼–æ’ã€‚

æœ¬æ¨¡å—è´Ÿè´£ï¼š
    - å®šä¹‰å’Œç®¡ç†å¤æ‚çš„å·¥ä½œæµçŠ¶æ€
    - å®ç°ä»»åŠ¡é—´çš„ä¾èµ–å…³ç³»å’Œè°ƒåº¦
    - åè°ƒå„ä¸ªæ¨¡å—ä¹‹é—´çš„äº¤äº’
    - æä¾›å¯æ‰©å±•çš„çŠ¶æ€è½¬æ¢é€»è¾‘

å­æ¨¡å—:
    - graph: å›¾ç»“æ„å’Œå·¥ä½œæµå®šä¹‰
        åŒ…å«LangGraphèŠ‚ç‚¹å’Œè¾¹çš„å®šä¹‰

    - state: çŠ¶æ€ç®¡ç†
        å®šä¹‰å…±äº«çŠ¶æ€ç»“æ„å’ŒçŠ¶æ€è½¬æ¢é€»è¾‘

å·¥ä½œæµç¨‹:
    1. åˆå§‹åŒ–å·¥ä½œæµçŠ¶æ€
    2. é€šè¿‡graphå®šä¹‰æ‰§è¡Œè·¯å¾„
    3. ä½¿ç”¨stateè·Ÿè¸ªå’Œç®¡ç†çŠ¶æ€å˜åŒ–
    4. åè°ƒå„æ¨¡å—æŒ‰åºæ‰§è¡Œ

ä½¿ç”¨ç¤ºä¾‹:
    from app.modules.orchestrator import graph, state

ä½œè€…: ApexBridge Team
ç‰ˆæœ¬: 1.0.0
"""

from . import graph, state

# çš„å­æ˜ç¡®åˆ—å‡ºæ‰€æœ‰å…¬å¼€æ¨¡å—
__all__ = [
    "graph",
    "state",
]
</file>

<file path="app/modules/orchestrator/graph.py">
# app/modules/orchestrator/graph.py

from langgraph.graph import StateGraph, END
import sqlite3
import json
import re
import asyncio 
import numpy as np

from app.core.config import settings
from app.modules.orchestrator.state import ResearchState, ReflectionLog
from app.modules.orchestrator.dag import DAGManager, TaskStatus
from app.modules.perception.search import search_generic as search_tool
from app.modules.perception.crawler import crawl_urls
from app.core.llm import simple_llm_call
from app.modules.knowledge.vector import KnowledgeBase, get_embedding
from app.modules.insight.prompts import ResearchPrompts
from app.modules.verification.verification_agent import VerificationAgent

kb = KnowledgeBase()

def log_step(step_name: str, content: dict):
    """æ ¼å¼åŒ–æ‰“å°æ—¥å¿—"""
    print(f"\nğŸš€ [Step: {step_name}]")
    log_content = content.copy()
    if "plan" in log_content:
        log_content["plan"] = f"[DAG with {len(log_content['plan'])} tasks]"
    print(json.dumps(log_content, indent=2, ensure_ascii=False, default=str))
    print("-" * 50)

def parse_critic_json(text: str) -> dict:
    try:
        match = re.search(r"\{.*\}", text, re.DOTALL)
        if match:
            return json.loads(match.group(0))
        return json.loads(text)
    except:
        return {"score": 5, "critique": "è§£æå¤±è´¥", "adjustment": "è¯·è¡¥å……æ•°æ®"}

def parse_dag_json(text: str) -> list:
    try:
        clean_text = text.replace("```json", "").replace("```", "").strip()
        return json.loads(clean_text)
    except:
        print(f"âŒ JSON Parse Error in DAG: {text[:100]}...")
        return []

def cosine_similarity(v1, v2):
    if not v1 or not v2: return 0.0
    vec1 = np.array(v1)
    vec2 = np.array(v2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0: return 0.0
    return np.dot(vec1, vec2) / (norm1 * norm2)

# --- èŠ‚ç‚¹é€»è¾‘ ---

async def node_planner(state: ResearchState):
    """[è§„åˆ’è€…] é›†æˆè¯­ä¹‰å»é‡ç†”æ–­æœºåˆ¶"""
    print(f"--- [Planner] Scheduling Tasks (Model: {settings.MODEL_PLANNER}) ---")
    dag = DAGManager(state["plan"])
    model_to_use = settings.MODEL_PLANNER
    
    # 1. å‡†å¤‡å†å²ä»»åŠ¡å‘é‡
    history_descriptions = [
        t.description for t in dag.tasks.values() 
        if t.status in [TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.SKIPPED]
    ]
    history_vecs = []
    if history_descriptions:
        print(f"ğŸ§  [Soft Limits] Loading vectors for {len(history_descriptions)} past tasks...")
        history_vecs = [get_embedding(desc) for desc in history_descriptions]

    def filter_duplicate_tasks(new_tasks_list):
        unique_tasks = []
        for task in new_tasks_list:
            desc = task.get("description", "")
            if not desc: continue
            if not history_vecs:
                unique_tasks.append(task)
                continue
            new_vec = get_embedding(desc)
            is_duplicate = False
            for idx, h_vec in enumerate(history_vecs):
                sim = cosine_similarity(new_vec, h_vec)
                if sim > 0.85:
                    print(f"ğŸ›‘ [Circuit Breaker] Semantic Loop Detected (Sim: {sim:.2f})!")
                    print(f"   Rejected Task: {desc}")
                    is_duplicate = True
                    break
            if not is_duplicate:
                unique_tasks.append(task)
        return unique_tasks

    has_feedback = False
    new_tasks_raw = []

    if state["reflection_logs"]:
        last_log = state["reflection_logs"][-1]
        if last_log["score"] < 8.0:
            print(f"ğŸ”„ [Planner] Handling Critique: {last_log['critique'][:50]}...")
            has_feedback = True
            feedback_str = f"æ‰¹è¯„: {last_log['critique']}\nå»ºè®®: {last_log['adjustment']}"
            existing_plan_str = json.dumps(dag.to_state(), ensure_ascii=False)
            prompt = ResearchPrompts.planner_dag_replanning(state["task"], existing_plan_str, feedback_str)
            response = await simple_llm_call(prompt, model=model_to_use)
            new_tasks_raw = parse_dag_json(response)

    if not dag.tasks and not has_feedback:
        print("ğŸ“ [Planner] Generating initial DAG plan...")
        prompt = ResearchPrompts.planner_dag_generation(state["task"])
        response = await simple_llm_call(prompt, model=model_to_use)
        new_tasks_raw = parse_dag_json(response)

    if new_tasks_raw:
        print(f"ğŸ” [Soft Limits] Checking {len(new_tasks_raw)} new tasks for semantic loops...")
        final_tasks = filter_duplicate_tasks(new_tasks_raw)
        if len(final_tasks) < len(new_tasks_raw):
            print(f"ğŸ›¡ï¸ [Soft Limits] Filtered out {len(new_tasks_raw) - len(final_tasks)} redundant tasks.")
        for t in final_tasks:
            try:
                dag.add_task(t["id"], t["description"], t.get("dependencies", []))
            except ValueError as e:
                print(f"âš ï¸ Add task error: {e}")
    else:
        if has_feedback:
            print("âš ï¸ [Planner] No new valid tasks generated after feedback.")

    ready_tasks = dag.get_ready_tasks()
    current_queries = []
    for t in ready_tasks:
        dag.set_task_running(t.id)
        current_queries.append(t.description)
    
    if current_queries:
        print(f"ğŸš€ [Planner] Dispatching {len(current_queries)} tasks")
    else:
        print(f"ğŸ’¤ [Planner] No ready tasks.")

    result = {"plan": dag.to_state(), "search_queries": current_queries}
    log_step("Planner", result)
    return result

async def node_search_execute(state: ResearchState):
    """[æ‰§è¡Œè€…] é²æ£’æ€§å¢å¼ºç‰ˆ"""
    dag = DAGManager(state["plan"])
    running_tasks = [t for t in dag.tasks.values() if t.status == TaskStatus.RUNNING]
    
    if not running_tasks:
        return {}

    print(f"--- [Search] Parallel Execution: {len(running_tasks)} tasks ---")
    
    search_coros = []
    for t in running_tasks:
        async def safe_search(task_id, query):
            try:
                return await search_tool(query, num_results=2)
            except Exception as e:
                print(f"âŒ Task {task_id} hard failed: {e}")
                return e 
        search_coros.append(safe_search(t.id, t.description))

    search_results_list = await asyncio.gather(*search_coros)
    
    crawl_coros = []
    for res in search_results_list:
        if isinstance(res, list) and res:
            urls = [item["url"] for item in res]
            crawl_coros.append(crawl_urls(urls))
        else:
            crawl_coros.append(asyncio.sleep(0))
            
    if crawl_coros:
        crawl_results_list = await asyncio.gather(*crawl_coros)
    else:
        crawl_results_list = [[] for _ in running_tasks]

    all_new_docs = []
    for i, task in enumerate(running_tasks):
        search_res = search_results_list[i]
        if isinstance(search_res, Exception):
            dag.fail_task(task.id, str(search_res))
            continue
            
        crawl_res = crawl_results_list[i] if i < len(crawl_results_list) else []
        if isinstance(crawl_res, int): crawl_res = []

        if crawl_res:
            all_new_docs.extend(crawl_res)
            dag.complete_task(task.id, result=f"Scraped {len(crawl_res)} pages.")
        else:
            dag.complete_task(task.id, result="No content found.")

    if all_new_docs:
        kb.add_documents(all_new_docs)
    
    dag.get_ready_tasks() 

    return {"plan": dag.to_state(), "web_results": all_new_docs}

async def node_analyst(state: ResearchState):
    """[åˆ†æå¸ˆ] ä½¿ç”¨å†™ä½œæ¨¡å‹ (MODEL_WRITER)"""
    print(f"--- [Analyst] Thinking (Model: {settings.MODEL_WRITER}) ---")
    topic = state["topic"]
    model_to_use = settings.MODEL_WRITER
    
    query = topic
    if state["reflection_logs"]:
        query += f" {state['reflection_logs'][-1]['adjustment']}"

    context = kb.search(query, task_id=state["task_id"], limit=15)

    # ğŸŸ¢ é²æ£’æ€§å¢å¼ºï¼šå¤„ç†ç©ºæœç´¢ç»“æœ
    # é˜²æ­¢ Prompt æ¥æ”¶ç©ºå­—ç¬¦ä¸²å¯¼è‡´å¹»è§‰
    if not context or len(context.strip()) < 10:
        print("âš ï¸ [Analyst] No valid context found in KnowledgeBase.")
        context = (
            "ã€ç³»ç»Ÿè­¦å‘Šã€‘ï¼šæœ¬è½®æœç´¢æœªèƒ½è·å–æœ‰æ•ˆäº’è”ç½‘æ•°æ®ï¼ˆå¯èƒ½æ˜¯ç”±äºåçˆ¬è™«é™åˆ¶æˆ–ç½‘ç»œé—®é¢˜ï¼‰ã€‚"
            "è¯·åœ¨æŠ¥å‘Šä¸­æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ï¼š'ç”±äºæ— æ³•è¿æ¥å¤–éƒ¨æ•°æ®æºï¼Œä»¥ä¸‹åˆ†æä»…åŸºäºå¸¸è¯†å’Œé€»è¾‘æ¨æ¼”ï¼Œå¯èƒ½ç¼ºä¹å®æ—¶æ•°æ®æ”¯æ’‘ã€‚'"
        )

    prompt = ResearchPrompts.analyst_reasoning(topic, context)
    
    raw_draft = await simple_llm_call(prompt, model=model_to_use)
    
    print("ğŸ›¡ï¸ [Analyst] Running Fact Verification...")
    verified_draft = await VerificationAgent.verify_report(raw_draft)
    
    result = {"draft_report": verified_draft}
    log_step("Analyst", result)
    return result

async def node_critic(state: ResearchState):
    """[æ‰¹è¯„å®¶] ä½¿ç”¨æ‰¹åˆ¤æ¨¡å‹ (MODEL_CRITIC)"""
    print(f"--- [Critic] Reviewing (Model: {settings.MODEL_CRITIC}) ---")
    topic = state["topic"]
    draft = state["draft_report"]
    model_to_use = settings.MODEL_CRITIC
    
    prompt = ResearchPrompts.critic_evaluation(topic, draft)
    response = await simple_llm_call(prompt, model=model_to_use)
    eval_data = parse_critic_json(response)
    
    log: ReflectionLog = {
        "step_name": f"Iter-{state['iteration_count']}",
        "critique": eval_data.get("critique", ""),
        "score": float(eval_data.get("score", 0)),
        "adjustment": eval_data.get("adjustment", "")
    }
    
    result = {"reflection_logs": [log]}
    log_step("Critic", result)
    return result

async def node_publisher(state: ResearchState):
    """[å‡ºç‰ˆè€…] ä½¿ç”¨å†™ä½œæ¨¡å‹ (MODEL_WRITER)"""
    print(f"--- [Publisher] (Model: {settings.MODEL_WRITER}) ---")
    topic = state["topic"]
    context = kb.search(topic, task_id=state["task_id"], limit=30) 
    model_to_use = settings.MODEL_WRITER
    
    prompt = ResearchPrompts.publisher_final_report(topic, context)
    final_report = await simple_llm_call(prompt, model=model_to_use)
    kb.clear_task_data(state["task_id"])
    return {"final_report": final_report}

# --- è·¯ç”±é€»è¾‘ ---
def route_planner(state: ResearchState) -> str:
    dag = DAGManager(state["plan"])
    running_tasks = [t for t in dag.tasks.values() if t.status == TaskStatus.RUNNING]
    if running_tasks: return "searcher"
    elif dag.is_all_completed(): return "analyst"
    else: 
        print("âš ï¸ [Router] No tasks running but DAG not complete. Moving to Analyst.")
        return "analyst"

def route_critic(state: ResearchState) -> str:
    if state["iteration_count"] >= state["max_iterations"]: return "publish"
    last_log = state["reflection_logs"][-1]
    if last_log["score"] >= 8.0: return "publish"
    else: return "planner"

def build_graph():
    workflow = StateGraph(ResearchState)
    workflow.add_node("planner", node_planner)
    workflow.add_node("searcher", node_search_execute)
    workflow.add_node("analyst", node_analyst)
    workflow.add_node("critic", node_critic)
    workflow.add_node("publisher", node_publisher)
    
    workflow.set_entry_point("planner")
    workflow.add_conditional_edges("planner", route_planner, {"searcher": "searcher", "analyst": "analyst"})
    workflow.add_edge("searcher", "planner")
    workflow.add_edge("analyst", "critic")
    workflow.add_conditional_edges("critic", route_critic, {"planner": "planner", "publish": "publisher"})
    workflow.add_edge("publisher", END)

    return workflow
</file>

<file path="app/modules/orchestrator/state.py">
from typing import List, TypedDict, Annotated, Optional, Literal, Union
import operator

# --- 1. åŸºç¡€æ•°æ®æ¨¡å‹ (Sub-models) ---

class ResearchStep(TypedDict):
    """
    å•æ­¥ç ”ç©¶è®¡åˆ’ (DAG èŠ‚ç‚¹)
    ç”¨äº Plan-and-Solve ç­–ç•¥ï¼Œæ”¯æŒä»»åŠ¡åˆ†è§£
    """
    id: str                 # æ­¥éª¤ID (e.g., "step_1")
    description: str        # æ­¥éª¤æè¿° (e.g., "æœç´¢2024å¹´å…¨çƒAIå¸‚åœºè§„æ¨¡æ•°æ®")
    status: Literal["pending", "running", "completed", "failed"] 
    dependencies: List[str] # ä¾èµ–çš„å‰ç½®æ­¥éª¤ID (æ”¯æŒ DAG)
    result: Optional[str]   # è¯¥æ­¥éª¤çš„æ‰§è¡Œç»“æœæ‘˜è¦

class ContextNode(TypedDict):
    """
    çŸ¥è¯†å›¾è°±èŠ‚ç‚¹
    æ¯”çº¯æ–‡æœ¬æ›´ç»“æ„åŒ–ï¼Œä¾¿äºåç»­çš„å¼•ç”¨å’Œå†²çªæ¶ˆè§£
    """
    id: str                 # å”¯ä¸€ID
    content: str            # æ ¸å¿ƒäº‹å®ç‰‡æ®µ
    source_url: str         # æ¥æº
    type: Literal["fact", "data", "opinion", "snippet"] # çŸ¥è¯†ç±»å‹
    confidence: float       # ç½®ä¿¡åº¦ (0.0 - 1.0)

class ReflectionLog(TypedDict):
    """
    åæ€æ—¥å¿—
    ç”¨äº Reflexion æœºåˆ¶ï¼Œè®°å½• Critic çš„è¯„ä»·
    """
    step_name: str          # åœ¨å“ªä¸ªç¯èŠ‚äº§ç”Ÿçš„åæ€
    critique: str           # æ‰¹è¯„æ„è§
    score: float            # è¯„åˆ† (0-10)
    adjustment: str         # å…·ä½“çš„æ”¹è¿›æªæ–½ (e.g., "ä¸‹æ¬¡æœç´¢éœ€è¦æ·»åŠ 'PDF'å…³é”®è¯")

# (ä¿ç•™) æ—§çš„æœç´¢ç»“æœç»“æ„ï¼Œç”¨äºå…¼å®¹ crawler.py
class SearchResult(TypedDict):
    url: str
    content: str
    source: str

# --- 2. æ ¸å¿ƒçŠ¶æ€å®šä¹‰ (Main State) ---

class ResearchState(TypedDict):
    """
    Deep Research V2 æ ¸å¿ƒçŠ¶æ€ (æ¸…ç†ç‰ˆ)
    """
    # --- æ ¸å¿ƒç»´åº¦ ---
    task_id: str
    task: str
    clarified_intent: str
    plan: List[ResearchStep]
    knowledge_graph: Annotated[List[ContextNode], operator.add]
    reflection_logs: Annotated[List[ReflectionLog], operator.add]
    iteration_count: int
    max_iterations: int
    
    # --- å¿…éœ€çš„ä¸­é—´å˜é‡ ---
    topic: str              # TODO: æœªæ¥å°† graph.py ä¸­çš„ state["topic"] æ›¿æ¢ä¸º state["task"] ååˆ é™¤
    draft_report: str
    final_report: str
    
    # --- è¿™é‡Œçš„ Annotated åˆ—è¡¨å¦‚æœä¸åˆå§‹åŒ– inputsï¼ŒLangGraph ä¼šè‡ªåŠ¨å¤„ç†ä¸ºç©ºåˆ—è¡¨
    # ä½†ä¸ºäº†ä»£ç æ¸…æ™°ï¼Œå»ºè®®ä¿ç•™å®šä¹‰ï¼Œä½†åœ¨ api ä¸­å¯ä»¥ä¸ä¼ 
    web_results: Annotated[List[dict], operator.add] 
    search_queries: Annotated[List[str], operator.add]
</file>

<file path="app/modules/perception/__init__.py">
"""
æ„ŸçŸ¥æ¨¡å— - ä¿¡æ¯æ„ŸçŸ¥å’Œæ”¶é›†

è´Ÿè´£ä¿¡æ¯çš„æ”¶é›†ã€æ„ŸçŸ¥å’Œåˆæ­¥å¤„ç†ã€‚

æœ¬æ¨¡å—æä¾›å¤šç§ä¿¡æ¯æ”¶é›†èƒ½åŠ›ï¼š
    - ä»äº’è”ç½‘æŠ“å–ç½‘é¡µå†…å®¹
    - é€šè¿‡æœç´¢å¼•æ“è·å–ç›¸å…³ä¿¡æ¯
    - è§£æå’Œæå–å…³é”®ä¿¡æ¯
    - å¯¹æ”¶é›†çš„æ•°æ®è¿›è¡Œé¢„å¤„ç†

å­æ¨¡å—:
    - crawler: ç½‘é¡µçˆ¬è™«
        æä¾›æ™ºèƒ½ç½‘é¡µæŠ“å–èƒ½åŠ›
        æ”¯æŒåŠ¨æ€å†…å®¹åŠ è½½å’Œåçˆ¬è™«ç­–ç•¥

    - search: æœç´¢å¼•æ“
        é›†æˆå¤šç§æœç´¢å¼•æ“API
        æä¾›æ™ºèƒ½æœç´¢å’Œç»“æœç­›é€‰åŠŸèƒ½

ä¸»è¦åŠŸèƒ½:
    1. å¤šæºä¿¡æ¯æ”¶é›†
    2. å®æ—¶ç½‘é¡µå†…å®¹æŠ“å–
    3. æ™ºèƒ½æœç´¢å’Œç»“æœä¼˜åŒ–
    4. æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
    5. æ”¯æŒå¤šç§æ•°æ®æ ¼å¼è¾“å‡º

ä½¿ç”¨ç¤ºä¾‹:
    from app.modules.perception import crawler, search

ä½œè€…: ApexBridge Team
ç‰ˆæœ¬: 1.0.0
"""

from . import crawler, search

# æ˜ç¡®åˆ—å‡ºæ‰€æœ‰å…¬å¼€çš„å­æ¨¡å—
__all__ = [
    "crawler",
    "search",
]
</file>

<file path="app/modules/perception/crawler.py">
# app/modules/perception/crawler.py
import asyncio
from crawl4ai import AsyncWebCrawler
from typing import List, Dict

async def crawl_urls(urls: List[str]) -> List[Dict]:
    """
    å¹¶å‘æŠ“å–å¤šä¸ª URL å¹¶è½¬æ¢ä¸º Markdown (ä¼˜åŒ–ç‰ˆ)
    """
    if not urls:
        return []

    print(f"ğŸ•·ï¸ [Crawl4AI] Starting concurrent crawl for {len(urls)} URLs...")
    
    # å®šä¹‰å•ä¸ª URL çš„å¤„ç†é€»è¾‘ (é—­åŒ…)
    async def process_url(crawler, url: str):
        try:
            # arun æ˜¯å¼‚æ­¥çš„ï¼Œè¿™é‡Œå¹¶å‘è°ƒç”¨åŒä¸€ä¸ª crawler å®ä¾‹
            result = await crawler.arun(
                url=url,
                bypass_cache=True,       # æ€»æ˜¯è·å–æœ€æ–°å†…å®¹
                word_count_threshold=50  # è¿‡æ»¤æ‰å†…å®¹è¿‡å°‘çš„é¡µé¢ (å¦‚ 403/404 é¡µ)
            )
            
            if result.success:
                print(f"âœ… [Crawl4AI] Scraped: {url[:30]}... ({len(result.markdown)} chars)")
                return {
                    "url": url,
                    "content": result.markdown,
                    "source": url
                }
            else:
                print(f"âš ï¸ [Crawl4AI] Failed to scrape {url}: {result.error_message}")
                return None
                
        except Exception as e:
            print(f"âŒ [Crawl4AI] Exception for {url}: {e}")
            return None

    # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¯åŠ¨æµè§ˆå™¨å®ä¾‹
    async with AsyncWebCrawler(verbose=True) as crawler:
        # 1. åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = [process_url(crawler, url) for url in urls]
        
        # 2. å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ (Gather)
        # å¦‚æœ URL éå¸¸å¤š(>10)ï¼Œå»ºè®®ä½¿ç”¨ asyncio.Semaphore é™åˆ¶å¹¶å‘æ•°
        # ä½† Deep Research æ¯æ¬¡ä¸€èˆ¬åªæœ 3-5 ä¸ªç»“æœï¼Œç›´æ¥ gather å³å¯
        results_with_none = await asyncio.gather(*tasks)
        
        # 3. è¿‡æ»¤æ‰å¤±è´¥çš„ç»“æœ (None)
        results = [r for r in results_with_none if r is not None]
                
    return results
</file>

<file path="app/modules/perception/search.py">
# app/modules/perception/search.py
import httpx
import random
import asyncio
from typing import List, Dict
from itertools import cycle
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from app.core.config import settings

# --- Key ç®¡ç†å™¨ ---
class KeyManager:
    """ç®€å•çš„ Key è½®è¯¢ç®¡ç†å™¨"""
    def __init__(self, keys: List[str]):
        self.keys = keys
        self._iterator = cycle(keys) if keys else None

    def get_key(self) -> str:
        if not self._iterator:
            raise ValueError("No Tavily API keys configured!")
        return next(self._iterator)

# åˆå§‹åŒ–ç®¡ç†å™¨
tavily_key_manager = KeyManager(settings.TAVILY_API_KEYS)

# --- å…·ä½“çš„å®ç°å‡½æ•° ---

async def _search_searxng(query: str, num_results: int) -> List[Dict[str, str]]:
    """SearXNG æœç´¢å®ç°"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    params = {
        "q": query,
        "format": "json",
        "engines": "google,bing,duckduckgo,wikipedia",
        "language": "zh-CN",
        "safesearch": "0"
    }
    
    async with httpx.AsyncClient() as client:
        resp = await client.get(settings.SEARXNG_BASE_URL, params=params, headers=headers, timeout=15.0)
        resp.raise_for_status()
        data = resp.json()
        
        results = []
        if "results" in data:
            for item in data["results"][:num_results]:
                if item.get("url", "").startswith("http"):
                    results.append({
                        "url": item["url"],
                        "title": item.get("title", ""),
                        "snippet": item.get("content", "")
                    })
        return results

async def _search_tavily(query: str, num_results: int) -> List[Dict[str, str]]:
    """Tavily æœç´¢å®ç° (æ”¯æŒå¤šKeyåˆ‡æ¢)"""
    
    # è·å–å½“å‰ Key
    api_key = tavily_key_manager.get_key()
    
    payload = {
        "api_key": api_key,
        "query": query,
        "search_depth": "basic", # æˆ– "advanced" ç”¨äºæ›´æ·±åº¦çš„æœç´¢ï¼ˆæ›´è´µï¼‰
        "include_answer": False,
        "include_images": False,
        "include_raw_content": False,
        "max_results": num_results
    }
    
    async with httpx.AsyncClient() as client:
        # Tavily REST API
        resp = await client.post("https://api.tavily.com/search", json=payload, timeout=15.0)
        
        # 401/403 é€šå¸¸æ„å‘³ç€ Key é¢åº¦ç”¨å®Œæˆ–æ— æ•ˆ
        if resp.status_code in [401, 403]:
            print(f"âš ï¸ [Tavily] Key {api_key[:8]}... failed (Quota/Auth). Rotating key.")
            # æŠ›å‡ºç‰¹å®šå¼‚å¸¸ï¼Œè™½ç„¶ Tenacity ä¼šé‡è¯•ï¼Œä½†ä¸‹æ¬¡è°ƒç”¨ KeyManager ä¼šæ‹¿åˆ°æ–° Key
            # (æ³¨æ„ï¼šä¸Šé¢çš„ get_key æ˜¯åŸºäº cycle çš„ï¼Œæ‰€ä»¥ä¸‹æ¬¡è°ƒç”¨å‡½æ•°æ—¶è‡ªç„¶ä¼šæ‹¿åˆ°ä¸‹ä¸€ä¸ª)
            resp.raise_for_status()
            
        resp.raise_for_status()
        data = resp.json()
        
        results = []
        if "results" in data:
            for item in data["results"]:
                results.append({
                    "url": item["url"],
                    "title": item.get("title", ""),
                    "snippet": item.get("content", "") # Tavily è¿”å›çš„æ˜¯ content
                })
        return results

# --- ç»Ÿä¸€å…¥å£ ---

# å®šä¹‰é‡è¯•ç­–ç•¥ï¼šåªé‡è¯•ç½‘ç»œç±»å¼‚å¸¸
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type((httpx.HTTPError, httpx.TimeoutException, ConnectionError)),
    reraise=True
)
async def search_generic(query: str, num_results: int = 5) -> List[Dict[str, str]]:
    """
    é€šç”¨æœç´¢å…¥å£ï¼šæ ¹æ®é…ç½®åˆ†å‘è¯·æ±‚
    """
    provider = settings.SEARCH_PROVIDER.lower()
    
    print(f"ğŸ” [Search] Requesting ({provider}): {query[:20]}...")

    try:
        if provider == "tavily":
            return await _search_tavily(query, num_results)
        elif provider == "searxng":
            return await _search_searxng(query, num_results)
        else:
            print(f"âš ï¸ Unknown provider '{provider}', falling back to SearXNG")
            return await _search_searxng(query, num_results)
            
    except Exception as e:
        # è¿™é‡Œç”± Tenacity æ•è·å¹¶é‡è¯•
        print(f"âŒ [Search] Error with {provider}: {e}")
        raise e

# ä¿æŒæ¥å£å…¼å®¹æ€§ï¼Œç›´æ¥å¯¼å‡ºåˆ«å
search_searxng = search_generic
</file>

<file path="app/modules/__init__.py">
"""
æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

åŒ…å«äº”å¤§æ ¸å¿ƒæ¨¡å—ï¼š
    - orchestrator: å·¥ä½œæµç¼–æ’å™¨
        ä½¿ç”¨LangGraphå®ç°çŠ¶æ€æœºå’Œå·¥ä½œæµç¼–æ’
        è´Ÿè´£ä»»åŠ¡è°ƒåº¦ã€æµç¨‹æ§åˆ¶å’Œæ¨¡å—åè°ƒ

    - perception: ä¿¡æ¯æ„ŸçŸ¥æ¨¡å—
        è´Ÿè´£ä¿¡æ¯æ”¶é›†å’Œæ„ŸçŸ¥
        åŒ…æ‹¬ç½‘é¡µæŠ“å–(crawler)å’Œæœç´¢å¼•æ“(search)èƒ½åŠ›

    - knowledge: çŸ¥è¯†å¼•æ“
        è´Ÿè´£æ•°æ®çš„å­˜å‚¨ã€ç´¢å¼•å’Œæ£€ç´¢
        åŒ…æ‹¬å‘é‡æ•°æ®åº“(LanceDB)å’Œå…³ç³»å‹æ•°æ®åº“(SQLModel)

    - insight: æ´å¯Ÿæ¨¡å—
        è´Ÿè´£ç”Ÿæˆå’Œç®¡ç†Promptæ¨¡æ¿
        ä¸ºLLMæä¾›ç»“æ„åŒ–çš„æç¤ºè¯

è¿™äº›æ¨¡å—ååŒå·¥ä½œï¼Œå®ç°å®Œæ•´çš„AIé©±åŠ¨ç ”ç©¶æµç¨‹ã€‚

ä½¿ç”¨ç¤ºä¾‹:
    from app.modules import (
        orchestrator,
        perception,
        knowledge,
        insight
    )

ä½œè€…: ApexBridge Team
ç‰ˆæœ¬: 1.0.0
"""

from . import (
    orchestrator,
    perception,
    knowledge,
    insight
)

# æ˜ç¡®åˆ—å‡ºæ‰€æœ‰å…¬å¼€çš„æ ¸å¿ƒæ¨¡å—
__all__ = [
    "orchestrator",
    "perception",
    "knowledge",
    "insight",
]
</file>

<file path="app/__init__.py">
"""
Deep Research Backend - æ ¸å¿ƒåº”ç”¨åŒ…

è¿™æ˜¯ä¸€ä¸ªåŸºäºAIçš„æ·±åº¦ç ”ç©¶ç³»ç»Ÿï¼Œæä¾›æ™ºèƒ½ä¿¡æ¯æ”¶é›†ã€åˆ†æå’Œæ´å¯Ÿèƒ½åŠ›ã€‚
ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼šå·¥ä½œæµç¼–æ’ã€ä¿¡æ¯æ„ŸçŸ¥ã€çŸ¥è¯†ç®¡ç†å’Œæ´å¯Ÿç”Ÿæˆã€‚

ä¸»è¦æ¨¡å—:
    - api: APIè·¯ç”±å’Œæ¥å£
    - modules: æ ¸å¿ƒåŠŸèƒ½æ¨¡å—é›†åˆ
        - orchestrator: å·¥ä½œæµç¼–æ’å™¨
        - perception: ä¿¡æ¯æ„ŸçŸ¥å’Œæ”¶é›†
        - knowledge: çŸ¥è¯†å­˜å‚¨å’Œæ£€ç´¢
        - insight: æ´å¯Ÿç”Ÿæˆ

ä½¿ç”¨è¯´æ˜:
    # å¯¼å…¥APIæ¨¡å—
    from app.api import research, history

    # å¯¼å…¥æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
    from app.modules import orchestrator, perception, knowledge, insight

    # å¯¼å…¥LLMè°ƒç”¨æ¥å£
    from app.core import simple_llm_call

ä½œè€…: ApexBridge Team
ç‰ˆæœ¬: 1.0.0
è®¸å¯è¯: MIT
"""

__version__ = "1.0.0"
__author__ = "ApexBridge Team"
__email__ = "team@apexbridge.ai"
__license__ = "MIT"
__description__ = "åŸºäºAIçš„æ·±åº¦ç ”ç©¶ç³»ç»Ÿ"


__all__ = [
    "__version__",
    "__author__",
    "__email__",
    "__license__",
    "__description__",
]
</file>

<file path=".gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/#use-with-ide
.pdm.toml

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be added to the global gitignore or merged into this project gitignore.  For a PyCharm
#  project, it is recommended to exclude .idea directory in version control.
.idea/

# VS Code
.vscode/
*.code-workspace

# ApexBridge Deep Research specific
# Data storage directories
data/lancedb/
data/checkpoints.db
data/*.db
data/*.sqlite

# Vector database files
*. lance
*. lancedb/

# Research outputs
reports/
outputs/
research_results/

# Model caches
model_cache/
.cache/

# Temporary files
tmp/
temp/
*.tmp

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Log files
logs/
*.log

# API keys and secrets (additional protection)
.env.local
.env.development
.env.production
secrets.json
config.json

# Testing artifacts
.coverage
.coverage.*
htmlcov/

# Documentation builds
docs/_build/
site/

# Jupyter Notebook checkpoints
*.ipynb_checkpoints

# pyenv
.python-version

# pipenv
Pipfile.lock

# Celery beat schedule file
celerybeat-schedule

# SageMath parsed files
*.sage.py

# Stores VSCode versions used for
.vscode-test

# yarn v2 testing VSCode extensions
.yarn/cache
.yarn/unplugged
.yarn/build-state.yml
.yarn/install-state.gz
.pnp.*

# IntelliJ based IDEs
.idea

# Finder (MacOS)
.Finder_# Docker
.dockerignore
Dockerfile.dev
docker-compose.*

override.yml

# SearXNG config (if custom)
searxng/
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2024 ApexBridge Deep Research System Contributors

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="main.py">
# main.py
from fastapi import FastAPI
from app.api.research import router as research_router
import uvicorn
from app.core.config import settings

app = FastAPI(title="Deep Research Backend")

# æ³¨å†Œè·¯ç”±
app.include_router(research_router, prefix="/api")

if __name__ == "__main__":
    print(f"ğŸš€ Starting server on {settings.API_HOST}:{settings.API_PORT}")
    uvicorn.run(
        "main:app", 
        host=settings.API_HOST, 
        port=settings.API_PORT, 
        reload=True
    )
</file>

<file path="requirements.txt">
# Web Framework & API
fastapi
uvicorn
sse-starlette
httpx

# AI & Orchestration
langgraph
langgraph-checkpoint-sqlite
langchain-text-splitters
litellm
tenacity
async_timeout

# Data & Storage
pydantic
pydantic_settings
lancedb
pyarrow
aiosqlite
sqlmodel
numpy

# Crawler
crawl4ai

# Utils
loguru
</file>

</files>
