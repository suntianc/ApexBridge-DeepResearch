This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
app/
  api/
    __init__.py
    history.py
    research.py
  core/
    __init__.py
    config.py
    llm.py
  modules/
    insight/
      __init__.py
      prompts.py
    knowledge/
      __init__.py
      rdb.py
      vector.py
    orchestrator/
      __init__.py
      graph.py
      state.py
    perception/
      __init__.py
      crawler.py
      search.py
    __init__.py
  __init__.py
  worker.py
data/
  lancedb/
    research_context.lance/
      _transactions/
        0-168820f8-6de7-4191-bf5c-736cad8579e5.txn
        1-86c8d8de-cc33-49c5-9b31-4022509af63c.txn
        10-299f8de6-2eff-48d5-920a-52c84f774229.txn
        11-81a498a3-5a26-42cb-a181-5f8941b71ba1.txn
        12-8307f7f1-2a17-4e4a-a3f2-96230d86c298.txn
        13-b6197cde-809f-4652-97ec-096cdacba8f6.txn
        14-68da54f3-dcd7-4500-a4ca-99be0b448eaa.txn
        2-b2f083f5-7ba2-447a-af3b-051b1d64952c.txn
        3-0ee3085a-ca25-49db-b4f0-0b25f640e7a1.txn
        4-e0c86ffa-b724-473d-b2de-1db109c1f881.txn
        5-2e435c94-9841-4e35-8b7c-e8c93f3cd6d1.txn
        6-91991850-eafd-4fc1-aa80-ef74de15cd94.txn
        7-2bb04635-d54e-465d-b216-4ac6feef16d2.txn
        8-6ef070e8-86b8-4d3f-ac07-be281e9445af.txn
        9-b9509433-11f6-42d8-91fb-a4328d2db128.txn
      _versions/
        1.manifest
        10.manifest
        11.manifest
        12.manifest
        13.manifest
        14.manifest
        15.manifest
        2.manifest
        3.manifest
        4.manifest
        5.manifest
        6.manifest
        7.manifest
        8.manifest
        9.manifest
      data/
        0000011010010110101000111731dd48b49b388455cc291ae8.lance
        001011000011011100001000196f114c15b6db8236cab28dae.lance
        001101010100101000101100be3ee0423ca1d1770735c1402e.lance
        001101010111111011111110b514dd4b339de7b4996372f8f0.lance
        0100000101111000010111015a10a0443fbbb1b4df16f8d889.lance
        010010001101011010000110a7019e4f9692bc451c3162d790.lance
        0100111110111001011010007898ae4009bcfbd2582d3515e1.lance
        0101100100101011000000119cb0ba4630b7f0111a8ecedc25.lance
        01100000110111110101011088c2d944fc898efb1438dc93d0.lance
        011101010001010111101101b3b57148a5b89bb495c00f1316.lance
        10000100100010011101101004b4da49109cb2e8683d622684.lance
        100111001011110000101100bcbd224ed88aced75fe8f80543.lance
        110111110011111001111101d6f4bb4cb0a3f94d698d2410b4.lance
        111101000101010101100010cc09d64642ae68aca24b4f68e3.lance
main.py
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(mkdir:*)",
      "Bash(python:*)",
      "Bash(tree:*)"
    ]
  }
}
</file>

<file path="app/api/__init__.py">
"""
API è·¯ç”±æ¨¡å—

æä¾›RESTful APIæ¥å£ï¼ŒåŒ…æ‹¬ï¼š
    - research: ç ”ç©¶ç›¸å…³APIæ¥å£
    - history: å†å²è®°å½•APIæ¥å£

æœ¬æ¨¡å—å°è£…äº†æ‰€æœ‰å¯¹å¤–æš´éœ²çš„APIè·¯ç”±ï¼Œ
é€šè¿‡FastAPIæ¡†æ¶å®ç°é«˜æ•ˆçš„ç½‘ç»œè¯·æ±‚å¤„ç†ã€‚

ä½¿ç”¨ç¤ºä¾‹:
    from app.api import research, history

ä½œè€…: ApexBridge Team
ç‰ˆæœ¬: 1.0.0
"""

from . import research, history

# æ˜ç¡®åˆ—å‡ºæ‰€æœ‰å…¬å¼€çš„å­æ¨¡å—
__all__ = [
    "research",
    "history",
]
</file>

<file path="app/api/research.py">
# app/api/research.py
from fastapi import APIRouter
from sse_starlette.sse import EventSourceResponse
from app.modules.orchestrator.graph import build_graph
import asyncio, json, uuid

router = APIRouter()
graph = build_graph()

@router.get("/stream")
async def stream_research(topic: str):
    # ç”Ÿæˆå”¯ä¸€ä¼šè¯ ID
    thread_id = str(uuid.uuid4())
    # é…ç½® config
    config = {"configurable": {"thread_id": thread_id}}
    """æµå¼è¾“å‡º Agent çš„æ€è€ƒè¿‡ç¨‹"""
    async def event_generator():
        # åˆå§‹åŒ–çŠ¶æ€
        inputs = {
            "topic": topic,
            "iteration_count": 0,
            "max_iterations": 3,
            "search_queries": [],
            "web_results": []
        }
        
        # è¿è¡Œå›¾è°± (astream ä¼šäº§ç”Ÿæ¯ä¸ªæ­¥éª¤çš„äº‹ä»¶)
        async for event in graph.astream(inputs, config=config):
            # event çš„æ ¼å¼é€šå¸¸æ˜¯ {"node_name": {updated_state_keys}}
            for node_name, state_update in event.items():
                
                # æ„é€ å‘é€ç»™å‰ç«¯çš„æ¶ˆæ¯
                payload = {
                    "step": node_name,
                    "data": state_update
                }

                json_str = json.dumps(
                        payload, 
                        default=str, 
                        ensure_ascii=False  # <--- å…³é”®ï¼è®©ä¸­æ–‡åŸæ ·è¾“å‡º
                    )
                
                # SSE æ ¼å¼: data: ...
                yield {
                    "event": "update",
                    "data": json_str
                }
                
                # ç¨å¾®è®©å­å¼¹é£ä¸€ä¼šå„¿ï¼Œæ–¹ä¾¿è§‚å¯Ÿ (ç”Ÿäº§ç¯å¢ƒå»æ‰)
                await asyncio.sleep(0.5)

        yield {"event": "finish", "data": "DONE"}

    return EventSourceResponse(event_generator())
</file>

<file path="app/core/__init__.py">
"""
æ ¸å¿ƒé…ç½®æ¨¡å—

æä¾›ç³»ç»Ÿæ ¸å¿ƒé…ç½®å’ŒLLMè°ƒç”¨æ¥å£ã€‚

ä¸»è¦ç»„ä»¶:
    - simple_llm_call: é€šç”¨LLMè°ƒç”¨æ¥å£
        æ”¯æŒDeepSeekã€OpenAIã€Claudeã€Ollamaç­‰å¤šç§æ¨¡å‹
        é€šè¿‡LiteLLMæä¾›ç»Ÿä¸€çš„è°ƒç”¨æ–¹å¼

ä½¿ç”¨ç¤ºä¾‹:
    from app.core import simple_llm_call

    response = await simple_llm_call("Hello, world!")

ä½œè€…: ApexBridge Team
ç‰ˆæœ¬: 1.0.0
"""

from .llm import simple_llm_call

__all__ = ["simple_llm_call"]
</file>

<file path="app/core/config.py">
# app/core/config.py
from pydantic_settings import BaseSettings
import os

class Settings(BaseSettings):
    # --- åŸºç¡€æœåŠ¡é…ç½® ---
    API_HOST: str = "0.0.0.0"
    API_PORT: int = 23800
    
    # --- è·¯å¾„é…ç½® (è‡ªåŠ¨è½¬æ¢ä¸ºç»å¯¹è·¯å¾„æˆ–ä¿æŒç›¸å¯¹è·¯å¾„) ---
    # çŸ¥è¯†åº“å‘é‡æ•°æ®è·¯å¾„
    LANCEDB_PATH: str = "./data/lancedb"
    # LangGraph æ£€æŸ¥ç‚¹æ•°æ®åº“è·¯å¾„
    CHECKPOINT_DB_PATH: str = "./data/checkpoints.db"
    
    # --- å¤–éƒ¨ä¾èµ–é…ç½® ---
    # SearXNG æœç´¢æœåŠ¡åœ°å€
    SEARXNG_BASE_URL: str = "http://localhost:8888/search"
    
    # --- æ¨¡å‹é…ç½® ---
    # å¦‚æœ .env ä¸­æ²¡æœ‰è®¾ç½®ï¼Œè¯»å–ç³»ç»Ÿç¯å¢ƒå˜é‡ï¼›å¦‚æœéƒ½æ²¡æœ‰ï¼Œåˆ™ä¸º None
    DEEPSEEK_API_KEY: str | None = None
    OPENAI_API_KEY: str | None = None

    class Config:
        # æŒ‡å®šåŠ è½½ .env æ–‡ä»¶
        env_file = ".env"
        # å¿½ç•¥å¤šä½™çš„å˜é‡
        extra = "ignore"

# å®ä¾‹åŒ–å…¨å±€é…ç½®å¯¹è±¡
settings = Settings()

# è‡ªåŠ¨ç¡®ä¿å…³é”®æ•°æ®ç›®å½•å­˜åœ¨ (å¯é€‰ï¼Œå¢å¼ºå¥å£®æ€§)
os.makedirs(os.path.dirname(settings.CHECKPOINT_DB_PATH), exist_ok=True)
os.makedirs(settings.LANCEDB_PATH, exist_ok=True)
</file>

<file path="app/core/llm.py">
from litellm import completion
import os
from dotenv import load_dotenv

# åŠ è½½ .env ç¯å¢ƒå˜é‡
load_dotenv()

async def simple_llm_call(
    prompt: str, 
    model: str = "deepseek/deepseek-chat", # é»˜è®¤æ”¹ä¸º DeepSeek V3
    temperature: float = 0.7
) -> str:
    """
    é€šç”¨ LLM è°ƒç”¨æ¥å£ï¼Œæ”¯æŒ DeepSeek, OpenAI, Claude, Ollama ç­‰
    """
    
    # æ‰“å°å½“å‰ä½¿ç”¨çš„æ¨¡å‹ï¼Œæ–¹ä¾¿è°ƒè¯•
    print(f"ğŸ¤– [LLM Call] Model: {model}")

    try:
        # LiteLLM ä¼šè‡ªåŠ¨æ ¹æ® model å‰ç¼€è¯†åˆ«ä¾›åº”å•†
        # deepseek/deepseek-chat -> è‡ªåŠ¨æ˜ å°„åˆ° DeepSeek API
        # ollama/deepseek-r1 -> è‡ªåŠ¨æ˜ å°„åˆ°æœ¬åœ° Ollama
        
        response = completion(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            # å¦‚æœæ˜¯ DeepSeek APIï¼Œä¸éœ€è¦æ‰‹åŠ¨è®¾ base_urlï¼ŒLiteLLM å†…ç½®äº†æ”¯æŒ
            # å¦‚æœæ˜¯ Ollamaï¼ŒLiteLLM é»˜è®¤è¿æ¥ http://localhost:11434
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        print(f"âŒ [LLM Error] {model} failed: {str(e)}")
        return f"Error generation response with {model}. Details: {str(e)}"

# --- ä½¿ç”¨è¯´æ˜ ---
# 1. DeepSeek API: 
#    model="deepseek/deepseek-chat" (V3)
#    model="deepseek/deepseek-reasoner" (R1)
#
# 2. æœ¬åœ° DeepSeek (é€šè¿‡ Ollama):
#    model="ollama/deepseek-r1"
#
# 3. OpenAI:
#    model="gpt-4o"
</file>

<file path="app/modules/insight/__init__.py">
"""
æ´å¯Ÿæ¨¡å— - æ´å¯Ÿç”Ÿæˆå’ŒPromptç®¡ç†

è´Ÿè´£ç”Ÿæˆå’Œç®¡ç†æ™ºèƒ½æ´å¯Ÿï¼Œä»¥åŠä¸ºLLMæä¾›ç»“æ„åŒ–çš„æç¤ºè¯æ¨¡æ¿ã€‚

æœ¬æ¨¡å—çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
    - è®¾è®¡å’Œç®¡ç†Promptæ¨¡æ¿
    - æ ¹æ®ä¸åŒåœºæ™¯ç”Ÿæˆå®šåˆ¶åŒ–æç¤ºè¯
    - ä¼˜åŒ–LLMè¾“å…¥ä»¥è·å¾—æ›´å¥½çš„è¾“å‡ºè´¨é‡
    - ç”Ÿæˆæ·±åº¦åˆ†æå’Œæ´å¯ŸæŠ¥å‘Š

å­æ¨¡å—:
    - prompts: Promptæ¨¡æ¿ç®¡ç†
        æä¾›å„ç§åœºæ™¯ä¸‹çš„æç¤ºè¯æ¨¡æ¿
        åŒ…æ‹¬ç ”ç©¶ã€åˆ†æã€æ€»ç»“ã€åˆ›ä½œç­‰å¤šç§ç±»å‹
        æ”¯æŒåŠ¨æ€æ¨¡æ¿å‚æ•°åŒ–å’Œæ¨¡æ¿é“¾å¼ç»„åˆ

ä¸»è¦åŠŸèƒ½:
    1. æ™ºèƒ½Promptæ¨¡æ¿åº“
        - ç ”ç©¶ç±»æ¨¡æ¿ï¼šæ·±åº¦ç ”ç©¶ã€å¯¹æ¯”åˆ†æ
        - æ€»ç»“ç±»æ¨¡æ¿ï¼šæ‘˜è¦ç”Ÿæˆã€è¦ç‚¹æå–
        - åˆ›ä½œç±»æ¨¡æ¿ï¼šå†…å®¹åˆ›ä½œã€åˆ›æ„ç”Ÿæˆ
        - åˆ†æç±»æ¨¡æ¿ï¼šæ•°æ®åˆ†æã€è¶‹åŠ¿æ´å¯Ÿ

    2. åŠ¨æ€æ¨¡æ¿ç”Ÿæˆ
        - æ ¹æ®è¾“å…¥å‚æ•°ç”Ÿæˆå®šåˆ¶åŒ–æç¤ºè¯
        - æ”¯æŒæ¨¡æ¿åµŒå¥—å’Œç»„åˆ
        - æ™ºèƒ½å‚æ•°å¡«å……å’ŒéªŒè¯

    3. æç¤ºè¯ä¼˜åŒ–
        - è‡ªåŠ¨ä¼˜åŒ–æç¤ºè¯ç»“æ„
        - æä¾›æç¤ºè¯è´¨é‡è¯„ä¼°
        - æ”¯æŒA/Bæµ‹è¯•å’Œè¿­ä»£æ”¹è¿›

    4. æ´å¯Ÿç”Ÿæˆ
        - åŸºäºæ”¶é›†çš„æ•°æ®ç”Ÿæˆæ·±åº¦æ´å¯Ÿ
        - æä¾›å¤šç»´åº¦åˆ†æç»“æœ
        - ç”Ÿæˆå¯è§£é‡Šçš„åˆ†ææŠ¥å‘Š

ä½¿ç”¨ç¤ºä¾‹:
    from app.modules.insight import prompts

ä½œè€…: ApexBridge Team
ç‰ˆæœ¬: 1.0.0
"""

from . import prompts

# æ˜ç¡®åˆ—å‡ºæ‰€æœ‰å…¬å¼€çš„å­æ¨¡å—
__all__ = [
    "prompts",
]
</file>

<file path="app/modules/insight/prompts.py">
# app/modules/insight/prompts.py
from textwrap import dedent

class ResearchPrompts:
    """
    ç»Ÿä¸€ç®¡ç†ç³»ç»Ÿä¸­çš„æ‰€æœ‰ Prompt æ¨¡æ¿ã€‚
    ä½¿ç”¨ dedent å»é™¤ç¼©è¿›ï¼Œä¿æŒ Prompt å¹²å‡€ã€‚
    """

    @staticmethod
    def planner_initial(topic: str) -> str:
        """[è§„åˆ’è€…] ç¬¬ä¸€è½®ï¼šå¹¿åº¦ä¼˜å…ˆè§„åˆ’"""
        return dedent(f"""
            ç”¨æˆ·æƒ³ç ”ç©¶ '{topic}'ã€‚
            è¯·ç”Ÿæˆ 1 ä¸ªæœ€é€‚åˆå…¥é—¨æˆ–è·å–å®è§‚æ•°æ®çš„æœç´¢å¼•æ“å…³é”®è¯ã€‚
            
            è¦æ±‚ï¼š
            1. å…³é”®è¯è¦ç²¾å‡†ï¼Œåˆ©äºæœç´¢å¼•æ“ç†è§£ã€‚
            2. åªè¿”å›å…³é”®è¯æœ¬èº«ï¼Œä¸è¦åŒ…å«è§£é‡Šæˆ–å¼•å·ã€‚
        """).strip()

    @staticmethod
    def planner_gap_driven(topic: str, gap: str) -> str:
        """[è§„åˆ’è€…] åç»­è½®æ¬¡ï¼šåŸºäºç¼ºå£çš„æ·±åº¦è§„åˆ’"""
        return dedent(f"""
            ç ”ç©¶ä¸»é¢˜: {topic}
            ç›®å‰å·²æœ‰çš„åˆ†ææŒ‡å‡ºç¼ºå¤±ä¿¡æ¯(Gap): "{gap}"
            
            ä»»åŠ¡ï¼š
            è¯·ç”Ÿæˆ 1 ä¸ª**éå¸¸å…·ä½“**çš„æœç´¢å…³é”®è¯ï¼Œä¸“é—¨ç”¨æ¥æŒ–æ˜ä¸Šè¿°ç¼ºå¤±çš„ä¿¡æ¯ã€‚
            
            ç¤ºä¾‹ç­–ç•¥ï¼š
            - å¦‚æœç¼ºæ•°æ®ï¼Œæœç´¢è¯åº”åŒ…å« "statistics", "data", "report", "2024" ç­‰ã€‚
            - å¦‚æœç¼ºæŠ€æœ¯ç»†èŠ‚ï¼Œæœç´¢è¯åº”åŒ…å« "architecture", "specs", "whitepaper" ç­‰ã€‚
            
            è¦æ±‚ï¼š
            åªè¿”å›å…³é”®è¯æœ¬èº«ï¼Œä¸è¦åŒ…å«å¼•å·ã€‚
        """).strip()

    @staticmethod
    def analyst_reasoning(topic: str, context: str) -> str:
        """[åˆ†æå¸ˆ] RAG åˆ†æä¸ Gap è¯†åˆ« (é€‚ç”¨äº DeepSeek R1)"""
        return dedent(f"""
            ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ·±åº¦ç ”ç©¶å‘˜ã€‚åŸºäºä»¥ä¸‹ã€å·²æ ¸å®çš„çŸ¥è¯†åº“ç‰‡æ®µã€‘è¿›è¡Œåˆ†æã€‚
            
            ã€å·²æ ¸å®ä¿¡æ¯ã€‘ï¼š
            {context}
            
            ã€ä»»åŠ¡ã€‘ï¼š
            1. ç»¼åˆç›®å‰å…³äº '{topic}' çš„æ‰€æœ‰ä¿¡æ¯ï¼Œå†™ä¸€æ®µæ·±åº¦åˆ†æï¼ˆDraftï¼‰ã€‚
            2. æ‰¹åˆ¤æ€§åœ°æŒ‡å‡ºï¼šæˆ‘ä»¬è¿˜**ç¼ºå°‘**ä»€ä¹ˆå…³é”®æ•°æ®æˆ–è§†è§’ï¼Ÿ(Gap Analysis)ã€‚
            
            ã€è¾“å‡ºè¦æ±‚ã€‘ï¼š
            - å¦‚æœä¿¡æ¯å·²ç»éå¸¸å……åˆ†ï¼Œèƒ½å¤Ÿå›ç­” '{topic}' çš„æ ¸å¿ƒé—®é¢˜ï¼ŒGap è¯·å›å¤ "æ— "ã€‚
            - å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·åœ¨ Gap éƒ¨åˆ†æ˜ç¡®æŒ‡å‡ºä¸‹ä¸€æ­¥éœ€è¦æœä»€ä¹ˆï¼ˆä¾‹å¦‚ï¼šâ€œç¼ºä¹å…·ä½“çš„2024å¹´Q4æˆæœ¬æ•°æ®â€ï¼‰ã€‚
            - ä¿æŒå®¢è§‚ã€ä¸­ç«‹ï¼Œç”¨æ•°æ®è¯´è¯ã€‚
        """).strip()

    @staticmethod
    def publisher_final_report(topic: str, context: str) -> str:
        """[å‡ºç‰ˆè€…] æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆ"""
        return dedent(f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡Œä¸šåˆ†æå¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹ã€å·²æ ¸å®çš„ä¿¡æ¯ç‰‡æ®µã€‘ï¼Œæ’°å†™ä¸€ä»½å…³äº '{topic}' çš„æ·±åº¦ç ”ç©¶æŠ¥å‘Šã€‚
            
            ã€ä¿¡æ¯ç‰‡æ®µã€‘ï¼š
            {context}
            
            ã€æŠ¥å‘Šè¦æ±‚ã€‘ï¼š
            1. æ ¼å¼ï¼šMarkdownã€‚
            2. ç»“æ„ï¼š
               - ğŸ“‘ **æ‘˜è¦** (Executive Summary)
               - ğŸ§­ **ç›®å½•**
               - ğŸ“– **æ­£æ–‡** (åˆ†ç« èŠ‚ï¼Œé€»è¾‘æ¸…æ™°)
               - ğŸ“Š **ç»“è®ºä¸å±•æœ›**
            3. **å¼•ç”¨æ ‡æ³¨** (å…³é”®)ï¼šåœ¨æ­£æ–‡ä¸­å¼•ç”¨å…·ä½“æ•°æ®æˆ–è§‚ç‚¹æ—¶ï¼Œå¿…é¡»åœ¨å¥æœ«æ ‡æ³¨æ¥æºï¼Œæ ¼å¼ä¸º [Source: URL]ã€‚
            4. è¯­æ°”ï¼šå®¢è§‚ã€ä¸“ä¸šã€è¯¦å®ã€‚
            5. å†…å®¹ï¼šé‡ç‚¹å›ç­”ç”¨æˆ·æœ€åˆçš„é—®é¢˜ï¼Œå¹¶æ•´åˆä¹‹å‰åˆ†æä¸­å‘ç°çš„è¡ç”Ÿè§‚ç‚¹ã€‚
        """).strip()

# å®ä¾‹åŒ–ï¼ˆå¦‚æœéœ€è¦å•ä¾‹ï¼Œæˆ–è€…ç›´æ¥ç”¨é™æ€æ–¹æ³•ï¼‰
prompts = ResearchPrompts()
</file>

<file path="app/modules/knowledge/__init__.py">
"""
çŸ¥è¯†å¼•æ“æ¨¡å— - æ•°æ®å­˜å‚¨ã€ç´¢å¼•å’Œæ£€ç´¢

è´Ÿè´£ç³»ç»Ÿä¸­æ‰€æœ‰æ•°æ®çš„æŒä¹…åŒ–ã€ç´¢å¼•å’Œç®¡ç†ã€‚

æœ¬æ¨¡å—æä¾›å¤šå±‚æ¬¡çš„æ•°æ®å­˜å‚¨è§£å†³æ–¹æ¡ˆï¼š
    - å‘é‡æ•°æ®åº“å­˜å‚¨ï¼šç”¨äºè¯­ä¹‰æœç´¢å’Œç›¸ä¼¼åº¦åŒ¹é…
    - å…³ç³»å‹æ•°æ®åº“å­˜å‚¨ï¼šç”¨äºç»“æ„åŒ–æ•°æ®å’Œäº‹åŠ¡ç®¡ç†
    - æ··åˆå­˜å‚¨æ¨¡å¼ï¼šç»“åˆä¸¤ç§æ•°æ®åº“çš„ä¼˜åŠ¿

å­æ¨¡å—:
    - vector: å‘é‡æ•°æ®åº“ (LanceDB)
        æä¾›é«˜æ•ˆå‘é‡å­˜å‚¨å’Œç›¸ä¼¼åº¦æ£€ç´¢
        æ”¯æŒæ–‡æœ¬åµŒå…¥ã€å›¾åƒå‘é‡ç­‰å¤šæ¨¡æ€æ•°æ®
        å®ç°è¯­ä¹‰æœç´¢å’Œæ™ºèƒ½æ¨èåŠŸèƒ½

    - rdb: å…³ç³»å‹æ•°æ®åº“ (SQLModel)
        æä¾›ç»“æ„åŒ–æ•°æ®å­˜å‚¨å’Œç®¡ç†
        æ”¯æŒå¤æ‚æŸ¥è¯¢ã€äº‹åŠ¡å¤„ç†å’Œæ•°æ®å®Œæ•´æ€§
        ç®¡ç†ç”¨æˆ·æ•°æ®ã€ç³»ç»Ÿé…ç½®ç­‰ç»“æ„åŒ–ä¿¡æ¯

ä¸»è¦åŠŸèƒ½:
    1. å¤šæ¨¡æ€æ•°æ®å­˜å‚¨ (æ–‡æœ¬ã€å›¾åƒã€å‘é‡)
    2. é«˜æ•ˆç›¸ä¼¼åº¦æœç´¢å’Œæ£€ç´¢
    3. ç»“æ„åŒ–æ•°æ®ç®¡ç†å’ŒæŸ¥è¯¢
    4. æ•°æ®ç´¢å¼•ä¼˜åŒ–
    5. æ•°æ®å¤‡ä»½å’Œæ¢å¤
    6. æ”¯æŒæ°´å¹³æ‰©å±•

æ•°æ®æµ:
    æ„ŸçŸ¥æ¨¡å—æ”¶é›†æ•°æ® â†’ çŸ¥è¯†æ¨¡å—å­˜å‚¨å’Œç´¢å¼• â†’ å…¶ä»–æ¨¡å—æ£€ç´¢å’Œä½¿ç”¨

ä½¿ç”¨ç¤ºä¾‹:
    from app.modules.knowledge import vector, rdb

ä½œè€…: ApexBridge Team
ç‰ˆæœ¬: 1.0.0
"""

from . import vector, rdb

# æ˜ç¡®åˆ—å‡ºæ‰€æœ‰å…¬å¼€çš„å­æ¨¡å—
__all__ = [
    "vector",
    "rdb",
]
</file>

<file path="app/modules/knowledge/vector.py">
import lancedb,os
from typing import List, Dict
from litellm import embedding
from langchain_text_splitters import RecursiveCharacterTextSplitter
import pyarrow as pa
from app.core.config import settings

# 1. åˆå§‹åŒ– LanceDB (æœ¬åœ°æ–‡ä»¶æ¨¡å¼)
DB_PATH = settings.LANCEDB_PATH
os.makedirs(DB_PATH, exist_ok=True)
db = lancedb.connect(DB_PATH)

# å®šä¹‰è¡¨ç»“æ„ (Schema)
# vector ç»´åº¦å–å†³äºä½ ä½¿ç”¨çš„æ¨¡å‹ï¼ŒOpenAI text-embedding-3-small æ˜¯ 1536 ç»´
schema = pa.schema([
    pa.field("vector", pa.list_(pa.float32(), 768)),
    pa.field("text", pa.string()),
    pa.field("source", pa.string()),
    pa.field("chunk_id", pa.string())
])

def get_embedding(text: str) -> List[float]:
    """
    ä½¿ç”¨æœ¬åœ° Ollama çš„ nomic-embed-text è·å–å‘é‡
    é’ˆå¯¹ Log ç»“æ„: response['data'][0]['embedding']
    """
    try:
        response = embedding(
            model="ollama/nomic-embed-text", 
            input=[text]
        )
        return response.data[0]['embedding']

    except Exception as e:
        print(f"âŒ Embedding Error (Ollama): {e}")
        return [0.0] * 768

class KnowledgeBase:
    def __init__(self, table_name: str = "research_context"):
        self.table_name = table_name
        # å¦‚æœè¡¨ä¸å­˜åœ¨åˆ™åˆ›å»º
        try:
            self.table = db.open_table(table_name)
        except:
            self.table = db.create_table(table_name, schema=schema)

    def add_documents(self, documents: List[Dict]):
        """
        æ¥æ”¶çˆ¬å–ç»“æœ -> åˆ‡ç‰‡ -> å‘é‡åŒ– -> å­˜å…¥
        """
        # nomic-embed-text æ”¯æŒ 8192 contextï¼Œæˆ‘ä»¬å¯ä»¥ç¨å¾®æŠŠ chunk åˆ‡å¤§ä¸€ç‚¹
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1200, 
            chunk_overlap=200
        )
        
        data_to_insert = []
        
        print(f"ğŸ’¾ [Knowledge] Processing {len(documents)} docs with nomic-embed-text...")
        
        for doc in documents:
            # ç®€å•çš„æ¸…æ´—ï¼Œå»é™¤è¿‡å¤šç©ºè¡Œ
            clean_content = doc["content"].replace("\n\n\n", "\n")
            chunks = splitter.split_text(clean_content)
            
            for idx, chunk in enumerate(chunks):
                vec = get_embedding(chunk)
                # ç®€å•æ ¡éªŒç»´åº¦ï¼Œé˜²æ­¢ Ollama å¶å°”è¿”å›ç©º
                if len(vec) == 768:
                    data_to_insert.append({
                        "vector": vec,
                        "text": chunk,
                        "source": doc.get("source", "unknown"),
                        "chunk_id": f"{doc.get('url', 'unknown')}_{idx}"
                    })
        
        if data_to_insert:
            # mode="append" è¿½åŠ æ¨¡å¼
            self.table.add(data_to_insert)
            print(f"âœ… [Knowledge] Inserted {len(data_to_insert)} chunks (Dim: 768)")

    def search(self, query: str, limit: int = 5) -> str:
        """
        è¯­ä¹‰æ£€ç´¢
        """
        print(f"ğŸ” [Retrieval] Searching for: {query[:30]}...")
        query_vec = get_embedding(query)
        
        # å‘é‡æœç´¢
        results = self.table.search(query_vec).limit(limit).to_list()
        
        context = ""
        for item in results:
            context += f"--- Source: {item['source']} ---\n{item['text']}\n\n"
            
        print(f"âœ… [Retrieval] Found {len(results)} relevant chunks")
        return context
</file>

<file path="app/modules/orchestrator/__init__.py">
"""
ç¼–æ’å™¨æ¨¡å— - å·¥ä½œæµç¼–æ’å™¨

ä½¿ç”¨LangGraphå®ç°çŠ¶æ€æœºå’Œå·¥ä½œæµç¼–æ’ã€‚

æœ¬æ¨¡å—è´Ÿè´£ï¼š
    - å®šä¹‰å’Œç®¡ç†å¤æ‚çš„å·¥ä½œæµçŠ¶æ€
    - å®ç°ä»»åŠ¡é—´çš„ä¾èµ–å…³ç³»å’Œè°ƒåº¦
    - åè°ƒå„ä¸ªæ¨¡å—ä¹‹é—´çš„äº¤äº’
    - æä¾›å¯æ‰©å±•çš„çŠ¶æ€è½¬æ¢é€»è¾‘

å­æ¨¡å—:
    - graph: å›¾ç»“æ„å’Œå·¥ä½œæµå®šä¹‰
        åŒ…å«LangGraphèŠ‚ç‚¹å’Œè¾¹çš„å®šä¹‰

    - state: çŠ¶æ€ç®¡ç†
        å®šä¹‰å…±äº«çŠ¶æ€ç»“æ„å’ŒçŠ¶æ€è½¬æ¢é€»è¾‘

å·¥ä½œæµç¨‹:
    1. åˆå§‹åŒ–å·¥ä½œæµçŠ¶æ€
    2. é€šè¿‡graphå®šä¹‰æ‰§è¡Œè·¯å¾„
    3. ä½¿ç”¨stateè·Ÿè¸ªå’Œç®¡ç†çŠ¶æ€å˜åŒ–
    4. åè°ƒå„æ¨¡å—æŒ‰åºæ‰§è¡Œ

ä½¿ç”¨ç¤ºä¾‹:
    from app.modules.orchestrator import graph, state

ä½œè€…: ApexBridge Team
ç‰ˆæœ¬: 1.0.0
"""

from . import graph, state

# çš„å­æ˜ç¡®åˆ—å‡ºæ‰€æœ‰å…¬å¼€æ¨¡å—
__all__ = [
    "graph",
    "state",
]
</file>

<file path="app/modules/orchestrator/graph.py">
# app/modules/orchestrator/graph.py

from langgraph.graph import StateGraph, END
from typing import Literal
import json,sqlite3

# 1. å¼•å…¥åŒçº§æˆ–è·¨çº§æ¨¡å— (è¿™æ˜¯å…³è”çš„å…³é”®)
from app.modules.orchestrator.state import ResearchState
from app.modules.perception.search import search_searxng
from app.modules.perception.crawler import crawl_urls
from app.core.llm import simple_llm_call
from app.modules.knowledge.vector import KnowledgeBase
from langgraph.checkpoint.sqlite import SqliteSaver
from app.modules.insight.prompts import ResearchPrompts
from app.core.config import settings

kb = KnowledgeBase()

def log_step(step_name: str, content: dict):
    """
    æ ¼å¼åŒ–æ‰“å°æ—¥å¿—ï¼Œæ”¯æŒä¸­æ–‡æ˜¾ç¤º
    """
    print(f"\nğŸš€ [Step: {step_name}]")
    # ä½¿ç”¨ json.dumps æ ¼å¼åŒ–æ‰“å°ï¼Œensure_ascii=False è®©ä¸­æ–‡æ­£å¸¸æ˜¾ç¤º
    print(json.dumps(content, indent=2, ensure_ascii=False, default=str))
    print("-" * 50)

# --- èŠ‚ç‚¹é€»è¾‘å®ç° ---

async def node_planner(state: ResearchState):
    """
    [è§„åˆ’è€…] åŠ¨æ€è§„åˆ’ä¸‹ä¸€æ­¥
    """
    iteration = state["iteration_count"]
    gap = state.get("gap_analysis", "æ— ")
    topic = state["topic"]
    
    print(f"--- [Planner] Iteration {iteration} | Gap: {gap[:50]}... ---")

    # ğŸŸ¢ ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨ç»Ÿä¸€ Prompt
    if iteration == 0:
        prompt = ResearchPrompts.planner_initial(topic)
    else:
        prompt = ResearchPrompts.planner_gap_driven(topic, gap)

    search_query = await simple_llm_call(prompt, model="deepseek/deepseek-chat")
    
    result = {
        "search_queries": [search_query],
        "iteration_count": iteration + 1
    }
    log_step("Planner", result)
    return result

async def node_search_execute(state: ResearchState):
    """
    [æ‰§è¡Œè€…] æœç´¢ -> çˆ¬å– -> å­˜å…¥å‘é‡åº“
    """
    current_query = state["search_queries"][-1]
    print(f"--- [Search] Executing: {current_query} ---")
    
    search_results = await search_searxng(current_query, num_results=3)
    urls = [item["url"] for item in search_results]
    web_contents = await crawl_urls(urls)
    
    if not web_contents and search_results:
        for item in search_results:
            web_contents.append({
                "url": item["url"],
                "content": item["snippet"],
                "source": "searxng_snippet"
            })

    if web_contents:
        print(f"ğŸ’¾ [Knowledge] Saving {len(web_contents)} docs...")
        kb.add_documents(web_contents)
            
    return {"web_results": web_contents}

async def node_analyst(state: ResearchState):
    """
    [åˆ†æå¸ˆ] RAG æ£€ç´¢ -> æ·±åº¦æ€è€ƒ -> å‘ç°ç›²ç‚¹
    """
    print("--- [Analyst] RAG Retrieval & Thinking ---")
    topic = state["topic"]
    
    query = state.get("gap_analysis") or topic
    context = kb.search(query, limit=10)
    
    if not context:
        context = "æš‚æ— ç›¸å…³ä¿¡æ¯ï¼Œè¯·å°è¯•æ–°çš„æœç´¢ã€‚"

    # ğŸŸ¢ ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨ç»Ÿä¸€ Prompt
    prompt = ResearchPrompts.analyst_reasoning(topic, context)
    
    # ä½¿ç”¨ DeepSeek R1
    response = await simple_llm_call(prompt, model="deepseek/deepseek-reasoner")
    
    # ç®€å•è§£æé€»è¾‘ (ä¿æŒä¸å˜)
    gap = "æ— "
    draft = response
    if "ç¼ºå°‘" in response or "ç¼ºä¹" in response or "éœ€è¦" in response:
        gap = "Need more specific data based on analysis." 
        
    result = {
        "draft_report": draft,
        "gap_analysis": gap
    }
    log_step("Analyst", result)
    return result

async def node_publisher(state: ResearchState):
    """
    [å‡ºç‰ˆè€…] ç”Ÿæˆå¸¦å¼•ç”¨çš„æœ€ç»ˆæŠ¥å‘Š
    """
    print("--- [Publisher] Compiling Final Report ---")
    topic = state["topic"]
    
    context = kb.search(topic, limit=20) 
    
    # ğŸŸ¢ ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨ç»Ÿä¸€ Prompt
    prompt = ResearchPrompts.publisher_final_report(topic, context)
    
    final_report = await simple_llm_call(prompt, model="deepseek/deepseek-reasoner")
    
    return {"final_report": final_report}

def check_sufficiency(state: ResearchState) -> Literal["continue", "publish"]:
    """
    [å†³ç­–é€»è¾‘] å†³å®šç»§ç»­è¿˜æ˜¯ç»“æŸ
    """
    if state["iteration_count"] >= state["max_iterations"]:
        print("--- [Decision] Max Limit Reached -> Publish ---")
        return "publish"
    
    # è¿™é‡Œçš„é€»è¾‘å¯ä»¥å†™å¤æ‚ç‚¹ï¼Œæ¯”å¦‚åˆ¤æ–­ gap_analysis æ˜¯å¦ä¸ºç©º
    # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬åªè·‘ 1 è½®å°±ç»“æŸ
    if state["iteration_count"] < 1: 
        return "continue"
        
    return "publish"

# --- å›¾è°±æ„å»º ---

def build_graph():

    # 1. åˆå§‹åŒ– SQLite è¿æ¥ä½œä¸ºè®°å¿†å­˜å‚¨
    conn = sqlite3.connect(settings.CHECKPOINT_DB_PATH,check_same_thread=False)
    memory = SqliteSaver(conn)

    workflow = StateGraph(ResearchState)

    # æ³¨å†ŒèŠ‚ç‚¹
    workflow.add_node("planner", node_planner)
    workflow.add_node("searcher", node_search_execute)
    workflow.add_node("analyst", node_analyst)
    workflow.add_node("publisher", node_publisher)

    # ç¼–æ’æµç¨‹
    workflow.set_entry_point("planner")
    workflow.add_edge("planner", "searcher")
    workflow.add_edge("searcher", "analyst")
    
    workflow.add_conditional_edges(
        "analyst",
        check_sufficiency,
        {
            "continue": "planner",
            "publish": "publisher"
        }
    )
    
    workflow.add_edge("publisher", END)

    return workflow.compile(checkpointer=memory)
</file>

<file path="app/modules/orchestrator/state.py">
# app/modules/orchestrator/state.py
from typing import List, TypedDict, Annotated
import operator

# å®šä¹‰ä¸€ä¸ªå•ä¸€çš„æœç´¢ç»“æœç»“æ„
class SearchResult(TypedDict):
    url: str
    content: str  # æ¸…æ´—åçš„ Markdown
    source: str   # æ¥æºåŸŸå

# å®šä¹‰ Agent çš„å…¨å±€çŠ¶æ€
class ResearchState(TypedDict):
    # 1. ç”¨æˆ·è¾“å…¥
    topic: str
    
    # 2. è¿­ä»£æ§åˆ¶
    iteration_count: int  # å½“å‰é€’å½’äº†å¤šå°‘è½®
    max_iterations: int   # æœ€å¤§å…è®¸è½®æ•° (é˜²æ­¢æ­»å¾ªç¯)
    
    # 3. çŸ¥è¯†ç§¯ç´¯ (ä½¿ç”¨ operator.add å®ç°å¢é‡æ›´æ–°ï¼Œè€Œéè¦†ç›–)
    # Annotated[List, operator.add] æ„å‘³ç€æ¯æ¬¡è¿”å›è¿™ä¸ªå­—æ®µï¼Œéƒ½ä¼š append åˆ°åˆ—è¡¨ä¸­
    search_queries: Annotated[List[str], operator.add] 
    web_results: Annotated[List[SearchResult], operator.add]
    
    # 4. æ€è€ƒä¸äº§å‡º
    gap_analysis: str    # å½“å‰è¿˜ç¼ºä»€ä¹ˆä¿¡æ¯
    draft_report: str    # è‰ç¨¿
    final_report: str    # æœ€ç»ˆäº§å‡º
</file>

<file path="app/modules/perception/__init__.py">
"""
æ„ŸçŸ¥æ¨¡å— - ä¿¡æ¯æ„ŸçŸ¥å’Œæ”¶é›†

è´Ÿè´£ä¿¡æ¯çš„æ”¶é›†ã€æ„ŸçŸ¥å’Œåˆæ­¥å¤„ç†ã€‚

æœ¬æ¨¡å—æä¾›å¤šç§ä¿¡æ¯æ”¶é›†èƒ½åŠ›ï¼š
    - ä»äº’è”ç½‘æŠ“å–ç½‘é¡µå†…å®¹
    - é€šè¿‡æœç´¢å¼•æ“è·å–ç›¸å…³ä¿¡æ¯
    - è§£æå’Œæå–å…³é”®ä¿¡æ¯
    - å¯¹æ”¶é›†çš„æ•°æ®è¿›è¡Œé¢„å¤„ç†

å­æ¨¡å—:
    - crawler: ç½‘é¡µçˆ¬è™«
        æä¾›æ™ºèƒ½ç½‘é¡µæŠ“å–èƒ½åŠ›
        æ”¯æŒåŠ¨æ€å†…å®¹åŠ è½½å’Œåçˆ¬è™«ç­–ç•¥

    - search: æœç´¢å¼•æ“
        é›†æˆå¤šç§æœç´¢å¼•æ“API
        æä¾›æ™ºèƒ½æœç´¢å’Œç»“æœç­›é€‰åŠŸèƒ½

ä¸»è¦åŠŸèƒ½:
    1. å¤šæºä¿¡æ¯æ”¶é›†
    2. å®æ—¶ç½‘é¡µå†…å®¹æŠ“å–
    3. æ™ºèƒ½æœç´¢å’Œç»“æœä¼˜åŒ–
    4. æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
    5. æ”¯æŒå¤šç§æ•°æ®æ ¼å¼è¾“å‡º

ä½¿ç”¨ç¤ºä¾‹:
    from app.modules.perception import crawler, search

ä½œè€…: ApexBridge Team
ç‰ˆæœ¬: 1.0.0
"""

from . import crawler, search

# æ˜ç¡®åˆ—å‡ºæ‰€æœ‰å…¬å¼€çš„å­æ¨¡å—
__all__ = [
    "crawler",
    "search",
]
</file>

<file path="app/modules/perception/crawler.py">
# app/modules/perception/crawler.py
import asyncio
from crawl4ai import AsyncWebCrawler
from typing import List, Dict

async def crawl_urls(urls: List[str]) -> List[Dict]:
    """
    å¹¶å‘æŠ“å–å¤šä¸ª URL å¹¶è½¬æ¢ä¸º Markdown (ä¼˜åŒ–ç‰ˆ)
    """
    if not urls:
        return []

    print(f"ğŸ•·ï¸ [Crawl4AI] Starting concurrent crawl for {len(urls)} URLs...")
    
    # å®šä¹‰å•ä¸ª URL çš„å¤„ç†é€»è¾‘ (é—­åŒ…)
    async def process_url(crawler, url: str):
        try:
            # arun æ˜¯å¼‚æ­¥çš„ï¼Œè¿™é‡Œå¹¶å‘è°ƒç”¨åŒä¸€ä¸ª crawler å®ä¾‹
            result = await crawler.arun(
                url=url,
                bypass_cache=True,       # æ€»æ˜¯è·å–æœ€æ–°å†…å®¹
                word_count_threshold=50  # è¿‡æ»¤æ‰å†…å®¹è¿‡å°‘çš„é¡µé¢ (å¦‚ 403/404 é¡µ)
            )
            
            if result.success:
                print(f"âœ… [Crawl4AI] Scraped: {url[:30]}... ({len(result.markdown)} chars)")
                return {
                    "url": url,
                    "content": result.markdown,
                    "source": url
                }
            else:
                print(f"âš ï¸ [Crawl4AI] Failed to scrape {url}: {result.error_message}")
                return None
                
        except Exception as e:
            print(f"âŒ [Crawl4AI] Exception for {url}: {e}")
            return None

    # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¯åŠ¨æµè§ˆå™¨å®ä¾‹
    async with AsyncWebCrawler(verbose=True) as crawler:
        # 1. åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = [process_url(crawler, url) for url in urls]
        
        # 2. å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ (Gather)
        # å¦‚æœ URL éå¸¸å¤š(>10)ï¼Œå»ºè®®ä½¿ç”¨ asyncio.Semaphore é™åˆ¶å¹¶å‘æ•°
        # ä½† Deep Research æ¯æ¬¡ä¸€èˆ¬åªæœ 3-5 ä¸ªç»“æœï¼Œç›´æ¥ gather å³å¯
        results_with_none = await asyncio.gather(*tasks)
        
        # 3. è¿‡æ»¤æ‰å¤±è´¥çš„ç»“æœ (None)
        results = [r for r in results_with_none if r is not None]
                
    return results
</file>

<file path="app/modules/perception/search.py">
# app/modules/perception/search.py
import httpx, json
from typing import List, Dict
from app.core.config import settings

SEARXNG_URL = settings.SEARXNG_BASE_URL

async def search_searxng(query: str, num_results: int = 5) -> List[Dict[str, str]]:
    """
    è°ƒç”¨æœ¬åœ° SearXNG æœç´¢ï¼Œè¿”å› URL åˆ—è¡¨
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    params = {
        "q": query,
        "format": "json",
        "engines": "quark (ZH),sogou (ZH),360search (ZH),wikimini (FR),yandex,mwmbl,currency,dictzone,libretranslate,lingva,mojeek,naver (KO),crowdview", 
        "language": "zh-CN", # æ ¹æ®éœ€æ±‚è°ƒæ•´
        "safesearch": "0"
    }
    print(f"ğŸ” [Debug] Requesting: {SEARXNG_URL}")
    print(f"ğŸ” [Debug] Params: {params}")
    async with httpx.AsyncClient() as client:
        try:
            resp = await client.get(SEARXNG_URL, params=params, headers=headers, timeout=15.0)
            
            # 2. æ£€æŸ¥ HTTP çŠ¶æ€ç 
            if resp.status_code != 200:
                print(f"âŒ [Error] SearXNG returned status: {resp.status_code}")
                print(f"âŒ [Error] Response text: {resp.text[:200]}") # æ‰“å°å‰200å­—ç¬¦çœ‹çœ‹æŠ¥é”™ä¿¡æ¯
                return []

            # 3. æ£€æŸ¥æ˜¯å¦è¿”å›äº† JSON
            try:
                data = resp.json()
            except json.JSONDecodeError:
                print("âŒ [Error] Returned content is NOT JSON. Maybe HTML?")
                print(f"âŒ [Content Preview]: {resp.text[:200]}...")
                return []

            # 4. æ£€æŸ¥æ˜¯å¦æœ‰ results å­—æ®µ
            if "results" not in data:
                print(f"âš ï¸ [Warning] JSON parsed but no 'results' field. Keys: {data.keys()}")
                # æœ‰æ—¶ SearXNG æŠ¥é”™ä¼šè¿”å› {"error": "..."}
                if "error" in data:
                    print(f"âš ï¸ [SearXNG Error]: {data['error']}")
                return []
            
            raw_results = data["results"]
            print(f"âœ… [Debug] Raw results count: {len(raw_results)}")

            # 5. æå–æœ‰æ•ˆé“¾æ¥
            results = []
            for item in raw_results[:num_results]:
                if item.get("url", "").startswith("http"):
                    results.append({
                        "url": item["url"],
                        "title": item.get("title", ""),
                        "snippet": item.get("content", "")
                    })
            
            return results

        except Exception as e:
            print(f"âŒ [Exception] Connection failed: {e}")
            return []
</file>

<file path="app/modules/__init__.py">
"""
æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

åŒ…å«äº”å¤§æ ¸å¿ƒæ¨¡å—ï¼š
    - orchestrator: å·¥ä½œæµç¼–æ’å™¨
        ä½¿ç”¨LangGraphå®ç°çŠ¶æ€æœºå’Œå·¥ä½œæµç¼–æ’
        è´Ÿè´£ä»»åŠ¡è°ƒåº¦ã€æµç¨‹æ§åˆ¶å’Œæ¨¡å—åè°ƒ

    - perception: ä¿¡æ¯æ„ŸçŸ¥æ¨¡å—
        è´Ÿè´£ä¿¡æ¯æ”¶é›†å’Œæ„ŸçŸ¥
        åŒ…æ‹¬ç½‘é¡µæŠ“å–(crawler)å’Œæœç´¢å¼•æ“(search)èƒ½åŠ›

    - knowledge: çŸ¥è¯†å¼•æ“
        è´Ÿè´£æ•°æ®çš„å­˜å‚¨ã€ç´¢å¼•å’Œæ£€ç´¢
        åŒ…æ‹¬å‘é‡æ•°æ®åº“(LanceDB)å’Œå…³ç³»å‹æ•°æ®åº“(SQLModel)

    - insight: æ´å¯Ÿæ¨¡å—
        è´Ÿè´£ç”Ÿæˆå’Œç®¡ç†Promptæ¨¡æ¿
        ä¸ºLLMæä¾›ç»“æ„åŒ–çš„æç¤ºè¯

è¿™äº›æ¨¡å—ååŒå·¥ä½œï¼Œå®ç°å®Œæ•´çš„AIé©±åŠ¨ç ”ç©¶æµç¨‹ã€‚

ä½¿ç”¨ç¤ºä¾‹:
    from app.modules import (
        orchestrator,
        perception,
        knowledge,
        insight
    )

ä½œè€…: ApexBridge Team
ç‰ˆæœ¬: 1.0.0
"""

from . import (
    orchestrator,
    perception,
    knowledge,
    insight
)

# æ˜ç¡®åˆ—å‡ºæ‰€æœ‰å…¬å¼€çš„æ ¸å¿ƒæ¨¡å—
__all__ = [
    "orchestrator",
    "perception",
    "knowledge",
    "insight",
]
</file>

<file path="app/__init__.py">
"""
Deep Research Backend - æ ¸å¿ƒåº”ç”¨åŒ…

è¿™æ˜¯ä¸€ä¸ªåŸºäºAIçš„æ·±åº¦ç ”ç©¶ç³»ç»Ÿï¼Œæä¾›æ™ºèƒ½ä¿¡æ¯æ”¶é›†ã€åˆ†æå’Œæ´å¯Ÿèƒ½åŠ›ã€‚
ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼šå·¥ä½œæµç¼–æ’ã€ä¿¡æ¯æ„ŸçŸ¥ã€çŸ¥è¯†ç®¡ç†å’Œæ´å¯Ÿç”Ÿæˆã€‚

ä¸»è¦æ¨¡å—:
    - api: APIè·¯ç”±å’Œæ¥å£
    - modules: æ ¸å¿ƒåŠŸèƒ½æ¨¡å—é›†åˆ
        - orchestrator: å·¥ä½œæµç¼–æ’å™¨
        - perception: ä¿¡æ¯æ„ŸçŸ¥å’Œæ”¶é›†
        - knowledge: çŸ¥è¯†å­˜å‚¨å’Œæ£€ç´¢
        - insight: æ´å¯Ÿç”Ÿæˆ

ä½¿ç”¨è¯´æ˜:
    # å¯¼å…¥APIæ¨¡å—
    from app.api import research, history

    # å¯¼å…¥æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
    from app.modules import orchestrator, perception, knowledge, insight

    # å¯¼å…¥LLMè°ƒç”¨æ¥å£
    from app.core import simple_llm_call

ä½œè€…: ApexBridge Team
ç‰ˆæœ¬: 1.0.0
è®¸å¯è¯: MIT
"""

__version__ = "1.0.0"
__author__ = "ApexBridge Team"
__email__ = "team@apexbridge.ai"
__license__ = "MIT"
__description__ = "åŸºäºAIçš„æ·±åº¦ç ”ç©¶ç³»ç»Ÿ"


__all__ = [
    "__version__",
    "__author__",
    "__email__",
    "__license__",
    "__description__",
]
</file>

<file path="main.py">
# main.py
from fastapi import FastAPI
from app.api.research import router as research_router
import uvicorn
from app.core.config import settings

app = FastAPI(title="Deep Research Backend")

# æ³¨å†Œè·¯ç”±
app.include_router(research_router, prefix="/api")

if __name__ == "__main__":
    print(f"ğŸš€ Starting server on {settings.API_HOST}:{settings.API_PORT}")
    uvicorn.run(
        "main:app", 
        host=settings.API_HOST, 
        port=settings.API_PORT, 
        reload=True
    )
</file>

<file path="requirements.txt">
fastapi
uvicorn
langgraph
pydantic
sse-starlette
httpx
crawl4ai
lancedb
litellm
sqlmodel
loguru
pydantic_settings
lancedb
langchain-text-splitters
langgraph-checkpoint-sqlite
numpy
</file>

</files>
