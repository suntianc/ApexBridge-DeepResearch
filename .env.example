# .env Template
# Copy this file to .env and fill in your values
# Comments use "#", don't add spaces around "="

# ============================================
# Service Configuration
# ============================================
API_HOST=0.0.0.0
API_PORT=23800

# ============================================
# Data Storage Paths
# ============================================
LANCEDB_PATH=./data/lancedb
CHECKPOINT_DB_PATH=./data/checkpoints.db

# ============================================
# Report Output Configuration
# ============================================
# Whether to save final report as Markdown file
SAVE_REPORT_TO_FILE=true
# Report output directory
REPORT_OUTPUT_DIR=./outputs

# ============================================
# Search Configuration
# ============================================
# Search provider: "tavily" or "searxng"
SEARCH_PROVIDER=tavily

# SearXNG Configuration (only used when SEARCH_PROVIDER=searxng)
SEARXNG_BASE_URL=http://localhost:8888/search

# Tavily Configuration (supports multiple API keys with round-robin)
# Separate multiple keys with commas: TAVILY_API_KEYS=key1,key2
# Get API keys at: https://tavily.com/
TAVILY_API_KEYS=your_tavily_api_keys_here

# ============================================
# LLM API Configuration
# ============================================
# DeepSeek API Key (recommended)
# Get yours at: https://platform.deepseek.com/
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# OpenAI API Key (optional)
# OPENAI_API_KEY=

# Anthropic (Claude) Custom Base URL (optional)
# ANTHROPIC_API_BASE="https://api.openai.com/v1"
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# ============================================
# Tiered Model Strategy
# ============================================
# Choose models based on task complexity

# [Planner/Reasoning]: DAG generation, task decomposition, complex reasoning
MODEL_PLANNER=anthropic/MiniMax-M2.1

# [Writer]: Long-form generation, report writing
MODEL_WRITER=anthropic/MiniMax-M2.1

# [Critic/Judge]: Reflection scoring, MAD debate裁决
MODEL_CRITIC=anthropic/MiniMax-M2.1

# [Fast Task]: Crawler cleaning, simple info extraction
MODEL_FAST=anthropic/MiniMax-M2.1

# [Long Context]: Reading large amounts of search results at once
MODEL_LONG=anthropic/MiniMax-M2.1

# [General Purpose]: Fallback default model
MODEL_SMART=anthropic/MiniMax-M2.1

# ============================================
# Runtime Controls
# ============================================
# LangGraph max recursion limit (prevent infinite loops)
MAX_RECURSION_LIMIT=100
# Global timeout in seconds
GLOBAL_TIMEOUT_SEC=600
# Search depth
MAX_SEARCH_RESULTS=5

# ============================================
# Embedding Model Configuration
# ============================================
# Embedding model for vector retrieval

# Options:
# 1. Local Ollama:
#    - ollama/nomic-embed-text (768 dim)
#    - ollama/llama3.2:3b (3072 dim)
# 2. OpenAI:
#    - openai/text-embedding-3-small (1536 dim)
#    - openai/text-embedding-3-large (3072 dim)
EMBEDDING_MODEL=ollama/nomic-embed-text
# Vector dimension (must match EMBEDDING_MODEL)
EMBEDDING_DIMENSION=768

# ============================================
# Optional: LangSmith Tracing (for debugging)
# ============================================
# LANGCHAIN_TRACING_V2=true
# LANGCHAIN_API_KEY=your_langsmith_api_key
# LANGCHAIN_PROJECT=apexbridge-research
