from litellm import completion
import os
from dotenv import load_dotenv

# åŠ è½½ .env ç¯å¢ƒå˜é‡
load_dotenv()

async def simple_llm_call(
    prompt: str, 
    model: str = "deepseek/deepseek-chat", # é»˜è®¤æ”¹ä¸º DeepSeek V3
    temperature: float = 0.7
) -> str:
    """
    é€šç”¨ LLM è°ƒç”¨æ¥å£ï¼Œæ”¯æŒ DeepSeek, OpenAI, Claude, Ollama ç­‰
    """
    
    # æ‰“å°å½“å‰ä½¿ç”¨çš„æ¨¡å‹ï¼Œæ–¹ä¾¿è°ƒè¯•
    print(f"ğŸ¤– [LLM Call] Model: {model}")

    try:
        # LiteLLM ä¼šè‡ªåŠ¨æ ¹æ® model å‰ç¼€è¯†åˆ«ä¾›åº”å•†
        # deepseek/deepseek-chat -> è‡ªåŠ¨æ˜ å°„åˆ° DeepSeek API
        # ollama/deepseek-r1 -> è‡ªåŠ¨æ˜ å°„åˆ°æœ¬åœ° Ollama
        
        response = completion(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            # å¦‚æœæ˜¯ DeepSeek APIï¼Œä¸éœ€è¦æ‰‹åŠ¨è®¾ base_urlï¼ŒLiteLLM å†…ç½®äº†æ”¯æŒ
            # å¦‚æœæ˜¯ Ollamaï¼ŒLiteLLM é»˜è®¤è¿æ¥ http://localhost:11434
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        print(f"âŒ [LLM Error] {model} failed: {str(e)}")
        return f"Error generation response with {model}. Details: {str(e)}"

# --- ä½¿ç”¨è¯´æ˜ ---
# 1. DeepSeek API: 
#    model="deepseek/deepseek-chat" (V3)
#    model="deepseek/deepseek-reasoner" (R1)
#
# 2. æœ¬åœ° DeepSeek (é€šè¿‡ Ollama):
#    model="ollama/deepseek-r1"
#
# 3. OpenAI:
#    model="gpt-4o"