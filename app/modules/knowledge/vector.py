# app/modules/knowledge/vector.py
import lancedb
import os
import time
import pyarrow as pa
from typing import List, Dict
from litellm import embedding
from langchain_text_splitters import RecursiveCharacterTextSplitter
from app.core.config import settings

# 1. åˆå§‹åŒ– LanceDB (æœ¬åœ°æ–‡ä»¶æ¨¡å¼)
DB_PATH = settings.LANCEDB_PATH
os.makedirs(DB_PATH, exist_ok=True)
db = lancedb.connect(DB_PATH)

# å®šä¹‰è¡¨ç»“æ„ (Schema) - åŠ¨æ€å‘é‡ç»´åº¦
# æ ¹æ® EMBEDDING_MODEL å’Œ EMBEDDING_DIMENSION é…ç½®
schema = pa.schema([
    pa.field("vector", pa.list_(pa.float32(), settings.EMBEDDING_DIMENSION)),
    pa.field("text", pa.string()),
    pa.field("source", pa.string()),
    pa.field("chunk_id", pa.string()),
    pa.field("model", pa.string()),
    pa.field("task_id", pa.string())
])

def get_embedding(text: str) -> List[float]:
    """
    ä½¿ç”¨é…ç½®çš„åµŒå…¥æ¨¡å‹è·å–å‘é‡
    æ”¯æŒå¤šç§æ¨¡å‹ï¼šOllamaæœ¬åœ°æ¨¡å‹ã€äº‘ç«¯APIç­‰
    """
    try:
        response = embedding(
            model=settings.EMBEDDING_MODEL,
            input=[text]
        )
        return response.data[0]['embedding']

    except Exception as e:
        print(f"âŒ Embedding Error ({settings.EMBEDDING_MODEL}): {e}")
        return [0.0] * settings.EMBEDDING_DIMENSION

class KnowledgeBase:
    def __init__(self, table_name: str = "research_context"):
        self.table_name = table_name
        
        # ğŸŸ¢ é²æ£’æ€§å¢å¼ºï¼šç»´åº¦å…¼å®¹æ€§æ£€æŸ¥ä¸è‡ªåŠ¨è¿ç§»
        try:
            # å°è¯•æ‰“å¼€ç°æœ‰è¡¨
            self.table = db.open_table(table_name)
            
            # æ£€æŸ¥ schema ä¸­çš„å‘é‡ç»´åº¦
            # PyArrow çš„ FixedSizeListType å…·æœ‰ list_size å±æ€§
            vec_field = self.table.schema.field("vector")
            existing_dim = vec_field.type.list_size
            
            if existing_dim != settings.EMBEDDING_DIMENSION:
                print(f"âš ï¸ [Knowledge] Dimension mismatch detected! Table: {existing_dim}, Config: {settings.EMBEDDING_DIMENSION}")
                
                # å¤‡ä»½æ—§è¡¨ (é‡å‘½å)
                backup_name = f"{table_name}_backup_{int(time.time())}"
                try:
                    db.rename_table(table_name, backup_name)
                    print(f"ğŸ“¦ Archived old table to '{backup_name}'.")
                except Exception as rename_err:
                    print(f"âš ï¸ Failed to rename table: {rename_err}")
                
                # åˆ›å»ºæ–°è¡¨
                print("ğŸ†• Creating new table with correct dimension...")
                self.table = db.create_table(table_name, schema=schema)
                
        except Exception:
            # å¦‚æœè¡¨ä¸å­˜åœ¨ï¼Œç›´æ¥åˆ›å»º
            # print(f"â„¹ï¸ Table '{table_name}' not found, creating new one.")
            self.table = db.create_table(table_name, schema=schema)

    def add_documents(self, documents: List[Dict], task_id: str):
        """
        æ¥æ”¶çˆ¬å–ç»“æœ -> åˆ‡ç‰‡ -> å‘é‡åŒ– -> å­˜å…¥
        """
        # æ ¹æ®åµŒå…¥æ¨¡å‹è°ƒæ•´ chunk_size
        if "openai" in settings.EMBEDDING_MODEL:
            chunk_size = 800
        else:
            chunk_size = 1200

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=200
        )

        data_to_insert = []

        print(f"ğŸ’¾ [Knowledge] Processing {len(documents)} docs with {settings.EMBEDDING_MODEL} (Dim: {settings.EMBEDDING_DIMENSION})...")

        for doc in documents:
            clean_content = doc["content"].replace("\n\n\n", "\n")
            chunks = splitter.split_text(clean_content)

            for idx, chunk in enumerate(chunks):
                vec = get_embedding(chunk)
                if len(vec) == settings.EMBEDDING_DIMENSION:
                    data_to_insert.append({
                        "vector": vec,
                        "text": chunk,
                        "source": doc.get("source", "unknown"),
                        "chunk_id": f"{doc.get('url', 'unknown')}_{idx}",
                        "model": settings.EMBEDDING_MODEL,
                        "task_id": task_id
                    })

        if data_to_insert:
            self.table.add(data_to_insert)
            print(f"âœ… [Knowledge] Inserted {len(data_to_insert)} chunks (Dim: {settings.EMBEDDING_DIMENSION})")

    def search(self, query: str, task_id: str, limit: int = 5) -> str:
        """
        è¯­ä¹‰æ£€ç´¢
        """
        print(f"ğŸ” [Retrieval] Searching for: {query[:30]}...")
        try:
            query_vec = get_embedding(query)
            results = self.table.search(query_vec).where(f"task_id = '{task_id}'").limit(limit).to_list()
            
            context = ""
            for item in results:
                context += f"--- Source: {item['source']} ---\n{item['text']}\n\n"
                
            print(f"âœ… [Retrieval] Found {len(results)} relevant chunks")
            return context
        except Exception as e:
            print(f"âš ï¸ Retrieval failed: {e}")
            return ""
        
    def clear_task_data(self, task_id: str):
        try:
            self.table.delete(f"task_id = '{task_id}'")
            print(f"ğŸ§¹ [Knowledge] Cleared vectors for task: {task_id}")
        except Exception as e:
            print(f"âš ï¸ Failed to clear task data: {e}")