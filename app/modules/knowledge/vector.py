import lancedb,os
from typing import List, Dict
from litellm import embedding
from langchain_text_splitters import RecursiveCharacterTextSplitter
import pyarrow as pa
from app.core.config import settings

# 1. åˆå§‹åŒ– LanceDB (æœ¬åœ°æ–‡ä»¶æ¨¡å¼)
DB_PATH = settings.LANCEDB_PATH
os.makedirs(DB_PATH, exist_ok=True)
db = lancedb.connect(DB_PATH)

# å®šä¹‰è¡¨ç»“æ„ (Schema)
# vector ç»´åº¦å–å†³äºä½ ä½¿ç”¨çš„æ¨¡å‹ï¼ŒOpenAI text-embedding-3-small æ˜¯ 1536 ç»´
schema = pa.schema([
    pa.field("vector", pa.list_(pa.float32(), 768)),
    pa.field("text", pa.string()),
    pa.field("source", pa.string()),
    pa.field("chunk_id", pa.string())
])

def get_embedding(text: str) -> List[float]:
    """
    ä½¿ç”¨æœ¬åœ° Ollama çš„ nomic-embed-text è·å–å‘é‡
    é’ˆå¯¹ Log ç»“æ„: response['data'][0]['embedding']
    """
    try:
        response = embedding(
            model="ollama/nomic-embed-text", 
            input=[text]
        )
        return response.data[0]['embedding']

    except Exception as e:
        print(f"âŒ Embedding Error (Ollama): {e}")
        return [0.0] * 768

class KnowledgeBase:
    def __init__(self, table_name: str = "research_context"):
        self.table_name = table_name
        # å¦‚æœè¡¨ä¸å­˜åœ¨åˆ™åˆ›å»º
        try:
            self.table = db.open_table(table_name)
        except:
            self.table = db.create_table(table_name, schema=schema)

    def add_documents(self, documents: List[Dict]):
        """
        æ¥æ”¶çˆ¬å–ç»“æœ -> åˆ‡ç‰‡ -> å‘é‡åŒ– -> å­˜å…¥
        """
        # nomic-embed-text æ”¯æŒ 8192 contextï¼Œæˆ‘ä»¬å¯ä»¥ç¨å¾®æŠŠ chunk åˆ‡å¤§ä¸€ç‚¹
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1200, 
            chunk_overlap=200
        )
        
        data_to_insert = []
        
        print(f"ğŸ’¾ [Knowledge] Processing {len(documents)} docs with nomic-embed-text...")
        
        for doc in documents:
            # ç®€å•çš„æ¸…æ´—ï¼Œå»é™¤è¿‡å¤šç©ºè¡Œ
            clean_content = doc["content"].replace("\n\n\n", "\n")
            chunks = splitter.split_text(clean_content)
            
            for idx, chunk in enumerate(chunks):
                vec = get_embedding(chunk)
                # ç®€å•æ ¡éªŒç»´åº¦ï¼Œé˜²æ­¢ Ollama å¶å°”è¿”å›ç©º
                if len(vec) == 768:
                    data_to_insert.append({
                        "vector": vec,
                        "text": chunk,
                        "source": doc.get("source", "unknown"),
                        "chunk_id": f"{doc.get('url', 'unknown')}_{idx}"
                    })
        
        if data_to_insert:
            # mode="append" è¿½åŠ æ¨¡å¼
            self.table.add(data_to_insert)
            print(f"âœ… [Knowledge] Inserted {len(data_to_insert)} chunks (Dim: 768)")

    def search(self, query: str, limit: int = 5) -> str:
        """
        è¯­ä¹‰æ£€ç´¢
        """
        print(f"ğŸ” [Retrieval] Searching for: {query[:30]}...")
        query_vec = get_embedding(query)
        
        # å‘é‡æœç´¢
        results = self.table.search(query_vec).limit(limit).to_list()
        
        context = ""
        for item in results:
            context += f"--- Source: {item['source']} ---\n{item['text']}\n\n"
            
        print(f"âœ… [Retrieval] Found {len(results)} relevant chunks")
        return context