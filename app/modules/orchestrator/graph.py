# app/modules/orchestrator/graph.py

from langgraph.graph import StateGraph, END
from typing import Literal
import json,sqlite3

# 1. å¼•å…¥åŒçº§æˆ–è·¨çº§æ¨¡å— (è¿™æ˜¯å…³è”çš„å…³é”®)
from app.modules.orchestrator.state import ResearchState
from app.modules.perception.search import search_searxng
from app.modules.perception.crawler import crawl_urls
from app.core.llm import simple_llm_call
from app.modules.knowledge.vector import KnowledgeBase
from langgraph.checkpoint.sqlite import SqliteSaver
from app.modules.insight.prompts import ResearchPrompts
from app.core.config import settings

kb = KnowledgeBase()

def log_step(step_name: str, content: dict):
    """
    æ ¼å¼åŒ–æ‰“å°æ—¥å¿—ï¼Œæ”¯æŒä¸­æ–‡æ˜¾ç¤º
    """
    print(f"\nğŸš€ [Step: {step_name}]")
    # ä½¿ç”¨ json.dumps æ ¼å¼åŒ–æ‰“å°ï¼Œensure_ascii=False è®©ä¸­æ–‡æ­£å¸¸æ˜¾ç¤º
    print(json.dumps(content, indent=2, ensure_ascii=False, default=str))
    print("-" * 50)

# --- èŠ‚ç‚¹é€»è¾‘å®ç° ---

async def node_planner(state: ResearchState):
    """
    [è§„åˆ’è€…] åŠ¨æ€è§„åˆ’ä¸‹ä¸€æ­¥
    """
    iteration = state["iteration_count"]
    gap = state.get("gap_analysis", "æ— ")
    topic = state["topic"]
    
    print(f"--- [Planner] Iteration {iteration} | Gap: {gap[:50]}... ---")

    # ğŸŸ¢ ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨ç»Ÿä¸€ Prompt
    if iteration == 0:
        prompt = ResearchPrompts.planner_initial(topic)
    else:
        prompt = ResearchPrompts.planner_gap_driven(topic, gap)

    search_query = await simple_llm_call(prompt, model="deepseek/deepseek-chat")
    
    result = {
        "search_queries": [search_query],
        "iteration_count": iteration + 1
    }
    log_step("Planner", result)
    return result

async def node_search_execute(state: ResearchState):
    """
    [æ‰§è¡Œè€…] æœç´¢ -> çˆ¬å– -> å­˜å…¥å‘é‡åº“
    """
    current_query = state["search_queries"][-1]
    print(f"--- [Search] Executing: {current_query} ---")
    
    search_results = await search_searxng(current_query, num_results=3)
    urls = [item["url"] for item in search_results]
    web_contents = await crawl_urls(urls)
    
    if not web_contents and search_results:
        for item in search_results:
            web_contents.append({
                "url": item["url"],
                "content": item["snippet"],
                "source": "searxng_snippet"
            })

    if web_contents:
        print(f"ğŸ’¾ [Knowledge] Saving {len(web_contents)} docs...")
        kb.add_documents(web_contents)
            
    return {"web_results": web_contents}

async def node_analyst(state: ResearchState):
    """
    [åˆ†æå¸ˆ] RAG æ£€ç´¢ -> æ·±åº¦æ€è€ƒ -> å‘ç°ç›²ç‚¹
    """
    print("--- [Analyst] RAG Retrieval & Thinking ---")
    topic = state["topic"]
    
    query = state.get("gap_analysis") or topic
    context = kb.search(query, limit=10)
    
    if not context:
        context = "æš‚æ— ç›¸å…³ä¿¡æ¯ï¼Œè¯·å°è¯•æ–°çš„æœç´¢ã€‚"

    # ğŸŸ¢ ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨ç»Ÿä¸€ Prompt
    prompt = ResearchPrompts.analyst_reasoning(topic, context)
    
    # ä½¿ç”¨ DeepSeek R1
    response = await simple_llm_call(prompt, model="deepseek/deepseek-reasoner")
    
    # ç®€å•è§£æé€»è¾‘ (ä¿æŒä¸å˜)
    gap = "æ— "
    draft = response
    if "ç¼ºå°‘" in response or "ç¼ºä¹" in response or "éœ€è¦" in response:
        gap = "Need more specific data based on analysis." 
        
    result = {
        "draft_report": draft,
        "gap_analysis": gap
    }
    log_step("Analyst", result)
    return result

async def node_publisher(state: ResearchState):
    """
    [å‡ºç‰ˆè€…] ç”Ÿæˆå¸¦å¼•ç”¨çš„æœ€ç»ˆæŠ¥å‘Š
    """
    print("--- [Publisher] Compiling Final Report ---")
    topic = state["topic"]
    
    context = kb.search(topic, limit=20) 
    
    # ğŸŸ¢ ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨ç»Ÿä¸€ Prompt
    prompt = ResearchPrompts.publisher_final_report(topic, context)
    
    final_report = await simple_llm_call(prompt, model="deepseek/deepseek-reasoner")
    
    return {"final_report": final_report}

def check_sufficiency(state: ResearchState) -> Literal["continue", "publish"]:
    """
    [å†³ç­–é€»è¾‘] å†³å®šç»§ç»­è¿˜æ˜¯ç»“æŸ
    """
    if state["iteration_count"] >= state["max_iterations"]:
        print("--- [Decision] Max Limit Reached -> Publish ---")
        return "publish"
    
    # è¿™é‡Œçš„é€»è¾‘å¯ä»¥å†™å¤æ‚ç‚¹ï¼Œæ¯”å¦‚åˆ¤æ–­ gap_analysis æ˜¯å¦ä¸ºç©º
    # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬åªè·‘ 1 è½®å°±ç»“æŸ
    if state["iteration_count"] < 1: 
        return "continue"
        
    return "publish"

# --- å›¾è°±æ„å»º ---

def build_graph():

    # 1. åˆå§‹åŒ– SQLite è¿æ¥ä½œä¸ºè®°å¿†å­˜å‚¨
    conn = sqlite3.connect(settings.CHECKPOINT_DB_PATH,check_same_thread=False)
    memory = SqliteSaver(conn)

    workflow = StateGraph(ResearchState)

    # æ³¨å†ŒèŠ‚ç‚¹
    workflow.add_node("planner", node_planner)
    workflow.add_node("searcher", node_search_execute)
    workflow.add_node("analyst", node_analyst)
    workflow.add_node("publisher", node_publisher)

    # ç¼–æ’æµç¨‹
    workflow.set_entry_point("planner")
    workflow.add_edge("planner", "searcher")
    workflow.add_edge("searcher", "analyst")
    
    workflow.add_conditional_edges(
        "analyst",
        check_sufficiency,
        {
            "continue": "planner",
            "publish": "publisher"
        }
    )
    
    workflow.add_edge("publisher", END)

    return workflow.compile(checkpointer=memory)