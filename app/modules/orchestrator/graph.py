# app/modules/orchestrator/graph.py

from langgraph.graph import StateGraph, END
import json,os

from app.core.config import settings
from app.core.utils import parse_json_safe
from app.modules.orchestrator.state import ResearchState
from app.modules.orchestrator.dag import DAGManager, TaskStatus
from app.modules.perception.search import search_generic as search_tool
from app.modules.perception.crawler import crawl_urls
from app.core.llm import simple_llm_call
# å¼•å…¥æ–°çš„æ–‡ä»¶å­˜å‚¨
from app.modules.knowledge.file_store import FileKnowledgeStore
from app.modules.insight.prompts import prompts
from app.modules.verification.verification_agent import VerificationAgent
from app.modules.utils.file_utils import save_markdown_report

# åˆå§‹åŒ–æ–‡ä»¶å­˜å‚¨
kb = FileKnowledgeStore()

# --- è¾…åŠ©å‡½æ•° ---

def _normalize_title(title: str) -> str:
    """
    æ ‡å‡†åŒ–æ ‡é¢˜ï¼šå»é™¤åºå·ã€ç©ºæ ¼ã€æ ‡ç‚¹ï¼Œè½¬å°å†™
    ç”¨äºè§£å†³ Planner ç”Ÿæˆçš„æ ‡é¢˜ä¸ Searcher æ‰“æ ‡ç­¾ä¸ä¸€è‡´çš„é—®é¢˜
    """
    import re
    if not title:
        return ""
    # å»é™¤åºå· (1., 1.1, ä¸€ã€, etc.)
    normalized = re.sub(r'^[\d\.\ï¼]+\s*|^\w[ã€]\s*', '', title)
    # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ï¼Œç§»é™¤æ‰€æœ‰æ ‡ç‚¹å’Œç©ºæ ¼
    normalized = re.sub(r'[^\w\u4e00-\u9fff]+', '', normalized)
    return normalized.lower()

def _match_sections(section_title: str, label: str) -> bool:
    """æ¨¡ç³ŠåŒ¹é…ç« èŠ‚æ ‡é¢˜"""
    return _normalize_title(section_title) == _normalize_title(label)

def log_step(step_name: str, content: dict):
    print(f"\nğŸš€ [Step: {step_name}]")
    try:
        text = json.dumps(content, indent=2, ensure_ascii=False, default=str)
        if len(text) > 2000:
            print(text[:2000] + "\n... (truncated)")
        else:
            print(text)
    except:
        print(str(content)[:2000])
    print("-" * 50)

# --- èŠ‚ç‚¹é€»è¾‘ ---

async def node_clarifier(state: ResearchState):
    print("--- [Clarifier] Checking Ambiguity ---")
    if state.get("clarified_intent"): return {}
    prompt = prompts.clarification_check(state["task"])
    response = await simple_llm_call(prompt, model=settings.MODEL_REASONING)
    result = parse_json_safe(response)
    
    if result and not result.get("is_clear", True):
        assumptions = result.get("assumptions", "Default assumptions")
        questions = result.get("questions", [])
        print(f"âš ï¸ Ambiguity Detected: {questions}")
        print(f"ğŸ¤– Auto-resolving: {assumptions}")
        new_intent = f"{state['task']} (Context: {assumptions})"
        return {"needs_clarification": True, "clarified_intent": new_intent, "clarification_history": questions}
    return {"needs_clarification": False, "clarified_intent": state["task"]}

async def node_planner(state: ResearchState):
    print(f"--- [Planner] (Model: {settings.MODEL_REASONING}) ---")
    dag = DAGManager(state["plan"])
    model_to_use = settings.MODEL_REASONING
    intent = state.get("clarified_intent", state["task"])
    
    # 1. å¤§çº²ç”Ÿæˆ
    current_outline = state.get("outline", [])
    if not current_outline:
        print("ğŸ“ [Planner] Generating Research Outline...")
        outline_resp = await simple_llm_call(prompts.outline_generation(state["task"], intent), model=model_to_use)
        current_outline = parse_json_safe(outline_resp) or []
        print(f"ğŸ“‘ Outline: {current_outline}")
    
    # 2. ä»»åŠ¡ç”Ÿæˆ
    has_feedback = False
    if state["reflection_logs"]:
        last_log = state["reflection_logs"][-1]
        if last_log.get("score", 0) < 8.0:
            print(f"ğŸ”„ [Planner] Replanning based on critique...")
            has_feedback = True

            # ğŸŸ¢ æ–°å¢ï¼šå¤„ç† Critic çš„ç« èŠ‚çº§åé¦ˆ
            focus_section = last_log.get("focus_section")
            reason = last_log.get("reason", "unknown")

            if focus_section and last_log.get("score", 0) < 7.5:
                print(f"ğŸ”§ [Planner] Handling Critic feedback for section: {focus_section}")

                if reason == "insufficient_data":
                    # ğŸŸ¢ ç¼ºæ•°æ® -> ç”Ÿæˆé’ˆå¯¹æ€§çš„æœç´¢ä»»åŠ¡
                    feedback_str = f"æ‰¹è¯„: {last_log.get('critique')}\nå»ºè®®: {last_log.get('adjustment')}"
                    resp = await simple_llm_call(
                        prompts.planner_section_retry(focus_section, feedback_str),
                        model=settings.MODEL_REASONING
                    )
                    new_tasks = parse_json_safe(resp) or []
                    for t in new_tasks:
                        if isinstance(t, dict) and "id" in t:
                            dag.add_task(
                                t["id"],
                                t["description"],
                                t.get("dependencies", []),
                                related_section=focus_section
                            )
                    print(f"   ğŸ“ Generated {len(new_tasks)} tasks for section '{focus_section}'")
                # reason æ˜¯ "writing_quality" -> å¯ä»¥è·³è¿‡æœç´¢ç›´æ¥é‡å†™ï¼ˆå½“å‰è®¾è®¡å› Planner å³å¯ï¼‰
            else:
                # ğŸŸ¢ é€šç”¨é‡è§„åˆ’
                feedback_str = f"æ‰¹è¯„: {last_log.get('critique')}\nå»ºè®®: {last_log.get('adjustment')}"
                plan_str = json.dumps(dag.to_state(), ensure_ascii=False)
                resp = await simple_llm_call(prompts.planner_dag_replanning(intent, plan_str, feedback_str), model=model_to_use)
                new_tasks = parse_json_safe(resp) or []
                # é˜²å¾¡æ€§å¤„ç†
                if isinstance(new_tasks, list):
                    for t in new_tasks:
                        if isinstance(t, dict) and "id" in t and "description" in t:
                            dag.add_task(t["id"], t["description"], t.get("dependencies", []))
    
    if not dag.tasks and not has_feedback:
        print("ğŸ“ [Planner] Generating Tasks from Outline...")
        plan_str = json.dumps(dag.to_state(), ensure_ascii=False)
        resp = await simple_llm_call(prompts.planner_tasks_from_outline(intent, current_outline, plan_str), model=model_to_use)
        new_tasks = parse_json_safe(resp) or []

        # é˜²å¾¡æ€§å¤„ç†ï¼šç¡®ä¿ new_tasks æ˜¯å­—å…¸åˆ—è¡¨
        if isinstance(new_tasks, list):
            valid_tasks = []
            for t in new_tasks:
                if isinstance(t, dict) and "id" in t and "description" in t:
                    valid_tasks.append(t)
                else:
                    print(f"âš ï¸ [Planner] Skipping invalid task format: {t}")
            for t in valid_tasks:
                dag.add_task(t["id"], t["description"], dependencies=t.get("dependencies", []), related_section=t.get("related_section"))
            
    ready_tasks = dag.get_ready_tasks()
    current_queries = [t.description for t in ready_tasks]
    for t in ready_tasks: dag.set_task_running(t.id)
    
    log_step("Planner", {"outline": current_outline, "plan": dag.to_state()})
    return {"outline": current_outline, "plan": dag.to_state(), "search_queries": current_queries}

async def node_search_execute(state: ResearchState):
    print("ğŸ”„ [Search Node] Entered...", flush=True)
    dag = DAGManager(state["plan"])
    running_tasks = [t for t in dag.tasks.values() if t.status == TaskStatus.RUNNING]

    if not running_tasks:
        print("âš ï¸ [Search Node] No running tasks found!")
        return {}

    print(f"--- [Search] Processing {len(running_tasks)} tasks ---", flush=True)
    collected_docs = []

    # ğŸŸ¢ åˆå§‹åŒ–æ˜ å°„è¡¨ï¼ˆä¿ç•™ä¹‹å‰çš„æ˜ å°„ï¼Œæ”¯æŒå¢é‡æ·»åŠ ï¼‰
    file_map = state.get("file_section_map", {}).copy()

    for task in running_tasks:
        print(f"ğŸ” Task: {task.description}")
        try:
            raw_results = await search_tool(task.description)
        except Exception as e:
            dag.fail_task(task.id, str(e))
            continue

        if not raw_results:
            dag.complete_task(task.id, "No results found")
            continue

        snippets = "\n".join([f"[{i}] {r['url']}\n    {r['snippet'][:100]}..." for i, r in enumerate(raw_results)])
        select_resp = await simple_llm_call(prompts.search_result_selection(task.description, snippets, num_select=3), model=settings.MODEL_CHAT)
        selected_urls = parse_json_safe(select_resp) or [r["url"] for r in raw_results[:3]]

        print(f"ğŸ¯ [Selector] Selected: {selected_urls}")
        crawl_results = await crawl_urls(selected_urls)

        if crawl_results:
            collected_docs.extend(crawl_results)

            # ğŸŸ¢ ä¿®æ”¹ç‚¹ï¼šè¿”å›ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
            saved_paths = kb.add_documents(crawl_results, task_id=state["task_id"])

            # ğŸŸ¢ æ ¹æ®ä»»åŠ¡å…³è”çš„ç« èŠ‚æ‰“æ ‡ç­¾
            section = task.related_section

            # å¦‚æœæ˜¯"ç»¼åˆè°ƒç ”"ç±»ä»»åŠ¡ï¼ˆæ— ç‰¹å®šç« èŠ‚å…³è”ï¼‰ï¼Œæ ‡è®°ä¸ºé€šç”¨
            if not section:
                if any(kw in task.description.lower() for kw in ["overview", "introduction", "èƒŒæ™¯", "æ¦‚å†µ"]):
                    section = "__general__"
                else:
                    section = "__uncategorized__"

            for path in saved_paths:
                file_map[path] = section
                print(f"   ğŸ“ {os.path.basename(path)} -> {section}")

            dag.complete_task(task.id, f"Scraped {len(crawl_results)} valid pages/files")
        else:
            dag.complete_task(task.id, "No valid content retrieved")

    if collected_docs:
        print(f"ğŸ’¾ [Knowledge] Saved {len(collected_docs)} files.")

    dag.get_ready_tasks()
    # ğŸŸ¢ è¿”å› file_section_map å­—æ®µ
    return {"plan": dag.to_state(), "knowledge_stats": [f"Added {len(collected_docs)} docs"], "file_section_map": file_map}

# ğŸŸ¢ æ ¸å¿ƒä¿®æ”¹ï¼šAnalyst èŠ‚ç‚¹ (åˆ†ç« èŠ‚æŠ¥å‘Šç”Ÿæˆ)
async def node_analyst(state: ResearchState):
    print(f"--- [Analyst] Section-based Reporting ---")

    outline = state.get("outline", [])
    file_map = state.get("file_section_map", {})
    all_files = kb.list_files(state["task_id"])

    if not all_files:
        return {"draft_report": "Error: No documents found to analyze."}

    # ğŸŸ¢ å‡†å¤‡ç« èŠ‚é˜Ÿåˆ—
    # ä¼˜å…ˆä½¿ç”¨ state ä¸­çš„ pending_sections (æ¥è‡ª Critic çš„è¿”å·¥è¦æ±‚)
    # å¦‚æœ state["pending_sections"] ä¸ºç©º (é¦–æ¬¡è¿è¡Œ)ï¼Œæ‰ä½¿ç”¨å®Œæ•´ outline
    current_pending = state.get("pending_sections", [])
    if current_pending:
        print(f"ğŸ”„ [Analyst] Resuming specific sections: {current_pending}")
        target_sections = current_pending
    else:
        target_sections = outline.copy() if outline else ["__general__"]

    section_drafts = state.get("section_drafts", {}).copy()  # è®°å¾— .copy() é˜²æ­¢åŸåœ°ä¿®æ”¹

    # ğŸŸ¢ æ ¸å¿ƒé€»è¾‘ï¼šæŒ‰ç« èŠ‚é€ä¸ªæ”»ç ´
    for section_title in target_sections:
        print(f"  Writing Section: {section_title}")

        # 1. ç­›é€‰å±äºå½“å‰ç« èŠ‚çš„æ–‡ä»¶ (ä¸‰å±‚ä¼˜å…ˆçº§ï¼Œä½¿ç”¨æ¨¡ç³ŠåŒ¹é…)
        section_files = [f for f in all_files if _match_sections(section_title, file_map.get(f, ""))]
        general_files = [f for f in all_files if file_map.get(f) == "__general__"]
        uncategorized_files = [f for f in all_files if file_map.get(f) in ("__uncategorized__", None)]

        # 2. åˆå¹¶æ–‡ä»¶åˆ—è¡¨ï¼ˆä¸“å±åœ¨å‰ï¼Œé€šç”¨æ¬¡ä¹‹ï¼Œæœªåˆ†ç±»å…œåº•ï¼‰
        relevant_files = section_files + general_files + uncategorized_files

        if not relevant_files:
            print(f"   No files for section: {section_title}")
            continue

        # 3. å¢é‡é˜…è¯»è¯¥ç« èŠ‚
        section_notes = ""
        for i, file_path in enumerate(relevant_files):
            doc_content = kb.read_file(file_path)
            if not doc_content:
                continue
            if len(doc_content) > 80000:
                doc_content = doc_content[:80000] + "\n...(truncated)..."

            print(f"   [{i+1}/{len(relevant_files)}] {os.path.basename(file_path)}")

            # è°ƒç”¨ LLM æ›´æ–°è¯¥ç« èŠ‚çš„ç¬”è®°
            prompt = prompts.analyst_section_writing(section_title, section_notes, doc_content)
            section_notes = await simple_llm_call(prompt, model=settings.MODEL_CHAT)

        # 4. ç”Ÿæˆè¯¥ç« èŠ‚çš„æœ€ç»ˆæ–‡æœ¬
        if section_notes:
            section_drafts[section_title] = section_notes

    # 5. æ‹¼è£…å®Œæ•´æŠ¥å‘Š (Merger)
    print("  Merging all sections...")
    topic = state.get("clarified_intent", state["task"])

    full_report = await simple_llm_call(
        prompts.analyst_merge_sections(topic, outline, section_drafts),
        model=settings.MODEL_CHAT
    )

    # 6. ç»Ÿä¸€çš„äº‹å®æ ¸æŸ¥
    print("  Running verification...")
    verified_report = await VerificationAgent.verify_report(full_report)

    return {
        "draft_report": verified_report,
        "section_drafts": section_drafts,
        "pending_sections": []
    }

async def node_critic(state: ResearchState):
    """æ”¯æŒæŒ‰ç« èŠ‚åé¦ˆçš„ Critic"""
    print("--- [Critic] Section-aware Reviewing ---")
    topic = state.get("clarified_intent", state["task"])
    draft = state.get("draft_report", "")
    section_drafts = state.get("section_drafts", {})

    prompt = prompts.critic_evaluation(topic, draft, section_drafts)
    resp = await simple_llm_call(prompt, model=settings.MODEL_REASONING)

    default_eval = {
        "score": 5,
        "critique": "Parsing failed",
        "adjustment": "Retry",
        "focus_section": None,
        "reason": "unknown"
    }
    eval_data = parse_json_safe(resp) or default_eval

    try:
        score = float(eval_data.get("score", 0))
    except:
        score = 5.0

    # å¦‚æœ Critic æŒ‡å‡ºç‰¹å®šç« èŠ‚é—®é¢˜ï¼Œå°†è¯¥ç« èŠ‚æ”¾å›å¾…åŠ
    focus_section = eval_data.get("focus_section")
    new_pending = [] 
    
    if focus_section and score < 7.5:
        # åªæœ‰åœ¨åˆ†æ•°ä½ä¸”æŒ‡å®šäº†ç« èŠ‚æ—¶ï¼Œæ‰æ ‡è®°ä¸ºå¾…åŠ
        new_pending = [focus_section]
        print(f"ğŸ”„ [Critic] Marking section for rework: {focus_section}")

    log = {
        "step_name": f"Iter-{state['iteration_count']}",
        "critique": eval_data.get("critique"),
        "score": score,
        "adjustment": eval_data.get("adjustment"),
        "focus_section": focus_section,
        "reason": eval_data.get("reason", "unknown")
    }
    
    return {
        "reflection_logs": [log],
        "iteration_count": state["iteration_count"] + 1,
        "pending_sections": new_pending  # âœ… è¿”å›æ–°çš„å¾…åŠåˆ—è¡¨ï¼Œä¾›ä¸‹ä¸€è½® Analyst ä½¿ç”¨
    }

async def node_publisher(state: ResearchState):
    print("--- [Publisher] Generating Final Report ---")
    topic = state.get("clarified_intent", state["task"])
    
    # è·å– Analyst ç”Ÿæˆå¹¶ç»è¿‡ Verification çš„è‰ç¨¿
    draft = state.get("draft_report", "")
    
    if not draft:
        return {"final_report": "Error: No draft report generated."}

    # Publisher çš„å·¥ä½œæ˜¯ï¼šæ ¼å¼åŒ–ã€æ¶¦è‰²ã€å¢åŠ å‰è¨€/ç›®å½•
    # æˆ‘ä»¬å°† draft ä½œä¸ºæ ¸å¿ƒä¸Šä¸‹æ–‡ä¼ ç»™ LLM
    prompt = prompts.publisher_final_report(topic, draft)
    
    final_report = await simple_llm_call(prompt, model=settings.MODEL_CHAT)
    
    # ä¿å­˜
    saved_path = save_markdown_report(state["task"], final_report)
    if saved_path: 
        print(f"âœ… Report saved to: {saved_path}")
    
    return {"final_report": final_report}

# --- è·¯ç”±é€»è¾‘ ---

def route_planner(state: ResearchState) -> str:
    print("ğŸš¦ [Router] Deciding next step after Planner...")
    dag = DAGManager(state["plan"])
    running = [t for t in dag.tasks.values() if t.status == TaskStatus.RUNNING]
    if running:
        print(f"   -> Going to 'searcher' ({len(running)} tasks running)")
        return "searcher"
    if dag.is_all_completed():
        print("   -> Going to 'analyst' (All tasks completed)")
        return "analyst"
    print("   -> Fallback to 'analyst'")
    return "analyst"

def route_critic(state: ResearchState) -> str:
    """æ”¯æŒç« èŠ‚çº§é‡è¯•çš„è·¯ç”± - å…³é”®ä¿®å¤ï¼šé¿å…æ­»å¾ªç¯"""
    if state["iteration_count"] >= state["max_iterations"]:
        print("ğŸ›‘ Max iterations reached -> Publisher")
        return "publisher"

    # é˜²å¾¡æ€§æ£€æŸ¥ï¼šé˜²æ­¢ reflection_logs ä¸ºç©º
    if not state.get("reflection_logs"):
        print("âš ï¸ No reflection logs found -> Publisher")
        return "publisher"

    last_log = state["reflection_logs"][-1]
    score = last_log.get("score", 0)

    if score >= 7.5:
        print("âœ… Score >= 7.5 -> Publisher")
        return "publisher"

    # æ ¸å¿ƒä¿®å¤ï¼šæ— è®ºé—®é¢˜æ˜¯å¦åœ¨ç‰¹å®šç« èŠ‚ï¼Œéƒ½å› Planner
    print(f"ğŸ”„ [Router] Score {score} < 7.5 -> Back to Planner for repair")
    return "planner"

# --- æ„å»ºå›¾è°± ---

def build_graph():
    workflow = StateGraph(ResearchState)
    workflow.add_node("clarifier", node_clarifier)
    workflow.add_node("planner", node_planner)
    workflow.add_node("searcher", node_search_execute)
    workflow.add_node("analyst", node_analyst)
    workflow.add_node("critic", node_critic)
    workflow.add_node("publisher", node_publisher)
    
    workflow.set_entry_point("clarifier")
    workflow.add_edge("clarifier", "planner")
    workflow.add_conditional_edges("planner", route_planner, {"searcher": "searcher", "analyst": "analyst"})
    workflow.add_edge("searcher", "planner")
    workflow.add_edge("analyst", "critic")
    workflow.add_conditional_edges("critic", route_critic, {"planner": "planner", "publisher": "publisher"})
    workflow.add_edge("publisher", END)
    return workflow