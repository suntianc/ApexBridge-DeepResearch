# app/modules/perception/crawler.py
import asyncio
from crawl4ai import AsyncWebCrawler
from typing import List, Dict

async def crawl_urls(urls: List[str]) -> List[Dict]:
    """
    å¹¶å‘æŠ“å–å¤šä¸ª URL å¹¶è½¬æ¢ä¸º Markdown (ä¿®å¤å¯¼èˆªå†²çªç‰ˆ)
    """
    if not urls:
        return []

    print(f"ğŸ•·ï¸ [Crawl4AI] Starting concurrent crawl for {len(urls)} URLs...")
    
    # å®šä¹‰å•ä¸ª URL çš„å¤„ç†é€»è¾‘ (é—­åŒ…)
    async def process_url(crawler, url: str):
        # ğŸŸ¢ å¢åŠ é‡è¯•æœºåˆ¶ï¼šé’ˆå¯¹ Navigation é”™è¯¯é‡è¯•æœ€å¤š 3 æ¬¡
        for attempt in range(3):
            try:
                # arun æ˜¯å¼‚æ­¥çš„ï¼Œè¿™é‡Œå¹¶å‘è°ƒç”¨åŒä¸€ä¸ª crawler å®ä¾‹
                result = await crawler.arun(
                    url=url,
                    bypass_cache=True,       # æ€»æ˜¯è·å–æœ€æ–°å†…å®¹
                    word_count_threshold=50, # è¿‡æ»¤æ‰å†…å®¹è¿‡å°‘çš„é¡µé¢
                    
                    # ğŸŸ¢ [å…³é”®ä¿®å¤] å¢åŠ ç­‰å¾…ç­–ç•¥
                    # 1. magic=True: è‡ªåŠ¨å¤„ç†ä¸€äº›åçˆ¬å’Œå¼¹çª— (å¦‚æœæœ‰è¿™ä¸ªå‚æ•°å»ºè®®å¼€å¯ï¼Œè§†ç‰ˆæœ¬è€Œå®š)
                    # 2. delay_before_return_html: å¼ºåˆ¶ç­‰å¾…é¡µé¢é™æ­¢ 2 ç§’ï¼Œé˜²æ­¢è·³è½¬ä¸­è¯»å–
                    delay_before_return_html=2.0, 
                    
                    # 3. wait_until: ç­‰å¾…ç½‘ç»œç©ºé—²ï¼Œç¡®ä¿é‡å®šå‘å®Œæˆ
                    # å¯é€‰: 'domcontentloaded', 'networkidle', 'load'
                    wait_until="domcontentloaded",
                    
                    # 4. timeout: é˜²æ­¢å•ä¸ªé¡µé¢å¡æ­»æ•´ä¸ªä»»åŠ¡
                    timeout=30000 
                )
                
                if result.success:
                    # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé˜²æ­¢ token çˆ†ç‚¸
                    content = result.markdown[:50000]
                    print(f"âœ… [Crawl4AI] Scraped: {url[:30]}... ({len(content)} chars)")
                    return {
                        "url": url,
                        "content": content,
                        "source": url
                    }
                else:
                    error_msg = result.error_message or "Unknown error"
                    # å¦‚æœæ˜¯ç‰¹å®šé”™è¯¯ï¼Œå¯èƒ½éœ€è¦é‡è¯•
                    if "navigating" in error_msg.lower() or "timeout" in error_msg.lower():
                         print(f"âš ï¸ [Crawl4AI] Retry {attempt+1}/3 for {url}: {error_msg}")
                         await asyncio.sleep(2) # é¿è®©ä¸€ä¸‹
                         continue
                    
                    print(f"âš ï¸ [Crawl4AI] Failed to scrape {url}: {error_msg}")
                    return None
                    
            except Exception as e:
                error_str = str(e)
                # ğŸŸ¢ æ•è· Playwright çš„ç‰¹å®šå¯¼èˆªé”™è¯¯å¹¶é‡è¯•
                if "navigating" in error_str.lower() and attempt < 2:
                    print(f"ğŸ”„ [Crawl4AI] Navigation conflict, retrying {attempt+1}/3: {url}")
                    await asyncio.sleep(2.0) # ç­‰å¾… 2 ç§’åé‡è¯•
                    continue
                
                print(f"âŒ [Crawl4AI] Exception for {url}: {e}")
                return None
        
        return None # é‡è¯•è€—å°½

    # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¯åŠ¨æµè§ˆå™¨å®ä¾‹
    async with AsyncWebCrawler(verbose=True) as crawler:
        # 1. åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = [process_url(crawler, url) for url in urls]
        
        # 2. å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results_with_none = await asyncio.gather(*tasks)
        
        # 3. è¿‡æ»¤æ‰å¤±è´¥çš„ç»“æœ
        results = [r for r in results_with_none if r is not None]
                
    return results