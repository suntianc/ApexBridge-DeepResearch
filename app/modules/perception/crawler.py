# app/modules/perception/crawler.py
import asyncio
import httpx
import fitz  # PyMuPDF
import numpy as np
import cv2
import logging
from crawl4ai import AsyncWebCrawler
from typing import List, Dict

# å°è¯•å¯¼å…¥ PaddleOCR
PADDLE_AVAILABLE = False
try:
    from paddleocr import PaddleOCR
    PADDLE_AVAILABLE = True
except ImportError:
    print("âš ï¸ PaddleOCR not installed. Scanning features disabled.")

# å…¨å±€ OCR å¼•æ“å•ä¾‹ (æ‡’åŠ è½½)
_ocr_engine = None

def get_ocr_engine():
    global _ocr_engine
    if _ocr_engine is None and PADDLE_AVAILABLE:
        print("ğŸ‘ï¸ [System] Loading PaddleOCR Model (This may take time)...")
        # use_angle_cls=True è‡ªåŠ¨çº æ­£æ–¹å‘, lang="ch" æ”¯æŒä¸­è‹±æ–‡
        _ocr_engine = PaddleOCR(use_angle_cls=True, lang="ch", show_log=False)
    return _ocr_engine

def process_pdf_sync(pdf_bytes: bytes, url: str) -> str:
    """
    [åŒæ­¥å‡½æ•°] PDF å¤„ç†æ ¸å¿ƒé€»è¾‘ï¼šPyMuPDF + PaddleOCR æ··åˆç­–ç•¥
    å°†åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼Œé¿å…é˜»å¡ Async äº‹ä»¶å¾ªç¯ã€‚
    """
    try:
        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    except Exception as e:
        print(f"âŒ [PDF] Failed to open: {e}")
        return ""

    full_text = []
    ocr = get_ocr_engine()
    total_pages = len(doc)
    
    print(f"ğŸ“„ [PDF] Processing {total_pages} pages from {url}...")

    # é™åˆ¶å¤„ç†é¡µæ•°ï¼Œé˜²æ­¢å‡ ç™¾é¡µçš„è´¢æŠ¥æŠŠ OCR è·‘æ­» (æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼Œæ¯”å¦‚å‰20é¡µæ ¸å¿ƒå†…å®¹)
    # å¦‚æœæ‚¨çœŸçš„"ä¸åœ¨ä¹æ—¶é—´"ï¼Œå¯ä»¥æŠŠè¿™ä¸ªé™åˆ¶å»æ‰æˆ–è°ƒå¤§
    MAX_OCR_PAGES = 15 

    for i, page in enumerate(doc):
        # 1. å°è¯•ç›´æ¥æå–æ–‡æœ¬ (æå¿«)
        text = page.get_text()
        
        # 2. å¯†åº¦æ£€æµ‹ï¼šå¦‚æœæ–‡å­—æå°‘ï¼Œåˆ¤å®šä¸ºæ‰«æä»¶/å›¾ç‰‡
        if len(text.strip()) < 50 and PADDLE_AVAILABLE:
            if i < MAX_OCR_PAGES:
                print(f"   ğŸ” [OCR] Page {i+1}/{total_pages} is image-based. Scanning...")
                try:
                    # æ¸²æŸ“ä¸ºé«˜åˆ†è¾¨ç‡å›¾ç‰‡ (zoom=2) æå‡è¯†åˆ«ç‡
                    pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
                    
                    # è½¬æ¢ PyMuPDF(Pix) -> Numpy(OpenCV)
                    img_data = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.h, pix.w, pix.n)
                    
                    # é¢œè‰²ç©ºé—´è½¬æ¢
                    if pix.n == 4:
                        img_data = cv2.cvtColor(img_data, cv2.COLOR_RGBA2RGB)
                    elif pix.n == 3:
                        pass # RGB
                    else:
                        img_data = cv2.cvtColor(img_data, cv2.COLOR_GRAY2RGB)

                    # æ‰§è¡Œ OCR
                    result = ocr.ocr(img_data, cls=True)
                    
                    # è§£æç»“æœ
                    ocr_lines = []
                    if result and result[0]:
                        for line in result[0]:
                            # line ç»“æ„: [[box], (text, score)]
                            txt = line[1][0]
                            ocr_lines.append(txt)
                    
                    ocr_text = "\n".join(ocr_lines)
                    text = f"\n--- [Page {i+1} OCR Scan] ---\n{ocr_text}\n"
                    
                except Exception as e:
                    print(f"âš ï¸ [OCR] Failed on page {i+1}: {e}")
            else:
                text = "\n[OCR Skipped: Page limit reached]\n"
        
        full_text.append(text)
    
    return "\n\n".join(full_text)

async def extract_pdf_content(url: str) -> str:
    """ä¸‹è½½å¹¶è§£æ PDF"""
    print(f"â¬‡ï¸ [PDF] Downloading: {url}")
    try:
        async with httpx.AsyncClient(verify=False, follow_redirects=True, timeout=60.0) as client:
            response = await client.get(url)
            # å¤„ç†éƒ¨åˆ†æœåŠ¡å™¨è¿”å› 403 çš„æƒ…å†µï¼Œæ¨¡æ‹Ÿ UA
            if response.status_code != 200:
                response = await client.get(url, headers={"User-Agent": "Mozilla/5.0"})
            
            response.raise_for_status()
            
            # æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯ PDF
            if b"%PDF" not in response.content[:10]:
                return None 

            # ğŸŸ¢ å…³é”®ï¼šå°†ç¹é‡çš„ PDF å¤„ç†æ”¾å…¥çº¿ç¨‹æ± 
            return await asyncio.to_thread(process_pdf_sync, response.content, url)

    except Exception as e:
        print(f"âŒ [PDF] Download error {url}: {e}")
        return None

async def crawl_urls(urls: List[str]) -> List[Dict]:
    """æ™ºèƒ½æ··åˆçˆ¬è™«å…¥å£"""
    if not urls: return []

    print(f"ğŸ•·ï¸ [Smart Crawler] Processing {len(urls)} URLs...")
    
    results = []
    pdf_urls = [u for u in urls if u.lower().endswith(".pdf")]
    web_urls = [u for u in urls if not u.lower().endswith(".pdf")]
    
    # 1. å¤„ç† PDF (é™åˆ¶å¹¶å‘ï¼Œé˜²æ­¢ CPU çˆ†ç‚¸)
    if pdf_urls:
        print(f"ğŸ“„ Found {len(pdf_urls)} PDFs. Queueing OCR...")
        # ä¿¡å·é‡æ§åˆ¶åŒæ—¶è¿›è¡Œçš„ OCR ä»»åŠ¡æ•° (CPUå¯†é›†å‹)
        sem = asyncio.Semaphore(2) 
        
        async def safe_pdf_task(u):
            async with sem:
                content = await extract_pdf_content(u)
                if content:
                    return {"url": u, "content": content, "source": "pdf_document"}
                return None

        pdf_results = await asyncio.gather(*[safe_pdf_task(u) for u in pdf_urls])
        
        for i, res in enumerate(pdf_results):
            if res:
                results.append(res)
            else:
                # å‡å¦‚è§£æå¤±è´¥ï¼Œå¯èƒ½æ˜¯ä¼ªè£…çš„ HTMLï¼Œä¸¢å› Web é˜Ÿåˆ—
                web_urls.append(pdf_urls[i])

    # 2. å¤„ç† Web é¡µé¢ (crawl4ai)
    if web_urls:
        print(f"ğŸŒ [Crawl4AI] Crawling {len(web_urls)} web pages...")
        async with AsyncWebCrawler(verbose=True) as crawler:
            async def process_web(url):
                # ç®€å•é‡è¯•
                for _ in range(2):
                    try:
                        res = await crawler.arun(
                            url=url, 
                            bypass_cache=True, 
                            word_count_threshold=50,
                            delay_before_return_html=1.0, # ç»™ JS ä¸€ç‚¹æ—¶é—´
                            timeout=30000
                        )
                        if res.success:
                            # é™åˆ¶å•é¡µé•¿åº¦ï¼Œé˜²æ­¢å•ä¸ªç½‘é¡µ 5MB æ–‡æœ¬æ’‘çˆ†å†…å­˜
                            return {"url": url, "content": res.markdown[:200000], "source": "web_page"}
                    except: pass
                return None
            
            web_results = await asyncio.gather(*[process_web(u) for u in web_urls])
            results.extend([r for r in web_results if r])

    return results