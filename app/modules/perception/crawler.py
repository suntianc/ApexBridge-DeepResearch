# app/modules/perception/crawler.py
import asyncio
from crawl4ai import AsyncWebCrawler
from typing import List, Dict

async def crawl_urls(urls: List[str]) -> List[Dict]:
    """
    å¹¶å‘æŠ“å–å¤šä¸ª URL å¹¶è½¬æ¢ä¸º Markdown (ä¼˜åŒ–ç‰ˆ)
    """
    if not urls:
        return []

    print(f"ğŸ•·ï¸ [Crawl4AI] Starting concurrent crawl for {len(urls)} URLs...")
    
    # å®šä¹‰å•ä¸ª URL çš„å¤„ç†é€»è¾‘ (é—­åŒ…)
    async def process_url(crawler, url: str):
        try:
            # arun æ˜¯å¼‚æ­¥çš„ï¼Œè¿™é‡Œå¹¶å‘è°ƒç”¨åŒä¸€ä¸ª crawler å®ä¾‹
            result = await crawler.arun(
                url=url,
                bypass_cache=True,       # æ€»æ˜¯è·å–æœ€æ–°å†…å®¹
                word_count_threshold=50  # è¿‡æ»¤æ‰å†…å®¹è¿‡å°‘çš„é¡µé¢ (å¦‚ 403/404 é¡µ)
            )
            
            if result.success:
                print(f"âœ… [Crawl4AI] Scraped: {url[:30]}... ({len(result.markdown)} chars)")
                return {
                    "url": url,
                    "content": result.markdown,
                    "source": url
                }
            else:
                print(f"âš ï¸ [Crawl4AI] Failed to scrape {url}: {result.error_message}")
                return None
                
        except Exception as e:
            print(f"âŒ [Crawl4AI] Exception for {url}: {e}")
            return None

    # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¯åŠ¨æµè§ˆå™¨å®ä¾‹
    async with AsyncWebCrawler(verbose=True) as crawler:
        # 1. åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = [process_url(crawler, url) for url in urls]
        
        # 2. å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ (Gather)
        # å¦‚æœ URL éå¸¸å¤š(>10)ï¼Œå»ºè®®ä½¿ç”¨ asyncio.Semaphore é™åˆ¶å¹¶å‘æ•°
        # ä½† Deep Research æ¯æ¬¡ä¸€èˆ¬åªæœ 3-5 ä¸ªç»“æœï¼Œç›´æ¥ gather å³å¯
        results_with_none = await asyncio.gather(*tasks)
        
        # 3. è¿‡æ»¤æ‰å¤±è´¥çš„ç»“æœ (None)
        results = [r for r in results_with_none if r is not None]
                
    return results