# app/modules/perception/search.py
import asyncio
import random
import re
from typing import List, Dict
import httpx
from app.core.llm import simple_llm_call
from app.core.utils import parse_json_safe
from app.modules.insight.prompts import prompts

# ğŸŸ¢ å¼•å…¥æˆç†Ÿçš„å¼€æºåº“
import arxiv
from github import Github, Auth
import wikipedia

from app.core.config import settings

# --- 0. æŸ¥è¯¢ç¿»è¯‘å…œåº•ç­–ç•¥ ---
# ä¸­æ–‡åœç”¨è¯åˆ—è¡¨ï¼ˆæœç´¢æ—¶ç§»é™¤è¿™äº›è¯ä»¥æé«˜æ£€ç´¢ç²¾åº¦ï¼‰
_QUERY_STOPWORDS = {
    "åˆ†æ", "ç ”ç©¶", "æŠ¥å‘Š", "å¸‚åœº", "å…¨çƒ", "ä¸­å›½", "è¡Œä¸š", "è¶‹åŠ¿",
    "è°ƒç ”", "æ·±åº¦", "å…¨é¢", "æœ€æ–°", "2023", "2024", "2025"
}

def _fallback_query_translate(query: str) -> str:
    """
    è§„åˆ™åŒ–ç¿»è¯‘ï¼šä¸­æ–‡ -> è‹±æ–‡å…³é”®è¯
    å½“ LLM é‡å†™å¤±è´¥æ—¶ï¼Œä½¿ç”¨æ­¤è§„åˆ™å¼•æ“ç”Ÿæˆè‹±æ–‡æœç´¢è¯
    """
    # ç§»é™¤åœç”¨è¯
    words = [w for w in query.split() if w not in _QUERY_STOPWORDS]
    # ç®€å•å¤„ç†ï¼šç§»é™¤å¸¸è§çš„ä¸­æ–‡ä¿®é¥°è¯ï¼Œä¿ç•™æ ¸å¿ƒåè¯
    cleaned = " ".join(words)
    # å¦‚æœç»“æœä»ä¸ºä¸­æ–‡ï¼Œå°è¯•ç®€å•çš„å…³é”®è¯æå–ï¼ˆå–å‰ 5 ä¸ªè¯ï¼‰
    if re.search(r'[\u4e00-\u9fff]', cleaned):
        # å°è¯•ä¿ç•™æŠ€æœ¯æœ¯è¯­å’Œæ ¸å¿ƒå®ä½“
        keywords = []
        for word in words:
            # è·³è¿‡çº¯ä¸­æ–‡è¯ï¼ˆå¯èƒ½æ˜¯é€šç”¨è¯ï¼‰
            if len(word) > 3 and not all('\u4e00' <= c <= '\u9fff' for c in word):
                keywords.append(word)
        cleaned = " ".join(keywords[:5]) if keywords else query
    return cleaned

# --- 1. arXiv æœç´¢ (åŸºäº arxiv åº“) ---
def _sync_arxiv_search(query: str, limit: int) -> List[Dict]:
    """[åŒæ­¥] arXiv æœç´¢é€»è¾‘"""
    print(f"ğŸ“š [arXiv] Searching: {query}...")
    try:
        # æ„é€ æœç´¢å®¢æˆ·ç«¯
        client = arxiv.Client()
        search = arxiv.Search(
            query=query,
            max_results=limit,
            sort_by=arxiv.SortCriterion.Relevance
        )
        
        results = []
        for r in client.results(search):
            results.append({
                "url": r.pdf_url, # ç›´æ¥ç»™ PDF é“¾æ¥ï¼Œé…åˆæˆ‘ä»¬çš„ PDF è§£æå™¨
                "title": f"[arXiv] {r.title}",
                "snippet": f"Published: {r.published.date()}\nAbstract: {r.summary[:500]}...",
                "source": "arxiv"
            })
        return results
    except Exception as e:
        print(f"âš ï¸ [arXiv] Error: {e}")
        return []

async def _search_arxiv(query: str, limit: int = 3) -> List[Dict]:
    """[å¼‚æ­¥åŒ…è£…] æ”¾å…¥çº¿ç¨‹æ± æ‰§è¡Œ"""
    if not settings.ENABLE_ARXIV: return []
    return await asyncio.to_thread(_sync_arxiv_search, query, limit)


# --- 2. GitHub æœç´¢ (åŸºäº PyGithub åº“) ---
def _sync_github_search(query: str, limit: int) -> List[Dict]:
    """[åŒæ­¥] GitHub æœç´¢é€»è¾‘"""
    print(f"ğŸ’» [GitHub] Searching: {query}...")
    try:
        # é‰´æƒ (å¼ºçƒˆå»ºè®®é…ç½® Tokenï¼Œå¦åˆ™é™åˆ¶æä¸¥)
        auth = Auth.Token(settings.GITHUB_TOKEN) if settings.GITHUB_TOKEN else None
        g = Github(auth=auth)
        
        # æœç´¢ä»“åº“
        repos = g.search_repositories(query=query, sort="stars", order="desc")
        
        results = []
        # PyGithub çš„åˆ†é¡µæ˜¯æ‡’åŠ è½½çš„ï¼Œåªå–å‰ limit ä¸ª
        for i, repo in enumerate(repos):
            if i >= limit: break
            
            results.append({
                "url": repo.html_url,
                "title": f"[GitHub] {repo.full_name} ({repo.stargazers_count}â­)",
                "snippet": f"Language: {repo.language}\nDescription: {repo.description}\n(Readme will be crawled)",
                "source": "github"
            })
        
        g.close()
        return results
    except Exception as e:
        print(f"âš ï¸ [GitHub] Error: {e}")
        return []

async def _search_github(query: str, limit: int = 3) -> List[Dict]:
    """[å¼‚æ­¥åŒ…è£…] æ”¾å…¥çº¿ç¨‹æ± æ‰§è¡Œ"""
    if not settings.ENABLE_GITHUB: return []
    return await asyncio.to_thread(_sync_github_search, query, limit)


# --- 3. Wikipedia æœç´¢ (åŸºäº wikipedia åº“) ---
def _sync_wiki_search(query: str, limit: int) -> List[Dict]:
    """[åŒæ­¥] Wiki æœç´¢é€»è¾‘"""
    print(f"ğŸ“– [Wiki] Searching: {query}...")
    try:
        # ä¼˜å…ˆå°è¯•ä¸­æ–‡ï¼Œè‹¥æ— ç»“æœå¯è€ƒè™‘å›é€€è‹±æ–‡ (æ­¤å¤„ç®€åŒ–ä¸ºä¸­æ–‡)
        wikipedia.set_lang("zh")
        
        # 1. æœç´¢è¯æ¡æ ‡é¢˜
        search_results = wikipedia.search(query, results=limit)
        if not search_results:
            # å›é€€åˆ°è‹±æ–‡
            wikipedia.set_lang("en")
            search_results = wikipedia.search(query, results=limit)
            
        final_results = []
        for title in search_results:
            try:
                # 2. è·å–è¯æ¡è¯¦æƒ…
                # auto_suggest=False é˜²æ­¢è‡ªåŠ¨çº é”™å¯¼è‡´æœåˆ°ä¸ç›¸å…³çš„
                page = wikipedia.page(title, auto_suggest=False)
                
                final_results.append({
                    "url": page.url,
                    "title": f"[Wiki] {page.title}",
                    "snippet": page.summary[:500] + "...",
                    "source": "wiki"
                })
            except wikipedia.DisambiguationError as e:
                # æ­§ä¹‰é¡µé¢ï¼Œå–ç¬¬ä¸€ä¸ªé€‰é¡¹é‡è¯•
                try:
                    page = wikipedia.page(e.options[0], auto_suggest=False)
                    final_results.append({
                        "url": page.url,
                        "title": f"[Wiki] {page.title}",
                        "snippet": page.summary[:500] + "...",
                        "source": "wiki"
                    })
                except: pass
            except wikipedia.PageError:
                pass # é¡µé¢ä¸å­˜åœ¨
                
        return final_results
    except Exception as e:
        print(f"âš ï¸ [Wiki] Error: {e}")
        return []

async def _search_wiki(query: str, limit: int = 2) -> List[Dict]:
    """[å¼‚æ­¥åŒ…è£…] æ”¾å…¥çº¿ç¨‹æ± æ‰§è¡Œ"""
    if not settings.ENABLE_WIKI: return []
    return await asyncio.to_thread(_sync_wiki_search, query, limit)


# --- 4. Web æœç´¢ (Tavily) - æ”¯æŒå¤š Key è½®è¯¢ ---
async def _search_web_tavily(query: str, limit: int) -> List[Dict]:
    from app.core.config import settings
    # éšæœºé€‰æ‹©ä¸€ä¸ª API Keyï¼Œå®ç°è´Ÿè½½å‡è¡¡
    api_key = random.choice(settings.TAVILY_API_KEYS) if settings.TAVILY_API_KEYS else None
    if not api_key: return []
    
    try:
        async with httpx.AsyncClient() as client:
            resp = await client.post(
                "https://api.tavily.com/search",
                json={"api_key": api_key, "query": query, "max_results": limit, "search_depth": "basic"},
                timeout=15.0
            )
            resp.raise_for_status()
            data = resp.json()
            return [{
                "url": r["url"], 
                "title": r["title"], 
                "snippet": r["content"], 
                "source": "web"
            } for r in data.get("results", [])]
    except Exception as e:
        print(f"âš ï¸ [Web] Error: {e}")
        return []


# --- 5. èšåˆå…¥å£ ---
async def search_generic(query: str) -> List[Dict[str, str]]:
    """
    [æ··åˆæœç´¢ V2] æ™ºèƒ½æŸ¥è¯¢é‡å†™ + å¹¶è¡Œæœç´¢
    """
    print(f"ğŸ¤” [Hybrid Search] Optimizing query: {query}...")

    # --- A. è°ƒç”¨ LLM è¿›è¡ŒæŸ¥è¯¢é‡å†™ (Query Rewriting) ---
    # ä½¿ç”¨ MODEL_CHAT (å¿«é€Ÿæ¨¡å‹) å³å¯ï¼Œä¸éœ€è¦æ¨ç†æ¨¡å‹
    try:
        rewrite_prompt = prompts.search_query_optimization(query)
        # è¿™é‡Œå»ºè®®ç”¨ MODEL_FAST æˆ– MODEL_CHATï¼Œè¿½æ±‚é€Ÿåº¦
        resp = await simple_llm_call(rewrite_prompt, model=settings.MODEL_CHAT)
        optimized_queries = parse_json_safe(resp)
    except Exception as e:
        print(f"âš ï¸ Query optimization failed: {e}, falling back to raw query.")
        optimized_queries = None

    # --- B. å‡†å¤‡å„å¹³å°çš„æŸ¥è¯¢è¯ ---
    # å¦‚æœé‡å†™å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™å¼•æ“å…œåº•
    if optimized_queries is None:
        print("ğŸ”„ [Search] LLM optimization failed, using rule-based fallback...")
        translated_query = _fallback_query_translate(query)
        q_arxiv = translated_query
        q_github = translated_query
        q_wiki = _fallback_query_translate(query)  # Wiki ä¹Ÿå°è¯•ç¿»è¯‘
        q_web = query  # Web æœç´¢ä¿ç•™ä¸­æ–‡
    else:
        q_arxiv = optimized_queries.get("arxiv", query)
        q_github = optimized_queries.get("github", query)
        q_wiki = optimized_queries.get("wiki", query)
        q_web = optimized_queries.get("web", query)

    print(f"ğŸš€ [Dispatching] \n   - ArXiv: {q_arxiv}\n   - GitHub: {q_github}\n   - Wiki: {q_wiki}\n   - Web: {q_web}")

    # --- C. å¹¶å‘æ‰§è¡Œ ---
    tasks = [
        # ä¼ å…¥å„è‡ªä¼˜åŒ–åçš„å…³é”®è¯
        _search_arxiv(q_arxiv, limit=settings.Result_Count_Arxiv),
        _search_github(q_github, limit=settings.Result_Count_Github),
        _search_wiki(q_wiki, limit=settings.Result_Count_Wiki),
        # Web æœç´¢é€šå¸¸æœ€å¼ºï¼Œä½¿ç”¨ä¼˜åŒ–åçš„ Web å…³é”®è¯
        _search_web_tavily(q_web, limit=settings.Result_Count_Web)
    ]
    
    results_list = await asyncio.gather(*tasks)
    
    # ... (åç»­çš„å±•å¹³ã€å»é‡é€»è¾‘ä¿æŒä¸å˜) ...
    all_results = []
    seen_urls = set()
    for res_group in results_list:
        for r in res_group:
            if r['url'] not in seen_urls:
                seen_urls.add(r['url'])
                all_results.append(r)
            
    print(f"âœ… [Hybrid Search] Found {len(all_results)} total results")
    return all_results

# å…¼å®¹å¯¼å‡º
search_tool = search_generic